# AZ-305 Practice Test 169 Questions

---

## Q14X:

---

### Answer:

---

### References

---

## Q141:

---

### Answer:

---

### References


---

## Q140:

You are developing a transaction processing application to handle back-end processing activity for your company's projected new online sales system. The application will integrate multiple Azure cloud services in an orchestrated solution to handle different components of the sales and fulfilment process. The application must support asynchronous communication passing transaction activity information between different cloud services through messages.
You need to recommend a solution to facilitate communication that can handle a backlog of up to thousands of messages at any time and messages are consumed in order.

What should you recommend?

Choose the correct answer

- Azure Service Bus
- Azure Queue Storage
- Azure Notification Hubs
- Azure Event Hubs

---

### Answer:
- Azure Service Bus

You should recommend Azure Service Bus. With Service Bus you can send thousands of messages in a queue and the messages can be consumed in a First-In-First-Out (FIFO) order.
You should not recommend Azure Queue Storage. Queue Storage lets you store a large number of messages that could be processed through the different services. However, it does not guarantee the ordering of the messages.
You should not recommend Azure Notification Hubs. Notification Hubs is a push engine designed to let you send notifications out to any platform, and it is not used for application data storage and delivery.
You should not recommend Azure Event Hubs. Event Hubs is used for real-time streaming of event data to event consumers, and it supports up to millions of events per second. It does not include any facilities for queueing data or managing a data backlog.

---

### References

[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  
[What is Azure Notification Hubs?](https://learn.microsoft.com/en-us/azure/notification-hubs/notification-hubs-push-notification-overview)  
[Azure Event Hubs - A big data streaming platform and event ingestion service](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  

---

## Q139:

You plan to move your company's API applications to Azure. Each API should be deployed in a specific Azure web app.
You have decided to implement Azure API Management.
For each of the following scenarios, select Yes if the feature is available in API Management. Otherwise, select No.


Throttle the requests to an API.
Yes

Allow only specific IP addresses to call the API.
Yes

Implement autoscaling for an API.
No

---

### Answer:

API Management allows you to provide a level of control to one or more APIs.
You should use API Management when you want to throttle the requests to an API. You can control throttling through a rate-limit policy definition. For example, you can limit the number of calls per minute to an API.
You should also use API Management when you want to allow only specific IP addresses to call the API.
You can control access restrictions through the IP filter policy definition for all of your company APIs in a single place.
You should not use API Management to implement autoscaling for an API. You can accomplish autoscaling for an API by modifying the scaling setting of the Azure App Services.

---

### References

[Advanced request throttling with Azure API Management](https://learn.microsoft.com/en-us/azure/api-management/api-management-sample-flexible-throttling)  
[API Management policy reference](https://learn.microsoft.com/en-us/azure/api-management/api-management-policies)  
[Get started with autoscale in Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-get-started?toc=%2Fazure%2Fapp-service%2Ftoc.json)  

---

## Q138:

You are working for a global company operating in East US, Central US, North Europe, West Europe, East Asia, and Southeast Asia. The company plans to use the Azure Monitor solution to centralize log storage. In order to store the log data in the Azure Monitor solution, the company has the following requirements:

- Minimize administrative overhead.
- Minimize costs.
- Conform to specific regional compliance requirements.
- Provide regional data sovereignty.

You need to identify the minimum number of log analytics workspaces to fulfill the requirements.

How many workspaces should you identify?

Choose the correct answer

- Five
- Three
- One
- Six

---

### Answer:
- Three

You should recommend three log analytics workspaces. Log analytics workspaces are used by Azure Monitor to store log data. Log analytics workspace is an Azure resource used to aggregate and store the data, provide administrative boundaries (data sovereignty), and define a geographic location for data storage (regional compliance requirements). In this scenario, the company operates within three Azure geographies: United States, Europe, and Asia Pacific. Therefore, to meet the requirements in this scenario, you must deploy three log analytics workspaces: one per geography.
You should not recommend one log analytics workspace. Although creating one log analytics workspace would reduce the costs and administrative overhead, it would not satisfy the requirement for data sovereignty and regional compliance requirements, as you would require one workspace per geography.

You should not recommend either five or six log analytics workspaces. Although creating four or five log analytics workspaces would provide even more data sovereignty, it would increase the costs. Regarding users requiring a comprehensive data overview, the administrative overhead would also increase.

---

### References

[Design for Azure Monitor Logs (Log Analytics) workspaces](https://learn.microsoft.com/en-us/training/modules/design-solution-to-log-monitor-azure-resources/3-design-for-log-analytics)  
[Azure geographies](https://azure.microsoft.com/en-us/explore/global-infrastructure/geographies/)  
[Making your data residency choices easier with Azure](https://azure.microsoft.com/en-us/blog/making-your-data-residency-choices-easier-with-azure/)  


---

## Q137:

Your company has several development projects planned. These include moving existing applications to the cloud and developing new applications for initial release in the cloud.
You need to identify the most appropriate compute service for each project requirement.
Which compute service should you recommend? To answer, drag the appropriate compute service to each requirement. A compute service may be used once, more than once, or not at all.

Drag and drop the answers

Azure App Service
Azure Batch
Azure Functions
Azure Kubernetes Service (AKS)
Azure VM

Requirement | Compute service

To migrate a web app and API from an on-premises deployment to the cloud when the app cannot be containerized
Azure App Service

To develop a new app that supports an event-driven workload based on short-lived processes
Azure Functions

To develop a microservices architecture app that requires full orchestration and .NET integration
Azure Kubernetes Service (AKS)

To migrate an on-premises, cloud-optimized, high- performance computing (HPC) workload application
Azure Batch

---

### Answer:

You should use Azure App Service when you need to migrate a web app and API from an on-premises deployment to the cloud when the app cannot be containerized. App Service is a managed service for hosting web apps, mobile app back ends, RESTful APIs, or automated business processes. It is an appropriate hosting option for web apps and APIs that cannot be containerized. However, App Service also allows you to run the applications as containers too.
Azure Functions is the most appropriate solution when you need to develop a new app that supports an event-driven workload based on short-lived processes. Azure Functions lets you run small pieces of code without having to worry about application infrastructure. Azure Functions can respond to Event Hubs, Event Grid, Blob events, and queue messages, among other triggers.
You should choose Azure Kubernetes Service (AKS) clusters as your service option when you need to develop a microservices architecture app that requires full orchestration and .NET integration. This is a fully managed and orchestrated Kubernetes service. AKS includes support for .NET.
You should use Azure Batch when migrating an on-premises, cloud-optimized, high-performance computing (HPC) workload application. An HPC workload uses several CPU or GPU-based computers to solve complex mathematical tasks and it is best suited to Azure Batch.
Azure VM is not the best solution for any of the development and deployment requirements. You would choose this as your solution when you needed full control over the application environment and operating system configuration.

---

### References

[Choose an Azure compute service](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree)  
[Run a custom container in Azure](https://learn.microsoft.com/en-us/azure/app-service/quickstart-custom-container?tabs=dotnet&pivots=container-linux-azure-portal)  
[Choosing Azure compute platforms for container-based applications](https://github.com/dotnet-architecture/eBooks/blob/1ed30275281b9060964fcb2a4c363fe7797fe3f3/current/modernize-with-azure-containers/Modernize-Existing-.NET-applications-with-Azure-cloud-and-Windows-Containers.pdf)  
[Azure Functions overview](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview?pivots=programming-language-csharp)  
[App Service overview](https://learn.microsoft.com/en-us/azure/app-service/overview)  
[What is Azure Batch?](https://learn.microsoft.com/en-us/azure/batch/batch-technical-overview)  
[What is Azure Kubernetes Service?](https://learn.microsoft.com/en-us/azure/aks/what-is-aks)  

---

## Q136:

You are developing a solution in which alerts and activity logs that are collected from Azure resources are automatically analyzed by a third-party security information and event management (SIEM) system. The analysis must occur in real time.
You need to provide a solution that integrates with the third party.
Which Azure service should you use?
Choose the correct answer
Azure Storage account
Azure Log Analytics
Azure Event Hubs
Azure Monitor
Explanation
You should use Azure Event Hubs to integrate with a third-party system such as SumoLogic or Splunk. The events that are coming from activity logs or alerts can be redirected to an event hub. The third-party system can then read data from the event hub.
You should not use an Azure Storage account because you would not be able to integrate it with a third- party system to perform real-time data analysis. The storage account would be helpful for historical reports.
You should not use Azure Log Analytics because you cannot integrate it with a third-party tool. You are limited to using queries to analyze the collected data.
You should not use Azure Monitor. Although you can use Azure Monitor to route data to an event hub so it can be integrated with a third-party SIEM system, Azure Monitor does not integrate directly with third-party systems. It is used to display different views of historical data.

---

### Answer:

---

### References

[What are the Microsoft Entra activity log integration options?](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/concept-log-monitoring-integration-options-considerations)  

[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)  

[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?WT.mc_id=Portal-Microsoft_Azure_Monitoring&tabs=portal)  

---

## Q135:

Your company has a Windows Workflow Foundation (WF) application that processes incoming invoices. The developers want to redesign the application so that they can use an Azure cloud service without losing the ability to design workflows graphically.
You need to recommend an Azure resource that meets the above requirements.

Which resource should you recommend?

Choose the correct answer

- Functions app
- Container Instance
- Logic app
- WebJob

---

### Answer:
- Logic app


You should recommend a logic app. A logic app is a Platform-as-a-Service (PaaS) offering that allows you to graphically design workflows.
You should not recommend a container instance. A container instance is a PaaS offering that allows you to package and deploy an application and its dependencies as a unit. Container instances do not require you to manage the operating system, and they run separately from applications in other containers. You cannot design graphical workflows with containers.
You should not recommend a Functions app. A Functions app is a PaaS offering that allows you to execute code in a variety of languages, including C#, on a schedule or via a trigger. You cannot design graphical workflows with a Functions app.
You should not recommend a WebJob. A WebJob must run in the context of an App service. WebJobs can be executed manually or on a trigger. You cannot design graphical workflows with a WebJob.


---

### References

[What is Azure Logic Apps?](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview)  

[What is Azure Container Instances?](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-overview)  

[Azure Functions overview](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview?pivots=programming-language-csharp)  

[Run background tasks with WebJobs in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/webjobs-create?tabs=windowscode)  

---

## Q134:

You are designing a high-performance computing (HPC) workload that will be used to support complex engineering projects. The workload runs multiple applications as an intrinsically parallel workload.
You need to select an application service to support the workload. The solution should manage job scheduling. You want to minimize costs related to the solution.

What should you use?

Choose the correct answer

- Azure Functions
- Azure Batch
- Azure Kubernetes Service (AKS) clusters
- Azure Virtual Machine Scale Sets

---

### Answer:
- Azure Batch

You should use Azure Batch. Azure Batch is designed to run large-scale parallel and high-performance computing (HPC) batch jobs efficiently in Azure. Azure Batch manages a pool of VMs to run the applications, installs the applications, and manages job scheduling. There is no charge for using Azure Batch. You only pay for the underlying resources used.
You should not use Azure Virtual Machine Scale Sets. Virtual Machine Scale Sets automatically scale VM images from platform or custom images. Virtual Machine Scale Sets provide a way to automate scaling to meet performance requirements, but provide no special support for parallel processing.
You should not use Azure Functions. Azure Functions are designed to let you run small pieces of code without having to worry about the underlying infrastructure. Functions are used to provide processing support for event-driven activities.
You should not use Azure Kubernetes Service (AKS) clusters. AKS clusters can be used for batch jobs, but they do not support the type of intrinsically parallel processing support required in this scenario.

---

### References

[High-performance computing (HPC) on Azure](https://learn.microsoft.com/en-us/azure/architecture/topics/high-performance-computing)  

[What are Virtual Machine Scale Sets?](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview)  

[Azure Functions overview](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview?pivots=programming-language-csharp)  

[What is Azure Batch?](https://learn.microsoft.com/en-us/azure/batch/batch-technical-overview)    

[What is Azure Kubernetes Service (AKS)?](https://learn.microsoft.com/en-us/azure/aks/what-is-aks)   

---

## Q133:

An application was originally developed and deployed on-premises for its pilot release phase. You have determined that it would be more cost-effective and provide better support for rapid upscaling and downscaling to rehost the application in the cloud. Changes to the application must be kept to a minimum.
Processing requirements and the number of users supported can vary widely. Your deployment must be able to scale up to over 300 instances throughout a region. Traffic with the instances must be load balanced. The application requires a customized Windows 2019 Server image to support it. You have tested the custom image as a VM.
You need to determine the best way to deploy the application to Azure. You want to minimize costs and reduce management overhead.

What should you do?

Choose the correct answer

- Create the deployment as a VM scale set.

- Package the application as a container application and deploy it to an AKS cluster.

- Package the application as a container application and deploy it to a container group in Azure Container Instances

- Create an Azure Resource Manager (ARM) template and a PowerShell script to deploy VM instances as required.

---

### Answer:
- Create the deployment as a VM scale set.

You should create the deployment as a VM scale set. This supports automatic scaling for VM images based on your custom image and application. VM scale sets support up to 600 images for a custom VM image and up to 1,000 VM images when based on a platform instance.

You should not create an Azure Resource Manager (ARM) template and a PowerShell script to deploy VMs as required. This would involve more management overhead than necessary, and it would quickly become difficult, if not impossible, to efficiently maintain the solution.
You should not package the application as a container application and deploy it to a container group in Azure Container Instances. This would likely require significant changes to the application and the solution would not meet the scale-out model required. Container groups are used when you want to simultaneously run multiple containers that share the same lifecycle.
You should not package the application as a container application and deploy it to an Azure Kubernetes Service (AKS) cluster. This would likely require significant changes to the application and the solution would not meet the scale-out model required. Also, you cannot use custom images with AKS cluster for the node pool.

---

### References

[Choosing Azure compute platforms for container-based applications](https://github.com/dotnet-architecture/eBooks/blob/1ed30275281b9060964fcb2a4c363fe7797fe3f3/current/modernize-with-azure-containers/Modernize-Existing-.NET-applications-with-Azure-cloud-and-Windows-Containers.pdf)  

[What are Virtual Machine Scale Sets?](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview)  

[FAQ for Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-faq)  

[Choose an Azure compute service](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree)    

[What is Azure Kubernetes Service?](https://learn.microsoft.com/en-us/azure/aks/what-is-aks)  

[Container groups in Azure Container Instances](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-container-groups)  

---

## Q132:

You manage a complex inventory management clearinghouse application named App1 that is deployed to Azure. When a new client joins clearinghouse, a new data connector must be added to App1. You note that App1 experiences measurable performance degradation as more connectors are being added.
You need to design a solution that allows new data connectors to be brought online without App1 experiencing any performance degradation.
App1 and data connectors for each client is developed as a microservice.
What should you do? For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement Yes No

Use Azure Kubernetes Service (AKS) to automatically scale your App1.
Yes

Use VMs to deploy additional Docker containers.
No

Use Azure Container Instances (ACI) to orchestrate scaling for App1.
No

---

### Answer:

You should use Azure Kubernetes Service (AKS) to automatically scale your App1. AKS features robust orchestration that allows you to deploy additional pods based on compute metrics like CPU or memory. It can auto scale depending on the usage from the data connectors.
You should not use VMs to deploy additional Docker containers. You cannot achieve auto-scale in Docker by just running additional containers. These are standalone containers and a lot of operational expertise is required to make them communicate with each other. You need to use a container orchestrator like Kubernetes to achieve the communication between the applications.
You should not use Azure Container Instances (ACI) to orchestrate scaling for App1. ACI is a basic, lightweight container solution that is used when you have isolated containers. ACI is reasonably simple to get up and running. However, it does not feature robust orchestration or advanced autoscaling compared to a more advanced orchestration solution like Kubernetes.

---

### References

[Orchestrate microservices and multi-container applications for high scalability and availability](https://learn.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/scalable-available-multi-container-microservice-applications)    


[What is Azure Kubernetes Service?](https://learn.microsoft.com/en-us/azure/aks/what-is-aks)  

[Azure Container Instances and container orchestrators](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-orchestrator-relationship)  

[What is Azure Container Instances?](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-overview) 

[Docker overview](https://docs.docker.com/guides/docker-overview/)  

---

## Q131:

You manage an Azure subscription at a technology company. You need to recommend a VM that hosts a batch processing application.
The application must analyze a very compute-intensive workload. The workload has consistent medium-to- high CPU utilization. Cost is not a major consideration.
Which VM series should you choose?

Choose the correct answer

- B-Series
- Dv3-Series
- Ev3-Series
- Fsv2-Series

---

### Answer:
- Fsv2-Series

You should choose the Fsv2-Series VM because it has a higher CPU-to-memory ratio. It is optimized for batch processing, analytics, and gaming. It can optimally run a very compute intensive workload.
You should not choose the B-Series VM because B-series are economical VMs. You should use them when
you do not need a lot of computational power. These VMs are also appropriate for workloads that vary in amount of CPU usage.
You should not use the Ev3-Series VM because this type of VM is intended for heavy in-memory operations such as SAP Hana. It can run a large in-memory business-critical workload.
You should not use the Dv3-Series VM because this type of VM is intended for relational databases, analytics, and in-memory caching. It has an optimal CPU-to-memory configuration. The Dv3-Series VM can run most of the production workload.

---

### References

[Virtual Machine series](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/)  
[Sizes for virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist)  
[Compute optimized virtual machine sizes](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist#compute-optimized)  
[Memory optimized virtual machine sizes](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist#memory-optimized)  

---

## Q130:  

Your team is planning to develop several container-based applications using microservice architecture. You want to deploy the applications so that they can run independently and communicate with each other easily. Hosting these applications should require minimal operation effort.
You need to choose a service to host the applications.

Which service should you use?

Choose the correct answer

- Azure Container Instance
- Web App for Containers
- Azure Service Fabric
- Azure Kubernetes Service (AKS)

---

### Answer:
- Azure Kubernetes Service (AKS)

You should use Azure Kubernetes Service (AKS). AKS is a managed Kubernetes service to orchestrate container applications. Applications deployed in AKS can run independently and communicate with each other easily. The orchestration capabilities allows easy service discovery.
You should not use Azure Container Instance (ACI). ACI is good for running container-based applications that are standalone. Since it does not have orchestration capabilities, it will be difficult to achieve easy network communication between the multiple applications.
You should not use Web App for Containers. This uses Azure App Service and provides the platform for deploying your applications in form of containers. However, similar to ACI, it lacks orchestration capabilities.
You should not use Azure Service Fabric. Service Fabric is also a good service that allows reliable microservice deployment and lifecycle. However, it is a little difficult to develop and manage applications for Service Fabric compared to AKS.

---

### References

[What is Azure Kubernetes Service?](https://learn.microsoft.com/en-us/azure/aks/what-is-aks)  
[What is Azure Container Instances?](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-overview)   
[Web App for Containers](https://azure.microsoft.com/en-us/products/app-service/containers/?activetab=pivot:deploytab)  
[Overview of Azure Service Fabric](https://learn.microsoft.com/en-us/azure/service-fabric/service-fabric-overview)  

---

## Q129:

You plan to create a virtual machine (VM) in Azure for a non-production solution. The VM will normally handle workloads with low to moderate CPU usage, with some short usage peaks.
You need to choose the most cost-effective VM series for this solution.

What VM series should you choose?

Choose the correct answer

- Av2-series
- B-series
- Dv2-series
- DCsv2-series

---

### Answer:
- B-series

You should choose the B-series. In addition to being low-cost, the B-series VM sizing is burstable, which means it can temporarily use more computing power without affecting costs by using CPU credits. The VM can run with this increased computing power until it has available CPU credits. Once the CPU usage reverts to low to moderate, the VM starts to accumulate CPU credits again.
You should not choose the Av2-series, Dv2-series, or DCsv2-series because they are not burstable. Also, the Dv2 and DCsv2-series are considerably more expensive than the B-series.

---

### References

[General purpose virtual machine sizes](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist#general-purpose)   

[Sizes for virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist)  

[B-series burstable virtual machine sizes](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-b-series-burstable)  

---

## Q128:

You operate distributed Azure Cosmos DB for a product catalog for suppliers of automotive parts worldwide. Attributes for specific parts are changed by the development department using the primary replica of Azure Cosmos DB in Europe and replicated to all secondary read-only replicas in other regions.
You need to recommend a high availability solution to guarantee returning the most recent version of the data in a region, while providing flexibility in managing replication lag and network latency.

Which consistency level should you recommend?

Choose the correct answer

- Bounded staleness
- Consistent prefix
- Strong
- Eventual

---

### Answer:
- Bounded staleness

You should recommend bounded staleness consistency. Bounded staleness consistency allows you to set a limit on how stale the data can be in a region compared to another region. This provides a balance between strong consistency and performance considerations. It ensures that the lag of data between any two regions is less than a specified amount, which can be defined by a particular number of versions ("K" versions) or by a time interval ("T" time intervals), whichever is reached first. In this scenario, bounded staleness consistency aligns with the need for returning the most recent version of the data in the region while allowing some flexibility to manage replication lag and network latency. It strikes a balance between immediate synchronization and the practical constraints of distributed systems.
You should not recommend strong consistency. Strong consistency could ensure the most recent version of the data, but it might not provide the desired flexibility in managing latency. Strong consistency ensures that, after a write operation is acknowledged as successful, all subsequent read operations from any region or replica will return the same, up-to-date data. In other words, strong consistency guarantees that reads will never return stale or outdated data.
You should not recommend eventual consistency. Eventual consistency might offer flexibility but does not guarantee the most recent version. In an eventually consistent system like Cosmos DB, there is no strict synchronization of data between all replicas immediately after a write operation. Instead, updates are propagated asynchronously, and different replicas may temporarily have slightly different versions of data.
You should not recommend consistent prefix consistency. Consistent prefix consistency might also ensure some level of consistency but does not provide the same control as bounded staleness for managing replication lag. Consistent prefix consistency guarantees that a client will observe a series of writes in a linear order, where each write is consistent with respect to all previous writes, but it does not guarantee that the client will see the most recent write. In other words, it ensures that writes are seen in a specific order, even though they might not be current up to the millisecond.

---

### References

[Consistency levels in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/consistency-levels)  
[Databases, containers, and items in Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/resource-model)  
[Classify your data](https://learn.microsoft.com/en-us/training/modules/choose-storage-approach-in-azure/2-classify-data)   

---

## Q127:

CompanyA operates an online shop with the majority of customers in the US and Asia. You plan to move your on-premises online shop web application to Azure and deploy it in the East US and West US regions. Your solution must meet the following requirements:
East US region is the primary region where all the customer requests should go.
If the East US region fails, the web application in the West US region must take over the operation automatically.
You need to recommend a solution for the online shop web application.
What should you include in the recommendation? To answer, select the appropriate solutions from the answer area.

Choose the correct options

Routing configuration:
Azure Traffic Manager

Routing method:
Priority routing

---

### Answer:

You should include Azure Traffic Manager in your routing configuration. Azure Traffic Manager provides a load-balancing solution to route your user requests to multiple service points, based on DNS query and DNS response. Unlike other load balancing solutions in Azure, Traffic Manager operates on layer 7 of the Open Systems Interconnection (OSI) Reference model and can load balance traffic across geographical regions. In this scenario, the customer requests to implement a high availability solution between the two regions.
You should not include Azure Load Balancer in your routing configuration. Azure Load Balancer provides a load-balancing solution to route your user requests to multiple service points. Unlike Azure Traffic Manager, Azure Load Balancer operates on layer 4 of the OSI reference model within your virtual network. The load balancing is accomplished based on an IP address. As the requirement, in this scenario, is to load balance traffic between Azure regions, Azure Load Balancer does not meet the requirement.
You should not include round-robin load balancing in your routing configuration. Round-robin is a method to distribute a load evenly among multiple endpoints of a single service. For example, you can assign to a single URL of a web application multiple service endpoints. Every request to the URL will be routed by round-robin to the next IP address of that service endpoint. In this scenario, customers should primarily access the service in their region. However, the round-robin load-balancing method cannot differentiate where the request comes from.
You should include Priority routing in your routing method. Azure Traffic Manager provides six routing methods, depending on your architecture and needs. The routing methods can be focused on service performance or service availability. In this scenario, service availability is requested. Priority routing is a high availability method. By configuring this method, you can define a primary (highest priority) resource in one region and one or multiple secondary (fail over) resources in another region. In this scenario, you have to se up East US as the primary and West US as the secondary endpoint.
You should not include Weighted routing in your routing method. Weighted routing is an Azure Traffic Manager routing method to use a predefined weight for the endpoints of service to distribute traffic. Traffic can be distributed evenly if all the endpoints have the same weight, or prioritized if some endpoints' weight is lower. Weighted routing cannot differentiate in terms of in which region a resource resides.
You should not include Multivalue routing in your routing method. Multivalue is an Azure Traffic Manager routing method to query all available IP addresses of healthy endpoints and distribute traffic among them evenly. In this scenario, a load balancer must be able to differentiate in terms of in which Azure region a resource resides.

---

### References

[What is Traffic Manager?](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-overview)  
[Quickstart: Create a Traffic Manager profile using the Azure portal](https://learn.microsoft.com/en-us/azure/traffic-manager/quickstart-create-traffic-manager-profile)  
[Traffic Manager routing methods](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods)  
[Disaster recovery using Azure DNS and Traffic Manager](https://learn.microsoft.com/en-us/azure/reliability/reliability-traffic-manager)  
[What is Azure Load Balancer?](https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-overview)  
[Windows Network Architecture and the OSI Model](https://learn.microsoft.com/en-us/windows-hardware/drivers/network/windows-network-architecture-and-the-osi-model)  
[Use DNS Policy for Application Load Balancing](https://learn.microsoft.com/en-us/windows-server/networking/dns/deploy/app-lb)  

---

## Q126:

Your company wants to maintain Activity log and resource log information for auditing and static analysis. The data must remain available for up to six months.
You need to identify an appropriate storage and maintenance solution.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement Yes No

You can use the Azure Monitor menu in the Azure portal to configure archive storage.
Yes

You can create an Azure storage account for log storage in the same subscription as the resource sending the logs only.
No

You can configure Blob storage policies to require time-based retention.
Yes

---

### Answer:

You can use the Azure Monitor menu in the Azure portal to configure archive storage. It is necessary to configure an Azure storage account for archive storage because Activity log entries are only maintained for 90 days by Azure. You should create a storage account separate from other types of data so you can better control access to the data.
You can create an Azure storage account for log storage in the same subscription or a different subscription as the resource sending the logs. The user who configures the storage must have appropriate RBAC access to both subscriptions.
You can configure Blob storage policies to require time-based retention. This gives you a way to prevent data from being deleted or modified during the retention period.

---

### References

[Azure Monitor activity log](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings)  
[Configure immutability policies for blob versions](https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-policy-configure-version-scope?tabs=azure-portal)  

---

## Q125:

CompanyA, which operates in East US only, is modernizing its infrastructure by moving to the cloud. The Enterprise resource planning (ERP) web application workload cannot be modified and it needs to be moved to Azure as it is.
You need to recommend a solution for migration. Your solution must meet the following requirements:

- The solution should be available if a datacenter fails.
- Cost and administrative effort must be minimized.

Which two solutions should you recommend? Each correct answer presents part of the solution.

Choose the correct answers

- Deploy a VM scale set across two availability zones.
- Deploy a web app across two regions.
- Deploy a load balancer.
- Deploy an Azure Traffic Manager profile.

---

### Answer:

You should deploy a VM scale set across two availability zones and deploy a load balancer to meet the requirements. An Azure availability zone is a physically separated location within an Azure region. Azure regions contain at least three availability zones (data centers) with their own infrastructure per availability zone (power, cooling, etc.), and they are connected with a high performance network. Because the Enterprise resource planning (ERP) web application cannot be modified, it should be deployed on an Azure VM. You should deploy a VM scale set to scale a web application on VMs across availability zones with minimized administrative effort. In this scenario, the requirement is met by deploying an ERP web application to multiple availability zones within an Azure region.
You should also deploy an Azure load balancer to route traffic automatically to the failover availability zone.
You should not deploy a web app across two regions. Since CompanyA operates only in East US and there is no interregional availability required, deploying a web app across two regions is not necessary. Deploying a web app across multiple regions also increases costs for the interregional data transfer.
You should not deploy an Azure Traffic Manager profile. Azure Traffic Manager is an Azure load balancing solution which is used to distribute traffic to multiple service endpoints across Azure regions for availability or performance purposes. Since there is no interregional availability required, deploying Azure Traffic Manager is not needed.

---

### References

[Overview of application migration examples for Azure](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/migrate/scenarios)  
[What are Azure regions and availability zones?](https://learn.microsoft.com/en-us/azure/reliability/availability-zones-overview?tabs=azure-cli) 
[What are Virtual Machine Scale Sets?](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview)  

---

## Q124:

Your company has developed an Azure web app that provides automated customer and technical support for many of its products. The web app uses Azure Blob storage to provide users with documentation and large file downloads. Many of the files are updated periodically to add vulnerability protections and bug fixes, and products are identified in the company's consumer software products.
The majority of your company's customers are located in central United States (US). You deploy multiple instances of the web app in the North Central US and South Central US Azure regions. The storage account for the primary data store is the North Central US region. The storage account is configured for locally redundant storage (LRS). There are performance concerns when users from the South Central US region download large files.

You need to modify your storage solution configuration to:

- Ensure that file updates can be applied only once.
- Enable users to download files from the same region as the web app instance they are accessing.
- Protect your data in case of a datacenter or zone-wide failure.
- Minimize costs and administrative overhead to implement and maintain the solution.

What should you do?

Choose the correct answer

- Configure your storage account for read-access geo-zone-redundant storage (RA-GZRS).
- Deploy and configure Azure File Sync.
- Implement Azure Content Delivery Network (CDN).
- Configure your storage account for geo-redundant storage (GRS).

---

### Answer:
- Configure your storage account for read-access geo-zone-redundant storage (RA-GZRS).

You should configure your storage account for read-access geo-zone-redundant storage RA-GZRS. With RA-GZRS, your data is copied across three Azure availability zones in the primary region and to one zone in an Azure paired region. The copy in the paired region is a read-only copy, but it is read-accessible and can be used as a file download source. You will only need to make updates to the primary copies as they will be propagated automatically. Because the data is replicated across two regions, you are protected against region-wide failure.
You should not configure your storage account for geo-redundant storage (GRS). In GRS, your data is only stored in a single datacenter in the primary region, and the copy in the secondary region is not read- accessible and cannot be used as a file download source.
You should not implement Azure Content Delivery Network (CDN). Azure CDN is a distributed network of servers designed to deliver web content to users. Content is cached on edge servers in point-of-presence (POP) locations that are close to end users, to minimize latency. Azure CDN facilitates content download and delivery, but it does not meet the requirements for supporting updates and protecting data in case of failure.
You should not deploy and configure File Sync. File Sync synchronizes on-premises file shares with Azure files. You must deploy Azure File Sync in the same region as the Azure File share it supports. File Sync does not meet the solution requirements.

---

### References

[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[Planning for an Azure File Sync deployment](https://learn.microsoft.com/en-us/azure/storage/file-sync/file-sync-planning)  
[What is a content delivery network on Azure?](https://learn.microsoft.com/en-in/azure/cdn/cdn-overview)  
[Cross-region replication in Azure: Business continuity and disaster recovery](https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure)  

---

## Q123:

Your company deploys a web app that streams audio and video content to users. Users require a high- bandwidth connection to the content to ensure continuous streaming. You want to set up data stores in the geographic locations nearest to high concentrations of users, which are in various Azure zones.
The data is stored in a General-purpose v2 storage account configured for zone-redundant storage (ZRS).
You need to recommend a solution to help to support continuous streaming in select locations.

What should you recommend?

Choose the correct answer

- Azure Cache for Redis
- Azure Web Application Firewall
- Geo-zone-redundant storage (GZRS)
- Azure Content Delivery Network (CDN)

---

### Answer:
- Azure Content Delivery Network (CDN)

You should recommend Azure Content Delivery Network (CDN), which is a distributed network of servers designed to deliver web content to users. CDN provides a global solution for rapidly delivering content to users. Content is maintained at strategic locations as physical nodes in different geographic locations.
You should not recommend Azure Cache for Redis. Cache for Redis is designed to improve the performance and scalability of systems that rely heavily on backend data stores by caching data locally in memory. Cache for Redis can be used as a distributed data cache, a session store, and a message broker.
You should not recommend Azure Web Application Firewall. Web Application Firewall is a feature of Azure Application Gateway and it is designed to provide centralized protection for web applications. It helps to protect web applications from web vulnerabilities and attacks without modification to back-end code.
You should not recommend Geo-zone-redundant storage (GZRS). You can change the storage redundancy from ZRS to GZRS but this does not meet the scenario's goal. GZRS helps to protect your data and ensure data recovery in case of a zone-wide failure, but it does not provide a point-of-presence for data streaming.

---

### References

[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  
[About Azure Cache for Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-overview)
[What is Azure Web Application Firewall on Azure Application Gateway?](https://learn.microsoft.com/en-us/azure/web-application-firewall/ag/ag-overview)  
[What is a content delivery network on Azure?](https://learn.microsoft.com/en-in/azure/cdn/cdn-overview)  
[Quickstart: Integrate an Azure Storage account with Azure CDN](https://learn.microsoft.com/en-in/azure/cdn/cdn-create-a-storage-account-with-cdn)  

---

## Q122:

Your company collects tenant, subscription, and resource data using Azure Monitor. The company also wants to stream this data because it is collected in real time, to a third-party monitoring solution for additional analysis.
You need to recommend the most effective method to stream monitoring data to a third-party tool.

What should you recommend?

Choose the correct answer

- Azure Service Bus
- Azure Event Hub
- Azure Logic Apps
- Azure Event Grid

---

### Answer:
- Azure Event Hub

You should recommend an Azure Event Hub solution. Event Hubs are designed to support telemetry and distributed data streaming and are the best fit to a real-time streaming solution. You would need to create an Event Hubs namespace and event hub as your monitoring data destination. You can configure an external data consumer.
You should not recommend Azure Event Grid as a solution. Azure Event Grid is designed to react to status changes and lets you integrate third-party tools to react to events without having to continually poll for event status.
You should not recommend Azure Service Bus. Azure Service Bus is an enterprise messaging system and is most commonly used to support order processing and financial transaction applications.
You should not recommend Azure Logic Apps as a real-time solution. If you had a situation where you could not directly stream to an event hub, you could have the data transferred to blob storage and then use a time-triggered Logic App to get the data from blob storage and push it as a message to the event hub.

---

### References

[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings)  
[Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/compare-messaging-services)  

---

## Q121:

You have a web site implementation with the following requirements

- Support up to 1,000 identical scale-out servers.
- Support multiple datacenters for high availability.
- Enable the servers to automatically scale up and down based on CPU utilization.

You need to design a high-availability solution that meets the requirements.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement
Yes
No

Web apps can meet the scale-out requirement.
No

Virtual machine scale sets can meet the requirement to support multiple datacenters.
Yes

Deploying the solution into availability sets would meet the reliability requirement.
No

---

### Answer:

Web apps cannot meet the scale-out requirement. Web apps can only scale to 50 instances of a web server.
Virtual machine scale sets can meet the requirement to support multiple datacenters and meet the requirement for scaling out. In addition to autoscaling and the ability to manage a group of VMS, VM scale sets also provide load balancing.
Deploying the solution into availability sets would not meet the reliability requirement. Availability sets enable you to isolate VM resources from each other when you deploy them. Availability sets do not meet the multiple datacenter requirement.

---

### References

[What are Virtual Machine Scale Sets?](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview)  
[Highly available multi-region web application](https://learn.microsoft.com/en-us/azure/architecture/web-apps/app-service/architectures/multi-region)  
[Create and deploy virtual machines in an availability set using Azure PowerShell](https://learn.microsoft.com/en-us/previous-versions/azure/virtual-machines/windows/tutorial-availability-sets)  


---

## Q120:

A company is building a mission-critical application for which downtime could lead to substantial business loss.
The company developers are seeking ways to build redundancy into the application to meet high availability goals. The company can devote considerable resources toward this objective.
You need to recommend a solution that meets the goals.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement
Yes
No

You should deploy the application across two or more regions.
Yes

You should configure database replicas in one or more secondary regions (geo-replication).
Yes

You should place VMs in more than one availability set.
No

---

### Answer:

You should deploy the application across two or more regions. During normal operations, network traffic is routed to the primary region. Should the primary region become unavailable, traffic is routed to the secondary region. This solution is expensive and may not be practical for some organizations. Because this company is willing to devote considerable resources toward achieving high availability, the extra expense and complexity of a multi-region deployment are justified.
You should configure database replicas in one or more secondary regions (geo-replication). In case of failure at the primary site, secondary databases are available for querying and failover. Azure SQL Database and Cosmos DB both support geo-replication.
You should not place VMs in more than one availability set. A single availability set, which contains two or more VMs, provides redundancy for the application. An availability set is a logical grouping that isolates VM resources from each other. If a failure occurs, only a subset of the resources is affected. Your overall solution remains operational.

---

### References

[Highly available multi-region web application](https://learn.microsoft.com/en-us/azure/architecture/web-apps/app-service/architectures/multi-region)  
[What are Azure regions and availability zones?](https://learn.microsoft.com/en-us/azure/reliability/availability-zones-overview?tabs=azure-cli)  
[Make all things redundant](https://learn.microsoft.com/en-us/azure/architecture/guide/design-principles/redundancy)  

---

## Q119:

You work for a large multi-national company that deals with highly sensitive data. Due to various regulations, the data that is collected and used by your company must remain within the country in which it is generated.
You are developing disaster recovery (DR) plans and you need to determine a strategy to geographically distribute data and backups in order to meet DR and regulatory requirements. Initially, costs are not a concern.
You need to choose the storage replication features for your company's data.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement
Yes
No

An Azure region that another region is paired with is nearly always located in the same geo-political area.
Yes

Automatic data replication via Azure geo-redundant storage (GRS) guarantees that the data remains within the same geo-political boundaries.
No

If you want to reduce data storage costs and maintain intra-region redundancy, Azure zone-redundant storage (ZRS) can be used.
Yes

---

### Answer:

With only one a single exception (Brazil South), every Azure paired region is within the same geo-political boundary.
Automatic data replication via GRS does not guarantee that the data remains within the same geo-political boundaries. In the region of Brazil South, data is not guaranteed to remain within the same geo-political boundary.
If you want to reduce data storage costs and maintain intra-region redundancy, Azure zone-redundant storage (ZRS) can be used. ZRS storage will ensure that the data is replicated to multiple data centers within a single region. By definition of the regional structure, this means that the data will remain within the same geographic boundary.

---

### References

[Cross-region replication in Azure: Business continuity and disaster recovery](https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure)  
[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  

---

## Q118:

Your company has an Azure environment. An administrator accidentally deletes a storage account named mystorageaccount1 containing important financial information. You successfully recover mystorageaccount1 and configure it to prevent accidental deletion in the future. You then execute the following command to verify your configuration and get the following output:

```
az lock list --resource
"/subscriptions/<ID>/resourceGroups/myresourcegroup1/providers/Microsoft.Storage/st orageAccounts/mystorageaccount1"
[
    {
    "id":
    "/subscriptions/<ID>/resourceGroups/myresourcegroup1/providers/Microsoft.Storage/st orageAccounts/mystorageaccount1/providers/Microsoft.Authorization/locks/AzureBackup ProtectionLock",
        "level": "CanNotDelete",
        "name": "AzureBackupProtectionLock",
        "notes": "Auto-created by Azure Backup for storage accounts registered with Recovery Services Vault. This lock guards against backup deletion due to accidental deletion of storage account.",
        "owners": null,
        "resourceGroup": "myresourcegroup1",
        "type": "Microsoft.Authorization/locks"
    }
]
```

For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement
Yes
No

Users are prevented from deleting mystorageaccount1.
Yes

Users are prevented from deleting or overwriting containers or blobs within mystorageaccount1.
No

Users are prevented from reading or modifying mystorageaccount1's configuration.
No

---

### Answer:

Users are prevented from deleting mystorageaccount1 but are permitted to read and modify its configuration. The output from the command shows that a "CanNotDelete" lock is applied to mystorageaccount1. This lock prevents users from deleting the storage account but does not restrict their ability to read or modify its configuration. So, users can still read and modify the configuration, making this statement true.
Users are not prevented from deleting or overwriting containers or blobs within mystorageaccount1. While the provided information does not specify whether any locks or restrictions are applied to containers or blobs within mystorageaccount1, only locking a storage account does not protect containers or blobs within that account from being deleted or overwritten. To protect containers or blobs within a storage account Microsoft recommends enabling container soft delete for the storage account to recover a deleted container and its contents and save the state of a blob at regular intervals.
Users are not prevented from reading or modifying mystorageaccount1's configuration. The "CanNotDelete" lock applied to mystorageaccount1 prevents users from deleting the storage account, but it does not explicitly prevent them from reading or modifying its configuration. To prevent users from modifying storage account configuration you would have to apply ReadOnly lock.

---

### References

[Apply an Azure Resource Manager lock to a storage account](https://learn.microsoft.com/en-us/azure/storage/common/lock-account-resource?tabs=portal)  
[Data protection overview](https://learn.microsoft.com/en-us/azure/storage/common/lock-account-resource?tabs=portal)  
[Enable and manage soft delete for containers](https://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-container-enable?tabs=azure-portal)  

---

## Q117:

You develop your business continuity plan for three Azure SQL databases of a mission critical application with high rate of data change. Each Azure SQL database resides in its own subscription. The application has the following availability requirements:

- Downtime: no more than one hour
- Data freshness: not older than five seconds

Your estimated costs for geo-replication are lower than the potential financial liability.
You are requested to recommend a recovery method for the Azure SQL databases to meet the requested level of RPO and RTO.

What should you recommend?

Choose the correct answer

- Point-in-time database restoration
- Geo-restore from geo-replicated backups
- Manual database failover
- Auto-failover groups
---

### Answer:
- Auto-failover groups

You should recommend auto-failover groups. Auto-failover groups in Azure SQL Database provide automatic and seamless failover to a secondary replica in case the primary replica becomes unavailable. This helps to achieve high availability and meet the specified recovery point objective (RPO) and recovery time objective (RTO) requirements. In an auto-failover group setup, you can have a primary database and one or more secondary databases located in different regions (geo-replication). Data replication is near real time, ensuring that the secondary replica is up to date. If the primary replica experiences an outage or goes offline, the failover group automatically promotes one of the secondary replicas to become the new primary database, minimizing downtime and data loss.
You should not recommend geo-restore from geo-replicated backups. Geo-restore from geo-replicated backups is a valid choice, but it might not provide as fast and automatic failover as auto-failover groups. You can perform geo-restore only on databases that reside in the same subscription, which is not the case in this scenario.
You should not recommend manual database failover. Manual database failover would require human intervention and might not meet the specified RTO and RPO requirements as effectively as an automated solution. Microsoft defines manual database failover as a failover that involves transitioning a single database to its geo-replicated secondary region by employing the unplanned mode of failover. An unplanned or forced geo-failover promptly transfers the geo-secondary into the primary role without synchronization with the original primary region. Transactions that have been committed on the original primary but have not yet been replicated to the secondary region will be lost.
You should not recommend point-in-time database restoration. Point-in-time database restoration proves valuable in recovery situations, such as addressing incidents resulting from errors or failures, rectifying issues with data loading, or recovering essential data that has been accidentally deleted. Additionally, it can be employed for the purpose of testing and auditing your database deployment.

---

### References

[Overview of business continuity with Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/business-continuity-high-availability-disaster-recover-hadr-overview?view=azuresql)  
[Restore a database from a backup in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/recovery-using-backups?view=azuresql&tabs=azure-portal)  
[Designing globally available services using Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/designing-cloud-solutions-for-disaster-recovery?view=azuresql)  
[Restore a database in Azure SQL Managed Instance to a previous point in time](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/point-in-time-restore?view=azuresql&tabs=azure-portal)  

---

## Q116:

You are administering an on-premises environment with a Hyper-V host server for hosting virtual machines. You are requested to recommend a solution to protect files and folders on your Windows machines and keep workloads running even if the whole on-premises environment stops functioning. Your solution must minimize maintenance efforts, downtime, and restore time.
Which two technologies should you recommend? Each correct answer presents part of the solution.

Choose the correct answers

- Storage Replica
- Azure Site Recovery
- High Availability (HA) clustering
- Azure Backup
- Volume Shadow Copy Service (VSS)

---

### Answer:
- Azure Site Recovery
- Azure Backup

You should recommend Azure Site Recovery. Azure Site Recovery is designed specifically for disaster recovery scenarios and can provide automated replication and failover of virtual machines to Azure in the event of on-premises failures. In the event of a site or infrastructure failure, you can fail over the virtual machines to Azure and continue operations from there. This solution provides automated replication, monitoring, and failover, making it a good choice for minimizing downtime and ensuring availability during disasters. Although it can be an excellent solution for disaster recovery, its primary focus is on ensuring that workloads remain available in case of site-level failures.
You should also recommend Azure Backup. Azure Backup is a cloud-based service that provides automated backups of your on-premises data to Azure. It allows you to schedule regular backups of files, folders, and system data from your Windows machines. The backups are stored securely in the Azure cloud, reducing the risk of data loss due to on-premises failures. In the event of a failure or data loss, you can easily restore the data from Azure Backup, minimizing downtime and restore time. Azure Backup is designed to minimize maintenance efforts as it automates the backup process and provides a user-friendly interface for managing backup policies, retention periods, and recovery operations.
You should not recommend High Availability (HA) clustering. HA clustering involves setting up multiple servers in a cluster to provide continuous availability of services and applications. If one server experiences a failure, another server in the cluster can take over its workload seamlessly. HA clustering ensures that there is minimal disruption to services in case of hardware or software failures. However, it is worth noting that HA clustering requires more maintenance effort compared to Azure Site Recovery, as it involves setting up and maintaining the cluster configuration, monitoring cluster health, and managing failover scenarios. Although it offers quick failover and availability, it may involve more complexity and administrative overhead.
You should not recommend Volume Shadow Copy Service (VSS). VSS is a technology developed by Microsoft that allows for the creation of point-in-time snapshots of files, folders, and volumes on Windows systems. These snapshots are referred to as shadow copies, and they capture the state of the files at a specific moment. VSS enables users to recover previous versions of files or folders without needing a full backup. It is often used for quickly recovering accidentally deleted or modified files, as well as for creating backups of files that are open or in use by applications. However, if the entire on-premises environment stops functioning, VSS falls short because it does not address the need for off-site replication, centralized management, and broader disaster recovery capabilities. Instead, solutions like Azure Backup and Azure Site Recovery are better suited for these scenarios, as they provide cloud-based backups, replication, and recovery options that extend beyond the limitations of a single on-premises machine and its local snapshots.

You should not recommend Storage Replica. Storage Replica is a Windows Server feature that enables block-level replication of volumes between servers or clusters for disaster recovery purposes. Although Storage Replica can be valuable for maintaining data availability in case of site failures, it might not fully address the requirement of protecting files and folders on Windows machines and ensuring availability during on-premises failures while minimizing maintenance efforts and restore time. Storage Replica solutions can involve complex setup and management, and they might be more suited for replicating entire volumes rather than individual files and folders. Additionally, they may require specific hardware and configurations to function effectively, which can impact the overall maintenance efforts and the speed of data recovery for specific files. For the given scenario, Azure Backup and Azure Site Recovery remain more aligned with the requirements and constraints described.


---

### References

[Design for Azure Site Recovery](https://learn.microsoft.com/en-us/training/modules/design-solution-for-backup-disaster-recovery/8-design-for-azure-site-recovery)  
[What is the Azure Backup service?](https://learn.microsoft.com/en-us/azure/backup/backup-overview)  
[Failover Clustering in Windows Server and Azure Stack HCI](https://learn.microsoft.com/en-us/windows-server/failover-clustering/failover-clustering-overview)  
[Volume Shadow Copy Service](https://learn.microsoft.com/en-us/windows-server/storage/file-server/volume-shadow-copy-service)  
[Storage Replica Overview](https://learn.microsoft.com/en-us/windows-server/storage/storage-replica/storage-replica-overview)  

---

## Q115:

You are designing a backup and recovery solution for your Azure Blob Storage to protect your unstructured data stored in a blob container of a storage account.
You need to provide recommendations to protect against data loss arising from the accidental deletion of the storage account.
What should you recommend for implementation?

Choose the correct answer

- Resource locks
- Blob versioning
- Blob soft delete
- Container soft delete

---

### Answer:
- Resource locks

You should recommend resource locks for implementation. Resource locks provide an extra layer of protection by preventing users from accidentally deleting or modifying critical Azure resources, such as storage accounts and their containers. By applying a resource lock to the storage account or container, you can ensure that no one can delete the resource without first removing or disabling the lock.
Although all the other options are also relevant in terms of data protection, they are more focused on the data within the blob storage rather than protecting against the accidental deletion of the entire storage account.
Blob soft delete is a feature that allows you to recover deleted blobs and their snapshots within a specified retention period. It helps in recovering accidentally deleted data within the blob storage, but it does not protect against the accidental deletion of the entire storage account.
Similar to blob soft delete, container soft delete allows you to recover deleted containers and their content within a specified retention period. This is useful for recovering accidentally deleted containers and their data, but it does not protect against the deletion of the entire storage account.
Blob versioning helps in preserving and accessing previous versions of blobs in the storage container. This is useful for tracking changes and recovering older versions of blobs, but it does not specifically protect against the accidental deletion of the entire storage account.

---

### References

[Enable soft delete for blobs](https://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-blob-enable?tabs=azure-portal)  
[Lock your resources to protect your infrastructure](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json)  
[Design for Azure blob backup and recovery](https://learn.microsoft.com/en-us/training/modules/design-solution-for-backup-disaster-recovery/4-design-for-azure-blob-backup-recovery)  
[Enable and manage soft delete for containers](https://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-container-enable?tabs=azure-portal)  
[Enable and manage blob versioning](https://learn.microsoft.com/en-us/azure/storage/blobs/versioning-enable?tabs=portal)  

---

## Q114:

You are designing a solution to host microservices in Azure Kubernetes Service (AKS) for a video streaming application.

You need to recommend a solution that meets the following requirements:

- The application should be highly available and should not be impacted by AKS platform updates.
- The cost of pulling images from the Azure Container Registry must be minimized.
- User requests must go to the nearest application deployment.

To answer, select the appropriate options from the drop-down menus.
Choose the correct options

Requirement:  
Solution

Ensure that the application is highly available and that it is not impacted by AKS platform updates:
Deploy multiple Azure Kubernetes Service (AKS) clusters in paired regions.

Minimize the cost of pulling images from the Azure Container Registry:
Enable geo-replication for container images in Azure container registries.

Ensure that user requests go to the nearest application deployment:
Use Traffic Manager to send the request based on the request region of the user.

---

### Answer:

You should deploy multiple Azure Kubernetes Service (AKS) clusters in paired regions to ensure that the application is highly available and that it is not impacted by AKS platform updates. With paired regions, the planned maintenance is applied with 24 hours of delay and in the case of region failures, recovery efforts are prioritized for a paired region. This will also protect the application from region failures and downtime due to planned or unplanned maintenance.
You should not deploy multiple Azure Kubernetes Service (AKS) clusters in different regions. Although it would provide high availability, there is no guarantee that AKS platform updates are performed at the same time to all the AKS clusters and for this reason your should deploy the clusters in paired regions.
You should not deploy the nodepool in availability sets. This would not help with any of the requirements indicated in this scenario. However, it is always best to deploy nodepool in availability sets so that it can tolerate datacenter failures within a region.
You should enable geo-replication for container images to minimize the cost of pulling images from the Azure Container Registry. This would minimize the cost for network egress when the images are pulled from secondary regions. It would also result in faster pulls and a low latency connection when pulling images.
You should not host multiple Azure container registries. This will increase your administrative effort to push the images to multiple registries. Enabling geo-replication can solve this problem and automate the process for failovers too.
You should use Traffic Manager to send the request based on the user's request region to ensure that user requests go to the nearest application deployment. With Traffic Manager, you can route the request globally to any region. Based on the routing method, you can send the user's request to the nearest geographic region, which would increase the performance and improve user experience.
You should not use Application Gateway to send the request based on the user's request region. Application Gateway is not capable of sending requests to the application based on the region. It offers other features such as TLS termination, path-based routing etc.
You should not use use Load Balancer to send the request based on the user's request region. Load balancer can only send traffic to a specific region and uses a tuple-based hashing algorithm for routing.

---

### References


[Best practices for business continuity and disaster recovery in Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/ha-dr-overview)  
[Cross-region replication in Azure: Business continuity and disaster recovery](https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure)  
[What is Traffic Manager?](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-overview)  
[Geo-replication in Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-geo-replication)  
[Azure Load Balancer algorithm](https://learn.microsoft.com/en-us/azure/load-balancer/concepts)  

---

## Q113:

You are designing a recovery solution for your company's primary IT systems, which reside in the Azure Central US region. The East US 2 region, the paired region for Central US, has been identified as the target region to be used for disaster recovery.
You need to determine which data replication strategy you should use to ensure that the core systems have the minimum Recovery Point Objective (RPO) without having to use additional services unnecessarily, while being able to failback to the primary site with the least amount of effort.
Which data replication technology or configuration should you use for each system? To answer, drag the appropriate data replication technology or configuration to each system. A data replication technology or configuration may be used once, more than once, or not at all.

Drag and drop the answers

Geo-redundant storage (GRS)
Azure Site Recovery
Availability groups
Log shipping
Zone-redundant storage (ZRS)

System | Data replication technology or configuration

Archive blob storage account:
Geo-redundant storage (GRS)

SQL Server on Azure Virtual Machine (VM):
Availability groups

Managed Disks for web-facing servers:
Azure Site Recovery

---

### Answer:

Geo-redundant storage (GRS) is the best option for the archive blob storage account because the two regions that are involved are the paired regions. Blobs will be automatically copied to the paired region if GRS storage is selected. No additional services (such as Site Recovery) must be purchased. GRS is supported for General-purpose v1, General-purpose v2, and Blob storage accounts only. With zone- redundant storage (ZRS), data is replicated to other data centers within the same region only.
You should use SQL Server availability groups for the SQL Server VMs because this does not require the purchase of additional services. Also, this provides for failback to the original site without additional work on behalf of the administrator, as would be required by log shipping.
You should use Azure Site Recovery for the web-facing servers' managed disks. You can use two types of replications for managed disks: locally-redundant storage (LRS) and zone-redundant storage (ZRS). Those two replication methods would not be enough in the event of a disaster in the Central US region. You can use Azure Site Recovery to continuously replicate the web-facing servers to the paired region and failover if necessary.

---

### References

[Cross-region replication in Azure: Business continuity and disaster recovery](https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure)  
[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  
[Fail Over to a Log Shipping Secondary (SQL Server)](https://learn.microsoft.com/en-us/sql/database-engine/log-shipping/fail-over-to-a-log-shipping-secondary-sql-server?view=sql-server-ver16)  
[Redundancy options for managed disks](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-redundancy)  
[About Site Recovery](https://learn.microsoft.com/en-us/azure/site-recovery/site-recovery-overview)  

---

## Q112:

Your company moved several databases from on-premises to Azure. You are currently using default automated database backups. 

Your current Azure database resources are shown in the table below:

| Database name | Database type                 | Performance tier |
|---------------|-------------------------------|-----------------|
| SqlDB01       | Azure SQL Database            | Basic            |
| SqlDB02       | Azure SQL Managed             | Standard  | 
| SqlDB03       | Azure SQL Managed nstancence  | Premium |
| SqlDW01       | Azure Synapse SQL pool        | Premium           |


Your company is reviewing its internal guidelines for data backup and recovery.
You need to identify the retention periods supported for each of the databases.
For each retention period, select the databases that support that retention period from the drop-down list.

SqlDB01. SqlDB02, SqIDB03, and SqlDW01
SqlDB02, and SqlDB03 only
SqlDB01, SqlDB02, and SqlDB03 only
SqlDB0, and SqlDB03 only
SqlDB02, and SqlDB03 and SqlDW01 only

Retains data for at least seven days by
default:
SqlDB01. SqlDB02, SqIDB03, and SqlDW01

Can retain data for up to 35 days:
SqlDB01, SqlDB02, and SqlDB03 only

Can be configured to retain data for up to 10 years:
SqlDB01. SqlDB02, and SqlDB03 only

---

### Answer:

A minimum seven-day default backup retention period is supported for Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse SQL pool. Therefore, all four databases will retain data for at least seven days. Azure Synapse SQL pool replaced Azure SQL Data Warehouse and is the data store for Azure Synapse Analytics.
SqlDB01, SqlDB02, and SqlDB03 can be configured for a retention period of up to 35 days without having to configure long-term retention (LTR) for the databases. This is not supported for an Azure Synapse SQL pool.
SqlDB01, SqlDB02, and SqIDB03 can all be configured for a retention period of up to 10 years after configuring LTR. This is not supported for an Azure Synapse SQL pool.
Except for the Azure SQL Database DTU-based basic tier, the same retention period is available for all performance tiers and for DTU-based and vCore-based pricing.


---

### References

[Backup and restore in Azure Synapse Dedicated SQL pool](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore)  
[Automated backups in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/automated-backups-overview?view=azuresql&tabs=single-database)  
[Long-term retention - Azure SQL Database and Azure SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/long-term-retention-overview?view=azuresql)  
[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  

---

## Q111:

Your company builds a demo environment in Azure by using an Azure Resource Manager (ARM) template. The demo environment includes three VMs that use general-purpose v2 storage accounts.
You plan to implement a disaster recovery solution for the demo environment.
The disaster recovery solution must:

- Be able to recover from a failure in a single Azure data center.
- Meet a recovery time objective (RTO) of 96 hours.
- Minimize cost.

You need to recommend a storage replication type, determine whether the Azure Site Recovery service should be included, and decide what should be done if an outage occurs.

What should you recommend? 
To answer, select the appropriate options from the drop-down menus.

Choose the correct options
ZRS | GRS | LRS
use ARM templates to re-create VMs | perform a manual fallover

You should use: [zone- redundant storage (ZRS)]
You [should not] include Azure Site Recovery.
If an outage occurs, you should [use ARM templates to re-create VMs]

---

### Answer:

You should recommend zone-redundant storage (ZRS). ZRS protects your data from single Azure datacenter failure and it is less expensive than GRS.
You should not recommend locally redundant storage (LRS). LRS protects your data from single server failure. LRS is less expensive than ZRS, but it does not offer protection against Azure datacenter failure.
You should not recommend geo-redundant storage (GRS). Although GRS protects your data from a region- wide outage, it is more expensive than ZRS.
You should not recommend Azure Site Recovery (ASR). Although ASR could be used to achieve the goal (perform failover to a secondary location), you must also minimize cost and you have a long recovery time objective (RTO) of 96 hours.
In the event of a single Azure datacenter failure, you should re-create the VMs by using ARM templates in the secondary location. The long RTO period makes this option feasible.
You should not perform a manual failover. When you perform a failover, all data in the storage account is failed over to the secondary region, and the secondary region becomes the new primary region. In this scenario, there is no secondary region.

---

### References

[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  
[General questions about Azure Site Recovery](https://learn.microsoft.com/en-us/azure/site-recovery/site-recovery-faq)  
[Create a Windows virtual machine from a Resource Manager template](https://learn.microsoft.com/en-us/azure/virtual-machines/windows/ps-template)  

---

## Q110:

You deploy a VM named vm-east to the East US region, and another VM named vm-west to the West US region.
You need to use Azure Site Recovery to replicate the VMs from East US and West US to Central US and South Central US.

In which two regions do cache storage accounts get created? Each correct answer presents part of the solution.

Choose the correct answers

- Central US
- East US
- South Central US
- West US
---

### Answer:
- East US
- West US

The cache storage accounts get created in the source region, i.e., East US and West US. Enabling VM replication for disaster recovery involves installing the Site Recovery Mobility service extension on the VM and registering it with Azure Site Recovery. The VM disk writes are sent to a cache storage account in the source region during replication. Subsequently, data is transmitted from the source to the target region, generating recovery points. During disaster recovery, failing over a VM utilizes a recovery point to restore it in the target region.
The cache storage accounts do not get created in the destination regions.

---

### References

[Tutorial: Set up disaster recovery for Azure VMs](https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-tutorial-enable-replication)  
[https://learn.microsoft.com/en-us/azure/backup/backup-azure-recovery-services-vault-overview](https://learn.microsoft.com/en-us/azure/backup/backup-azure-recovery-services-vault-overview)   
[Back up Azure VMs in a Recovery Services vault](https://learn.microsoft.com/en-us/azure/backup/backup-azure-arm-vms-prepare)  
[About Site Recovery](https://learn.microsoft.com/en-us/azure/site-recovery/site-recovery-overview)   

---

## Q109:

You are managing a data migration project. You have just discovered a new 12TB store of unstructured data. You suspect similar stores will be discovered as the project progresses. You need to design a solution for this data set that meets the following requirements:

- Data must be stored in its unfiltered, raw format.
- The solution must scale easily and at minimal cost.
- Relational and non-relational data must be supported.

Which of the following should you include in your design?

Choose the correct answer

- Azure SQL Database
- Azure Data Factory
- Azure Files
- Azure Data Lake

---

### Answer:

You should include Azure Data Lake in your design. Azure Data Lake is a massively scalable data storage solution built on Azure Blob storage. It is primarily used to store large data sets used in machine learning, real-time analytics, or other similar scenarios. Among its other features, Azure Data Lake can store structured, semi-structured, and unstructured data, and it can store all three types of data in a single repository. Azure Data Lake scales more easily and at a lower cost than other storage solutions, such as Azure Cosmos DB. Finally, Azure Data Lake can store both relational and non-relational data.
You should not include Azure Files in your design. Azure Files is the primary file storage and sharing solution on the Azure platform and is optimized for the random-access workloads common in a file-sharing scenario. In its most basic form, Azure Files is designed to replace traditional Windows Server-based file shares using Server Messaging Blocks (SMB) for Windows clients, or Network File System for *nix-based clients. Azure Files is also accessible via the Azure Files Representational State Transfer (REST) Application Programming Interface (API).
You should not include Azure Structured Query Language (SQL) database in your design. Azure SQL Database is a cloud-based database platform based on Microsoft SQL Server, a platform that has been around for decades and is designed to provide high-performance transactional data processing. While Azure SQL Database is capable of storing non-relational data, data must be structured in such a way that it can be queried using SQL. Additionally, Azure SQL Database does not scale as efficiently as Azure Data Lake.
You should not include Azure Data Factory in your design. Azure Data Factory helps to bridge data sources by facilitating data-driven workflows. It can be used to extract data from a cloud-based or on-premises data source, such as a database server, transform that data using rules and filters you specify, and then load that data into a new destination, such as Azure Synapse Analytics.

---

### References

[What is a Data Lake?](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-a-data-lake/)  
[What is Azure Files?](https://learn.microsoft.com/en-us/azure/storage/files/storage-files-introduction)  
[What is Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview?view=azuresql)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  


---

## Q108:

Your organization uses Microsoft SQL Server to support operational tasks and Azure Synapse Analytics for analytical tasks. Recently, the organization acquired a competitor and you have been asked to recommend a solution for integrating that data into Azure Synapse.
Your solution must eliminate the need for extract, transform, load (ETL) processes. Additionally, developers must be able to query the data using the internal SQL Server instance.

What should you include in your recommendation?

Choose the correct answer

- Migrate the data to Azure Files and configure the Azure REST API for queries.
- Virtualize the competitor's data through the SQL instance using PolyBase.
- Publish the data through Azure Cosmos DB using the change feed feature.
- Install a self-hosted integration runtime on the server hosting the data.

---

### Answer:
- Virtualize the competitor's data through the SQL instance using PolyBase.
You should recommend that your organization virtualize the competitor's data through the SQL instance using PolyBase. Azure Synapse Analytics provides a platform for storing, analyzing, and visualizing data at scale. In this scenario, while the newly acquired competitor's data could be loaded into a Synapse data warehouse, you have been asked to avoid using this approach. However, Synapse's analytical power can be expanded to consume external data from both non-relational and relational datasets using PolyBase technology. PolyBase makes the external data query-able by virtualizing the data through a Microsoft SQL instance. Once configured, your developers will be able to query both the internal Synapse data and the external customer data concurrently
You should not recommend that your organization install a self-hosted integration runtime on the server hosting the data. Azure Data Factory uses integration runtimes to bridge data sources by facilitating data- driven workflows. While Data Factory can be used to extract data from a cloud-based or on-premises data source, such as a database server, you have been asked to recommend a solution that allows the data to remain in its original location.
You should not recommend that your organization migrate the data to Azure Files and configure the Azure REST API for queries. Azure Files is the primary file storage and sharing solution on the Azure platform and is optimized for the random access workloads common in a file-sharing scenario. Azure Files supports
connections via Server Messaging Blocks (SMB), Network File System (NFS), or the Azure Representational
State Transfer (REST) Application Programming Interface (API). This method cannot be used to allow your
developers to query the data using the internal SQL Server instance.
You should not recommend that your organization publish the data through Azure Cosmos DB using the change feed feature. Azure Cosmos DB is designed to provide practically limitless storage at a massive scale and can support both relational and nonrelational data models, including open-source PostgreSQL, Apache Cassandra, and MongoDB. However, using Cosmos DB would require the data to be extracted, possibly transformed, and then loaded into Cosmos DB.

---

### References

[Design a data integration and analytic solution with Azure Synapse Analytics](https://learn.microsoft.com/en-us/training/modules/design-data-integration/5-solution-azure-synapse-analytics)  
[Introducing data virtualization with PolyBase](https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver16)  
[Create and configure a self-hosted integration runtime](https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory)  
[What is Azure Files?](https://learn.microsoft.com/en-us/azure/storage/files/storage-files-introduction)  
[Welcome to Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)  

---

## Q107:

Your company uses Azure Synapse Analytics to support artificial intelligence (Al) development projects. As part of its data analysis work, the Al engineering team frequently executes complex queries. These queries join and aggregate data from multiple tables. However, the Al engineers complain that as their data sets continue to grow on a daily basis, query performance has started to degrade.
You need to recommend a solution that will use caching to reduce the execution time for these queries.

What should you recommend?

Choose the correct answer

- Result set caching
- Materialized views
- Serverless SQL pool
- PolyBase integration

---

### Answer:
- Materialized views

You should recommend materialized views. In essence, a view is a named Structured Query Language (SQL) statement that you can query and join like a table. Unlike a table, however, when a view is called, the underlying query is executed and the results tabulated. This process requires additional processing time. On the other hand, a material view is executed, and the results are stored in the database, just like a table. The result is that queries that reference the view run faster.
You should not recommend a serverless SQL pool. Serverless SQL pool is a cloud-based Azure service that provides SQL server capabilities without requiring the underlying infrastructure. Every Azure Synapse Analytics workspace includes a default serverless SQL pool you can use to query the data you use in Synapse Analytics.
You should not recommend result set caching. This feature is offered by dedicated SQL pools, but it is designed for relatively static data. In this scenario, the base tables being queried are growing on a daily basis.
You should not recommend PolyBase integration. PolyBase is used to integrate external data with Azure Synapse Analytics. It is not used to cache query results or improve query performance.

---

### References

[Azure Synapse SQL architecture](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture)  
[Performance tune with materialized views](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-materialized-views)  
[What is dedicated SQL pool (formerly SQL DW) in Azure Synapse Analytics?](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is) 
[Serverless SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview)  
[Introducing data virtualization with PolyBase](https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver16)  

---

## Q106:

You are part of a team that uses Azure Stream Analytics to collect and analyze sensor data in real time. Recently, you have discovered that the incoming data has frequent value spikes that are skewing results.
You need to identify which Azure Stream Analytics feature to use to analyze this data and perform calculations based on time intervals.

Which feature should you use?

Choose the correct answer

- Date and time functions
- Aggregate functions
- Windowing functions
- Array functions

---

### Answer:
- Windowing functions

You should use windowing functions. Azure Stream Analytics is used to ingest, process, and analyze streaming data in real time. This is useful when real-time decision making is needed. For example, an e- tailer may want to analyze how customers interact with its website to provide real-time purchase recommendations. Streaming data can come from a variety of sources, including transaction processing systems, Internet of Things (IoT) devices, or mobile devices. Stream Analytics provides a variety of functions for performing this analysis, and windowing functions allow streaming data to be analyzed within a specific time window. For example, you could use a windowing function on streaming environmental sensor data to calculate the average temperature for every sixty-minute interval.
You should not use aggregate functions. Aggregate or group functions are used to analyze a set of values and return a single scalar value, such as a minimum or maximum value in a data set. Aggregate functions can be used with windowing functions, as in the example above where the average temperature is calculated but cannot group data into time intervals on their own.
You should not use date and time functions. These functions are used to manipulate and format date or time values. For example, you could use the MONTH function to extract the month from a date-time value.
You should not use array functions. Arrays are collections of items or elements and can be loosely compared to spreadsheets or database tables, and array functions are used to manipulate arrays. For example, you could use the GetArray Element function to return the third element in an array of three or more elements.

---

### References

[Windowing functions (Azure Stream Analytics)](https://learn.microsoft.com/en-us/stream-analytics-query/windowing-azure-stream-analytics)  
[Aggregate Functions (Azure Stream Analytics)](https://learn.microsoft.com/en-us/stream-analytics-query/aggregate-functions-azure-stream-analytics)  
[Date and Time Functions (Azure Stream Analytics)](https://learn.microsoft.com/en-us/stream-analytics-query/date-and-time-functions-azure-stream-analytics)  
[Array Functions (Stream Analytics)](https://learn.microsoft.com/en-us/stream-analytics-query/array-functions-stream-analytics)  

---

## Q105:

You are designing an Azure SQL deployment to support two business intelligence applications, App1 and App2. The App1 database must be hosted using a serverless compute service and support databases up to 100 TB in size. The App2 database must use a dedicated database engine and support the ability to move into or out of an elastic pool.
Your solution must minimize management overhead and optimize cost.
Which two options should you include in your design? Each correct answer presents part of the solution.

Choose the correct answers

- Use the single database resource type for App2.
- Use an instance pool for App1.
- Use the single instance feature for App2.
- Use the Hyperscale service tier for App1.
- Use SQL Server on an Azure Virtual Machine for App2.

---

### Answer:
- Use the single database resource type for App2.
- Use the Hyperscale service tier for App1.

You should use the Hyperscale service tier for App1 and the single database resource type for App2. The Azure Structured Query Language (SQL) Database platform provides three products as part of its SQL Server-based offerings: Azure SQL Database, Azure SQL Managed Instance, and SQL Server on Azure Virtual Machines (VMs). The Azure SQL Database single database service provides serverless compute and, when coupled with the Hyperscale service tier, supports databases up to 100TB in size. Databases created using the single database approach can also move into or out of elastic pools. This allows you to enhance database scalability while also optimizing costs.
You should not use the single instance feature for App2. Single instance is a feature of the SQL Managed Instance product and is the most commonly used Azure SQL product. While SQL Managed Instance supports most of the features included in SQL Server, it does not meet the requirements stated in the scenario, including serverless compute or elastic pools.
You should not use SQL Server on an Azure Virtual Machine for App2. This product is used to provide a native, Windows-based SQL Server environment. Like SQL Managed Instance, this approach does not support serverless compute or elastic pools.
You should not use an instance pool for App1. An instance pool can be used to pre-provision SQL resources, which can help to optimize costs. However, instance pools are part of the SQL Managed Instance product and do not support serverless compute or elastic pools.

---

### References

[What is Azure SQL?](https://learn.microsoft.com/en-us/azure/azure-sql/azure-sql-iaas-vs-paas-what-is-overview?view=azuresql)  
[Azure SQL Database Hyperscale FAQ](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-frequently-asked-questions-faq?view=azuresql)  
[What is a single database in Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-overview?view=azuresql)  
[Design for SQL Server on Azure Virtual Machines](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/4-design-for-sql-server-azure)  
[What is an Azure SQL Managed Instance pool (preview)?](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/instance-pools-overview?view=azuresql)  

---

## Q104:

You have implemented a data analysis solution utilizing Azure Synapse Analytics. Recently, your Al engineering team has implemented Apache Spark in one of the machine learning (ML) pipelines and wants to query the external Spark tables from the Synapse workspace.
You need to recommend a solution that allows engineers to analyze data from the Spark tables using T-SQL without requiring deploying or managing clusters or other infrastructure.

What should you include in your recommendation?

Choose the correct options
Utilize the built-in SQL pool | Create a dedicated SQL Pool | Implement Azure SQL DB
Configure Private Link | Configure Azure Synapse Link | Configure PolyBase

To allow users to analyze data using T-SQL, you should:
Utilize the built-in SQL pool.

To allow users to query external Spark tables, you should:
Configure Private Link.

---

### Answer:

To allow users to analyze data using T-SQL, you should utilize the built-in Structured Query Language (SQL) pool. Azure Synapse Analytics provides a platform for storing, analyzing, and visualizing data at scale. This process is configured and managed using an Azure Synapse Analytics workspace, and every workspace includes a serverless SQL pool. One benefit of this approach is that no additional infrastructure is required, and you can query your data using T-SQL syntax. The serverless SQL pool scales automatically as needed, and you are only charged for the queries you run without the need to worry about instance utilization, storage, or other concerns.
To allow users to query external Spark tables, you should configure Private Link. Private Link provides secure connectivity between an Azure virtual network and Microsoft services such as Azure Synapse Analytics. In addition to facilitating connectivity between Synapse and your external Apache Spark tables, Private Link enhances security by ensuring your data never leaves Microsoft-owned network infrastructure.
To allow users to analyze data using T-SQL, you should not create a dedicated SQL pool or implement Azure SQL Database. While valid for data storage in this scenario, both options require supporting infrastructure.
To allow users to query external Spark tables, you should not configure PolyBase or Azure Synapse Link. PolyBase is used to integrate external data with Azure Synapse Analytics; however, it does not support querying Apache Spark tables. Similarly, Azure Synapse Link is designed to support integration with SQL or Cosmos DB databases, not Apache Spark tables.

---

### References

[Serverless SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview)  
[Connect to your Azure Synapse workspace using private links](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/how-to-connect-to-workspace-with-private-links)  
[Private Link](https://azure.microsoft.com/en-us/products/private-link)  
[What is Azure SQL?](https://learn.microsoft.com/en-us/azure/azure-sql/azure-sql-iaas-vs-paas-what-is-overview?view=azuresql)  
[Introducing data virtualization with PolyBase](https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver16)  
[What is Azure Synapse Link for SQL?](https://learn.microsoft.com/en-us/azure/synapse-analytics/synapse-link/sql-synapse-link-overview)  

---

## Q103:

You support a development team that utilizes Azure Databricks for machine learning (ML) tasks. The team uses Delta Live Tables for data processing and has asked you to recommend a solution that will allow members from other teams to query their Delta Live Tables using Databricks SQL.
Which two tasks could the team perform to publish the tables to the Hive metastore? Each correct answer presents a complete solution.

Choose the correct answers

- Create a shared access signature (SAS) for the target tables.
- Set the target value in a JSON configuration file.
- Specify a schema name in the pipeline settings.
- Define a catalog where the pipeline can persist tables.
- Configure strong consistency for the database.

---

### Answer:
- Set the target value in a JSON configuration file.
- Specify a schema name in the pipeline settings.

The team should specify a schema name in the pipeline settings or set the target value in a JSON configuration file. Azure Databricks is an Apache Spark based analytics platform that can be used for tasks such as machine learning (ML). Databricks defines analytical workflows using pipelines, which can include data ingestion, transformation, analysis, and visualization tasks, and Delta Live Tables provide an analysis framework to support this task. By default, the data in a pipeline can only be accessed by task in the pipeline itself. To make this data accessible outside the pipeline, you can publish it to a Hive metastore by specifying a target schema in the pipeline settings, or by setting the target value in a JavaScript Object Notation (JSON) configuration file.
The team should not create a shared access signature (SAS) for the target tables. A SAS allows you to implement granular security controls for Azure Storage resources including files, blobs, tables, and other storage constructs. When a SAS is defined, a Uniform Resource Identifier (URI) is created and is associated with the permissions, expiry, and other restrictions you have configured for the SAS. You can then share this URI with internal and external users without compromising the keys associated with your storage account.
The team should not configure strong consistency for the database. Azure Cosmos DB uses consistency levels to define how effectively a database engine can return data that is the most up-to-date. Azure Cosmos DB supports five consistency levels, with the strong consistency level guaranteeing that all reads will return the most recently committed write.
The team should not define a catalog where the pipeline can persist tables. This process would be used to make the tables available via the Unity Catalog. Delta Live Tables can be published to the Hive metastore or shared using the Unity Catalog, but not both.

---

### References

[What is Azure Databricks?](https://learn.microsoft.com/en-us/azure/databricks/introduction/)  
[Publish data from Delta Live Tables pipelines to the Hive metastore](https://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/publish)  
[Create shared access signatures](https://learn.microsoft.com/en-us/training/modules/configure-storage-security/3-create-shared-access-signatures)  
[Choose the right consistency level](https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/5-choose-cosmos-db-consistency-level)  
[Use Unity Catalog with your Delta Live Tables pipelines](https://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/unity-catalog)  

---

## Q102:

You are designing a data migration project that will move 1TB of data from multiple on-premises servers, FILESRV_HR and ARCHIVE_SRV1, to Azure Storage. To facilitate this process, you plan to utilize Azure Data Factory. You must now outline the requirements for self-hosted integration runtimes.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

You must configure two self-hosted integration runtimes, one each for FILESRV_HR and ARCHIVE_SRV1.
No

You must install each self-hosted integration runtime on a separate Windows machine.
Yes

You must install a self-hosted integration runtime on FILESRV_HR and on ARCHIVE_SRV1.
No

---

### Answer:

You are not required to configure two self-hosted integration runtimes, one each for FILESRV_HR and ARCHIVE_SRV1. Azure Data Factory is used to bridge data sources by facilitating data-driven workflows. This means that you can extract data from a cloud-based or on-premises data source, such as a database server, transform that data using rules and filters you specify, and then load that data into a new destination, such as Azure Synapse Analytics. Azure Data Factory uses integration runtimes to facilitate and orchestrate this data movement.
In this scenario, you are using the self-hosted integration runtime and a single runtime can be used for multiple data sources. Reasons for adding additional runtimes might include improving processing time by running multiple pipelines in parallel, but this is not required.
You must install each self-hosted integration runtime on a separate Windows machine. The runtime only runs on Windows-based computers, and you can only run a single runtime on each machine.
You are not required to install a self-hosted integration runtime on FILESRV_HR and on ARCHIVE_SRV1. The runtime can live on any on-premises, Windows-based server. It does not need to reside on the data source itself.

---

### References

[Create and configure a self-hosted integration runtime](https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Azure Data Factory](https://azure.microsoft.com/en-us/products/data-factory)  

---

## Q101:

You are planning a new VM that will run a SQL Server instance. You identify the following requirements:
A database named DB1 must support up to 15000 input/output operations per second (IOPS) and it requires 1 TB of disk space.
The tempdb database requires 10 GB of disk space.
DB1 and tempdb need to be hosted on different disks.
You need to specify the disk type for each database. The solution must support the smallest possible VM and must minimize costs.
Which disk type should you specify for each database? To answer, drag the appropriate disk type to each database. A disk type may be used once, more than once, or not at all.

Drag and drop the answers

Local storage
Premium SSD
Standard HDD
Standard SSD
Ultra disk

Database | Disk type
DB1: Premium SSD
tempdb: Local storage

---

### Answer:

You should specify Premium SSD for DB1 because it is the only option that supports more than 15000 IOPS and fulfills the requirements of minimizing the VM size and costs.
You should specify local storage for tempdb because this is the fastest available storage, and it has no cost. The downside of using local storage is that the data would not be persisted if the VM restarted, which is not a problem for tempdb.
You should not specify Standard HDD or Standard SSD disks because they do not meet the performance requirements.
You should not specify Ultra SSD disks because they are more expensive than Premium storage.

---

### References

[Checklist: Best practices for SQL Server on Azure VMs](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-practices-checklist?view=azuresql)  
[Choose drives for Azure Stack HCI and Windows Server clusters](https://learn.microsoft.com/en-us/azure-stack/hci/concepts/choose-drives)  
[Using Azure ultra disks](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-enable-ultra-ssd?tabs=azure-portal)  
[Azure managed disk types](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-types)  

---

## Q100:

You plan to migrate company data to an Azure storage account.
The company policy states that company data must be encrypted by using the company's own encryption key.
You need to identify the Azure storage account services that can be used to meet the company requirements for storing the data.
Which storage account services can you use?

Choose the correct answer

- Azure Blob storage, Azure Files and Azure Table storage only
- Azure Blob storage and Azure Files only
- Azure Blob storage only
- Azure Blob storage, Azure Files, Azure Table storage and Azure Queue storage

---

### Answer:
- Azure Blob storage, Azure Files, Azure Table storage and Azure Queue storage

Storage service encryption protects your data at rest. Azure Storage encrypts your data while it is written in Azure datacenters, and automatically decrypts it for you when you access it.
You can use all Azure Storage account services with a custom encryption key. By default, data is encrypted using Microsoft Managed Keys for Azure Blob storage, Azure Table storage, Azure Files and Azure Queue storage. You may bring your own custom key for encryption for Azure Blob storage and Azure Files services. To use a custom encryption key for Azure Table storage and Azure Queue storage, you need to create a storage account that uses a key scoped to the account instead a key scoped to the services.

---

### References

[Azure encryption overview](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-overview)  
[Create an account that supports customer-managed keys for tables and queues](https://learn.microsoft.com/en-us/azure/storage/common/account-encryption-key-create?tabs=portal)  
[Azure Storage encryption for data at rest](https://learn.microsoft.com/en-us/azure/storage/common/storage-service-encryption)  

---

## Q099:

You have deployed an application that stores non-relational graph data using Azure SQL databases. To enhance availability, you have configured active geo-replication for each database. You are now responsible for designing a disaster recovery strategy for the application.
You need to recommend a solution, which, in the event of a regional failure, must meet the following requirements:

- The recovery time objective (RTO) should not exceed 30 seconds.
- The recovery point objective (RPO) should not exceed 5 seconds.

Which of the following disaster recovery options should you include in your recommendation?

Choose the correct answer

- Zone-redundant storage (ZRS)
- Manual database failover
- Auto-failover groups
- Geo-restore

---

### Answer:
- Manual database failover

You should include manual database failover in your disaster recovery recommendation. As you have already configured active geo-replication, a continuously synchronized replica has been created. These replicas can be placed in the same region as the primary database, but you can increase availability by placing replicas in different regions. Once configured, transaction log data, which contains committed transactions, are automatically propagated to the replicas with latencies typically measured in milliseconds. This will allow you to meet the recovery point objective (RPO) requirement, which stipulates that only data written in the last five seconds can be lost and all other data should be restored. The only downside of this approach is that the database failure must be detected by a human or other monitoring tool and the failover initiated manually. This is because automatic geo-failover waits at least one hour to trigger to prevent premature failover.
You should not include geo-restore in your disaster recovery recommendation. This is because geo-restores rely on geo-replicated backups, which does not use the same mechanism as geo-replication. A geo- replicated backup can take up to an hour to complete and then replicate. Using this method, your RPO will be at least one hour. Additionally, the recovery time objective (RTO), which is the time it takes to perform the restore, can be up to 12 hours using this method.
You should not include auto-failover groups in your disaster recovery recommendation. Auto-failover groups use the same replica architecture and mechanisms the correct answer option. However, as mentioned, Azure waits a least one hour to trigger an auto-failover. This means that while you could meet your RPO objective, you will not meet your RTO objective of 30 seconds.
You should not include zone-redundant storage (ZRS) in your disaster recovery recommendation. ZRS provides storage options for geo-redundant backups and synchronizes database backups to three different availability zones in the primary region. As already discussed, geo-restores based on geo-redundant backups have an RPO of at least one hour and an RTO of up to 12 hours.

---

### References

[Overview of business continuity with Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/business-continuity-high-availability-disaster-recover-hadr-overview?view=azuresql)  
[Restore a database from a backup in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/recovery-using-backups?view=azuresql&tabs=azure-portal)  
[Auto-failover groups overview & best practices (Azure SQL Database)](https://learn.microsoft.com/en-us/azure/azure-sql/database/failover-group-sql-db?view=azuresql)  
[Automated backups in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/automated-backups-overview?view=azuresql)  
[Active geo-replication](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql)  

---

## Q098:

You have been assigned a project that includes increasing the protection of files and blobs that store sensitive information. You have decided to use shared access signatures (SAS) with your Azure storage account to complete this task. Your design stipulates that your organization will implement account-level SAS. As part of this configuration, you have specified that access will only be allowed using approved protocols. Additionally, each access grant will be scoped with an expiration date that is based on the sensitivity of the data stored in the target location.

For each of the following statements, select Yes if the statement is true. Otherwise, select No.

For each SAS, you should limit access to applications using secure protocols such as HTTPS and SSH.
No

For each SAS, you should configure files and blobs using a single SAS.
Yes

For each SAS, you should specify an expiration date and time zone- specific time.
Yes

---

### Answer:

For each shared access signature (SAS), you should not limit access to applications using secure protocols such as Hypertext Transfer Protocol Secure (HTTPS) and Secure Shell (SSH). An SAS provides you with two protocol options: you can restrict access to HTTPS only, or to HTTPS and Hypertext Transfer Protocol (HTTP). HTTP was the primary protocol used on the internet for several decades. However, most modern browsers and websites primary use HTTPS as this provides data confidentiality. SSH, like HTTPS, also encrypts data as it traverses a network. However, SSH is not supported as a configured SAS protocol.
A SAS allows you to implement granular security controls for Azure Storage resources including files, blobs, tables, and other storage constructs. When an SAS is defined, a Uniform Resource Identifier (URI) is created and is associated with the permissions, expiry, and other restrictions you have configured for the SAS. You can then share this URI with internal and external users without compromising the keys associated with your storage account.
For each SAS, you should configure files and blobs using the same SAS. An SAS can be created to protect any combination of files, blobs, tables, or queues.
For each SAS, you should specify an expiration date and time zone-specific time. This allows you to limit access to a specific timeframe and negates the need to manually disable access to a storage resource in the future.

---

### References

[Create shared access signatures](https://learn.microsoft.com/en-us/training/modules/configure-storage-security/3-create-shared-access-signatures)  
[Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)  
[Delegate access by using a shared access signature](https://learn.microsoft.com/en-us/rest/api/storageservices/delegate-access-with-shared-access-signature)  

---

## Q097:

You are working on a project that involves migrating on-premises semi-structured and unstructured data to Azure Storage. Management has stipulated that all migrated data must be highly resilient to power, network, or other hardware-related outages. Specifically, your solution must ensure that:
Data stored in the primary region must be replicated across three availability zones.
Data must be available even if a zone becomes unavailable for any reason.
Data must be protected from regional disasters that may impact more than one zone.
Data stored in the secondary region must be redundantly stored across three copies.
You recommend that redundant storage should be used to meet these requirements.

Which two statements correctly describe the design implications of this approach?

Choose the correct answers

- Geo-redundant storage (GRS) will be used in the primary region.
- Zone-redundant storage (ZRS) will be used in the primary region.
- Zone-redundant storage (ZRS) will be used in the secondary region.
- Geo-zone-redundant storage (GZRS) will be used in the secondary region.
- Geo-redundant storage (GRS) will be used in the secondary region.

---

### Answer:
- Zone-redundant storage (ZRS) will be used in the primary region.
- Geo-zone-redundant storage (GZRS) will be used in the secondary region.

Zone-redundant storage (ZRS) will be used in the primary region. When you configure an Azure Storage account, redundant copies of your data are automatically created using locally redundant storage (LRS), which stores all replicas in the same data center. If you want to store redundant copies in remote regions, you must configure ZRS, which triggers Azure Storage to copy data replicas across three availability zones in the primary region associated with your storage account. This configuration will meet the first two requirements stated in the scenario.
Geo-zone-redundant storage (GZRS) will be used in the secondary region. Azure Storage supports two methods for enhancing durability by copying data to a secondary region: geo-redundant storage (GRS) and GZRS. The option you choose is dependent on the redundancy option configured for the primary region. In this scenario, as ZRS will be configured for the primary region, GZRS will be used to replicate data to a secondary region. Keep in mind that you cannot select the secondary region; it is selected for you. The data in the secondary region is copied using locally redundant storage (LRS), which means three replicas will be created in the same data center.
GRS will not be used in the primary region. GRS is used to configure redundancy in a secondary region.
GRS will not be used in the secondary region. If you want to use GRS in the secondary region, you should specify LRS for the primary region and then specify you want to enhance availability by using a secondary region.
ZRS will not be used in for the secondary region. ZRS is used in the primary region to create three replicas across three availability zones.

---

### References

[Design for data redundancy](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/4-design-for-data-redundancy)  
[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  
[Change how a storage account is replicated](https://learn.microsoft.com/en-us/azure/storage/common/redundancy-migration?tabs=portal)  

---

## Q096:

You are designing a storage solution that will facilitate the migration of on-premises data to the Azure cloud. As part of this migration, you have been tasked with consolidating resources where possible to minimize costs while still providing the required features and performance. Your design must address the following requirements:

- Data should be accessible via SMB or a REST API.
- The solution should be optimized for random access workloads.
- Affordability should be prioritized over low latencies or support for heavy workloads.

Which storage technology and storage tier should you include in your design?

Choose the correct options

Azure Files | Azure NetApp Files | Azure Blob Storage
Hot access tier | Pemium | Transaction Optimized

Storage technology: Azure Files
Storage tier: Hot access tier

---

### Answer:

You should recommend Azure Files as the storage technology in your design. Azure Files is the primary file storage and sharing solution on the Azure platform and is optimized for the random access workloads common in a file-sharing scenario. In its most basic form, Azure Files is designed to replace traditional Windows Server-based file shares using Server Messaging Blocks (SMB) for Windows clients, or Network File System (NFS) for *nix-based clients. Azure Files is also accessible via the Azure Files Representational State Transfer (REST) Application Programming Interface (API). Note that while *nix-based clients typically use NFS to access file shares, most modern operating systems, including Linux and MacOS, also support SMB-based file shares.
You should recommend the hot access tier in your design. You can choose from four access tiers for Azure Files, with the hot access tier providing the best combination of affordability and performance. This tier is designed for general purpose file sharing.
You should not recommend Azure Blob Storage or Azure NetApp Files as the storage technology in your design. Azure Blob Storage would be the best choice for read-heavy workloads such as storing massive datasets used in analytical applications. Azure Blob Storage does not support SMB access.
Azure NetApp files supports massive Input/Output Operations Per Second (IOPS) workloads and provides low-latency along with advanced management capabilities. Azure NetApp files does not support REST API
access.
You should not recommend the premium or transaction optimized storage tiers in your design. These tiers offer higher performance than the hot access tier but do so at an increased cost. Premium file shares are solid-state drive (SSD) backed, which offers increased performance. The transaction optimized tier is not backed by SSDs, but can still support heavy workloads. The key design decision here is that the hot access tier meets the performance requirements while also reducing costs.

---

### References

[What is Azure Files?](https://learn.microsoft.com/en-us/azure/storage/files/storage-files-introduction)  
[Design for Azure Files](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files)  
[Planning for an Azure Files deployment](https://learn.microsoft.com/en-us/azure/storage/files/storage-files-planning)  
[SMB file shares in Azure Files](https://learn.microsoft.com/en-us/azure/storage/files/files-smb-protocol?tabs=azure-portal)  

---

## Q095:

You are designing a storage plan for unstructured data using Azure Blob Storage. You have been given the following requirements:

- Availability should be 99% or higher.
- Retrieval latency should be measured in milliseconds.
- Storage costs should be minimized.

Which Azure blob access tiers should you include in your design?

Choose the correct answer

- Premium Blob Storage
- Cool access tier
- Archive access tier
- Hot access tier
- Cool access tier

---

### Answer:

You should include the cool Azure blob access tier in your design. Of the four Azure blob access tiers available, the cool access tier is the only one that meets all the requirements while also minimizing storage costs. The cool access tier is primarily designed to store data that needs to be readily available but is not accessed frequently. For example, you might have a machine learning app that analyzes a file set every other month. This tier offers the lowest storage costs for data that is kept online and is readily available.
You should not include the hot access tier in your design. Although this tier meets the availability and latency requirements in this scenario, it does so at a higher storage cost. This tier is designed to store data that is accessed frequently and as a result has lower transaction costs.
You should not include the archive access tier in your design. As the name implies, this tier is designed to hold data that has been archived. As archive data is stored offline, it can take several hours between when the data is requested and when it is available. This tier boasts the lowest storage costs.
You should not include the Premium Blob Storage tier in your design. This tier offers the same availability as the hot tier but boasts single-digit milliseconds of latency times. This means that the latency between when data is requested and when it is returned is minimized. This is accomplished by storing data on fast solid- state drives (SSDs). The Premium Blob Storage tier also incurs higher costs.

---

### References

[Design for Azure Blob Storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage)  
[What is Azure Blob storage?](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-overview)  
[Azure Blob Storage vs File Storage - What's the Difference?](https://cloudinfrastructureservices.co.uk/azure-blob-storage-vs-file-storage-whats-the-difference-pros-and-cons/)  
[Manage blob containers using the Azure portal](https://learn.microsoft.com/en-us/azure/storage/blobs/blob-containers-portal)  

---

## Q094:

You are designing a mission-critical web app that will process large amounts of data from clients located around the world. You have selected Microsoft Azure as your platform of choice, but you need to finalize the design of a backend database that minimizes latency while supporting read requests and write requests on a global scale. Your solution must meet the following requirements:
Client requests should always return the most recent committed version of data.
Requests should be serviced concurrently.
Read latency minimization should be backed by a service-level agreement (SLA).
The service must offer 99.999% availability.
Which database service and consistency level should you include in your design?

Choose the correct options

Azure Cosmos DB | Azure DB for MySQL | Azure SQL DB
Strong consistency | Bounded staleness consistency | Strong consistency

Database service: Azure Cosmos DB
Consistency level: Strong consistency

---

### Answer:

You should include the Azure Cosmos DB database service in your design. Azure Cosmos DB is one of several database services offered by Microsoft Azure, but it is the only database service that meets all the requirements stated in this scenario. Azure Cosmos DB is purpose-built for Azure and is designed to provide practically limitless storage at a massive scale. This performance and scalability is supported by service-level agreement (SLA) guarantees that outline 99.999% availability and read latencies of less than 10 milliseconds. Cosmos DB can support both relational and non-relational data models, including open- source PostgreSQL, Apache Cassandra, and MongoDB.
You should recommend that the Azure Cosmos DB consistency level be set to strong consistency. Consistency levels are used to define how effectively a database engine can return data that is the most up to date. For example, in a real-world scenario involving a geographically distributed database, consistency levels would define how quickly a request written to an instance in Asia would be available for retrieval by a client in South America. Azure Cosmos DB supports five consistency levels, with the strong consistency level guaranteeing that all reads will return the most recently committed write.
You should not recommend the Azure Database for MySQL or Azure SQL Database services. While both
platforms offer solid performance, they are primarily designed to store structured data using tables, which are comparable to spreadsheets, that can be queried using Structured Query Language (SQL). Neither offers SLA-backed latency minimization or 99.999% uptime guarantees.
You should not recommend the session or bounded staleness consistency levels. As the name indicates, session consistency ensures that data is read-consistent for the writer, meaning the writer will see the latest version of data they have written, while other readers may not always see the latest data. Bounded staleness offers better read consistency than session consistency but can still suffer from lag, especially if data is being stored across multiple regions.

---

### References

[Choose the right consistency level](https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/5-choose-cosmos-db-consistency-level)  
[Welcome to Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)  
[Azure Cosmos DB](https://azure.microsoft.com/en-us/products/cosmos-db/)  
[Azure Database for MySQL documentation](https://learn.microsoft.com/en-us/azure/mysql/)  
[What is Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview?view=azuresql)  
[Databases on Azure](https://azure.microsoft.com/en-us/products/category/databases) 

---

## Q093:

A company has two applications named App1 and App2 for a point of sale (POS) system deployed as Azure functions. App1 helps to scan the products and sends a message to App2 when payment is initiated. App2 takes care of the payment and sends a message on the status of the payment to App1. It currently uses Azure Queue Storage to deliver the messages.
In future, the applications will be processing large amounts of data when new stores open.
You need to recommend a solution to replace Azure Queue Storage so that it has high throughput and the data is also stored in a NoSQL database.
Which two solutions should you recommend? Each correct answer presents part of the solution.

Choose the correct answers

- Azure Service Bus
- Azure Data Factory
- Azure Event Hubs
- Azure Stream Analytics

---

### Answer:
- Azure Event Hubs
- Azure Stream Analytics

You should recommend Azure Event Hubs for message delivery. Event Hubs can send millions of messages per second and would be a good solution as the number of stores will be increasing in future. Event Hubs can deliver messages to multiple sources. Functions can be triggered based on the message received in the event hub. So, App1 and App2 can use the event hub to trigger each other.
You should also recommend Azure Stream Analytics for storing messages in the NoSQL database. Stream Analytics is a real-time analytics engine that can retrieve data from multiple sources, an event hub being one of them. It can then process the message and write the data to a NoSQL database like Cosmos DB.
You should not recommend Azure Service Bus. Even though Service Bus has high throughput, you would need additional applications to read the message and write it to the database. Service Bus has two solutions: queues and topics. Queues are for single consumer delivery, while topics can be used for multiple consumers. Azure Service Bus would have been a good choice, but stream analytics does not support Service Bus as an input.
You should not recommend Azure Data Factory. This is used for big data solutions that require Extract, Transform, and Load (ETL) and Extract, Load, and Transform (ELT) operations. It uses pipelines to process the data and can store it in the NoSQL database. However, you cannot send messages directly from the event hub to the data factory. You would need to have another solution in between that can store the data for ingesting it into the data factory. You can do this with Event Hub Capture, with which you can store the data into an Azure storage account. The data factory can then read the captured messages from the storage account and process them to update the database.

---

### References

[Azure Event Hubs trigger for Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=python-v2%2Cisolated-process%2Cnodejs-v4%2Cfunctionsv2%2Cextensionv5&pivots=programming-language-csharp)  
[Azure Event Hubs - A big data streaming platform and event ingestion service](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  
[Understand inputs for Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs)  
[Outputs from Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs)  
[Service Bus queues, topics, and subscriptions](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions)  
[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[Capture events through Azure Event Hubs in Azure Blob Storage or Azure Data Lake Storage](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview)  
[Quickstart: Create a data factory by using the Azure portal](https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory)  

---

## Q092:

You work at a retail company that stores large amounts of data in relational and non-relational databases, and that has file storage in both cloud and an on-premises datacenter. The company decides to analyze some data in real time to update its dashboards. The data will also be used to train the machine-learning model that will be used to provide recommendations in the online store.
You need to recommend a solution to gather actionable insights from the data being collected.
What should you recommend?

Choose the correct answer

- Azure Databricks
- Azure Data Lake
- Azure Data Factory
- Azure Stream Analytics

---

### Answer:

You should recommend Azure Databricks for this scenario. With Databricks, you can run big data pipelines using both batch and real-time data. It can gather data from the Data Factory in batches and, for real-time data, Event Hubs or loT Hub can be used. You can also train machine-learning models inside your Databricks workspaces. It uses Spark, which is a large-scale data-analytics language that can perform all the required actions.
You should not recommend Azure Data Lake. Data Lake is purely a storage solution and it cannot perform any analytics on the data. The data collected is generally unstructured, in the form of blobs and files. To analyze the data, you can use any Hadoop filesystem (HDFS) framework. Data Lake can also store data from real-time services like Event Hubs.
You should not recommend Azure Data Factory. With Data Factory, you can perform big data analytics using data-driven pipelines. It is generally used to orchestrate data movement between various sources and to transform the data. It cannot do real-time processing and machine learning, so it does not fit the requirements in this scenario.
You should not recommend Azure Stream Analytics, since it is a real-time processing engine. It can collect data from multiple sources and provide quick insights. It can store these insights in multiple data storages. However, it cannot train machine-learning models, which are required for recommending the products in the online store.

---

### References

[What is Azure Databricks?](https://learn.microsoft.com/en-gb/azure/databricks/introduction/)
[Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Welcome to Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)  

---

## Q091:

A company hosts an e-commerce website that uses Azure Content Delivery Network (CDN) to serve the photos and videos of the products from the catalog. A storage account named assetstrg is used to store all of the photos and videos. The product catalogue managers use custom scripts to upload the photos and videos of a product.
You need to recommend a solution that provides the product catalog managers with access that only allows them to upload the new files to the storage account from the Company's office network.
Which two security solutions should you recommend? Each correct answer presents part of the solution.

Choose the correct answers

- Use account shared access signature (SAS).
- Use access keys.
- Enable firewall policies and rules.
- Enforce the minimum TLS version.
- Enable secure transfer.
- Use service shared access signature (SAS).

---

### Answer:
- Enable firewall policies and rules.
- Use service shared access signature (SAS).

You should recommend using service shared access signature (SAS). A service SAS provides fine-grained control to access the resources in a storage account. It also allows you to set the start and end date of the token. With service SAS, you can limit access to the blob service in the storage account. Since the product catalog managers only need access to add the new files, a service SAS token for blob service with Add permissions can be created with a short expiry time.
You should also recommend enabling firewall policies and rules. By default, the storage account is available from all networks, including the internet. You can define the containers' public access levels to restrict who can access the blobs in the container. For further security, you can also put network restrictions on the storage account. You can add the IP address range of the company's network such that the storage account is only accessible from the office.
You should not recommend an account shared access signature (SAS). Account SAS is used when access to one or more storage account services is required. This solution can fulfill similar requirements to a service SAS, but it also provides access to other storage account services like queues, tables, and files. A better solution for this scenario is to create a service SAS.
You should not recommend access keys. Access keys are equivalent to super administrator credentials, which allow a user to perform any activity and operation on the storage account. Since the product catalog managers only need to upload the photos and videos, you should follow the principle of least privilege and provide them with upload access only using a service SAS.
You should not recommend enabling secure transfer. This setting is enabled by default and it ensures that any requests made to the storage account are made using HTTPS protocol. The storage account is available via HTTP protocol, as well, but this is an insecure way to communicate with the storage account. Even though it is recommended to use secure communication, it does not meet the scenario's goals.
You should not recommend enforcing the minimum TLS version. Since HTTPS connections are enabled by default, the storage account allows all the TLS versions for backward compatibility for the client connections.
The CDN allows the photos and videos to be served from one of the many points of presence (PoP) locations that are closer to the user accessing the website. The content is cached in these PoP locations from the storage account assetstrg. This improves user experience as the website loads faster, which means greater engagement and value for the business.

---

### References

[Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)  
[Configure Azure Storage firewalls and virtual networks](https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal)  
[Require secure transfer to ensure secure connections](https://learn.microsoft.com/en-us/azure/storage/common/storage-require-secure-transfer)  
[What is a content delivery network on Azure?](https://learn.microsoft.com/en-us/azure/cdn/cdn-overview)  
[Enforce a minimum required version of Transport Layer Security (TLS) for requests to a storage account](https://learn.microsoft.com/en-us/azure/storage/common/transport-layer-security-configure-minimum-version?tabs=portal)  

---

## Q090:

Your company is identifying requirements and preparing implementation designs for moving most of its resources from on-premises to Azure. These requirements include several storage scenarios with various size, performance, access, and long-term storage requirements. You are focused on matching storage account types to appropriate storage requirements.
You need to identify which types of Azure storage accounts support access tiers.
Which two storage account types should you include? Each correct answer presents part of the solution.

Choose the correct answers

- BlockBlobStorage
- FileStorage
- BlobStorage
- General-purpose V1
- General-purpose V2

---

### Answer:
- BlobStorage
- General-purpose V2

You should include General-purpose V2 and BlobStorage. These are the only storage accounts that support hot, cool, and archive access tiers. Archive storage is supported for block blobs only.
General-purpose V2 storage accounts support blob, file, queue, table, disk, and Data Lake Gen2 storage services, and let you choose from standard or premium performance tiers. BlobStorage supports block blobs and append blobs only, and it is limited to the standard performance tier.
You should not use General-purpose V1, BlockBlobStorage, or FileStorage. General-purpose V1, BlockBlobStorage, and FileStorage do not support access tiers.
General-purpose V1 storage accounts support blob, file, queue, table, and disk storage services, and the standard and premium performance tiers.
BlockBlobStorage accounts support block blobs and append blobs only, and they are limited to the premium performance tier.
FileStorage accounts support file services only, and they are limited to the premium performance tier.

---

### References

[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview) 

---

## Q089:

Your company currently sends all Azure log data to a Log Analytics workspace. The company plans to configure an event hub to also send the data to blob storage for long-term storage and trend analysis. Data will be maintained for at least 180 days..
The analysis will compare legacy data with current activity log data periodically. Access latency to the stored data and storage costs should be minimized. The solution should support zone-redundant storage (ZRS). You plan to create a new storage account to hold the log data.
You need to determine the type of storage account that you should create.
What type of storage account should you create?

Choose the correct answer

- General-purpose v2
- General-purpose v1
- FileStorage
- BlockBlobStorage

---

### Answer:
- General-purpose v2

You should create a General-purpose v2 storage account. This account type supports blob storage and lets you choose an access tier. In this scenario, you would choose a cool storage tier to meet the access latency requirement while minimizing costs. General-purpose v2 storage accounts support zone-redundant storage (ZRS).
You should not create a General-purpose v1 account. This account type does not support access tiers nor does it does support ZRS.
You should not create a BlockBlobStorage account. This account type does not support access tiers. It is
not the most cost-effective solution because it only uses the premium storage tier.
You should not create a FileStorage account. A FileStorage account does not support blob storage.


---

### References

[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)  
[Send Azure Monitor activity log data](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)   

---

## Q088:

You are the cloud architect for a newly founded manufacturing company that has built out the entirety of its IT infrastructure in Azure. You are reviewing the company's core business IT systems to identify its data archiving needs.
Due to financial regulations, the accounting system's database backups need to be retained for 10 years in the event of a historical audit. If needed, these backups must be retrieved within one day after the request for a file is received.
The order tracking system has a database that tracks order details and associated materials. The system also generates PDF files of the final build sheets for the company's work. The company must keep the database backups for three months, at which point the backups can be deleted. If needed, database backup of the order tracking system must be available immediately. The generated PDFs must be kept indefinitely with minimal costs.
You need to identify if the Azure storage archive tier meets the requirements for the long-term storage of each IT system.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.


The accounting system backups can be stored in an Azure Storage account that has the access tier set to Archive.
Yes
The order tracking system PDF files can be stored in an Azure Storage account that has the access tier set to Archive.
Yes

The order tracking system database backups can be stored in an Azure Storage account that has the access tier set to Archive.
No

---

### Answer:

The accounting system's backups can be stored in the archive tier. This is the least expensive method to store files that will be kept for at least 180 days. Files stored in the archive tier cannot be accessed immediately, because rehydration can take up to 15 hours. However, this is less than one day, which is the maximum access time for the backups in this scenario.
The order tracking system PDF files can be stored in the archive tier. Files will be kept indefinitely (which is more than 180 days) and the archive tier has the minimum storage cost.
The order tracking system database backups should not be stored in the archive tier because the backups must be available immediately when needed. Files stored in the archive tier cannot be accessed immediately, because rehydration can take up to 15 hours.

---

### References

[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  

---

## Q087:

A company is planning to use Azure Blob storage for a web application which is hosted on an Azure VM.
The data consists of several hundred files that are 1 GB or larger in size. The data is frequently accessed and staged for processing.
You need to provide a solution that minimizes the costs of data access.

Which access tier should you recommend?

Choose the correct answer

- Hot
- Cool
- Archive

---

### Answer:
- Hot

You should recommend the Hot access tier because it is expected to be accessed frequently for both read and write operations. For this storage tier, the cost is higher for the storage and lower for the access methods. This meets the requirement of low access costs.
You should not recommend the Cool access tier because it is designed for storing data that is infrequently accessed and stored for at least 30 days.
You should not recommend the Archive access tier because it is designed for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements.

---

### References

[Azure Archive Storage](https://azure.microsoft.com/en-us/products/storage/)  
[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  

---

## Q086:

Your company wants to configure a storage account for a new application. The storage account must remain available if a single Azure data center fails. The new application should perform more than 95% write operations. When the application needs read access, data must be available immediately.
You need to recommend a solution that offers the lowest storage cost for the required usage pattern.
Which storage account type and access tier should you use? To answer, select the appropriate options from the drop-down menus.

Choose the correct options
ZRS | LRS | GRS | RA-GRS
Hot | Cool | Archive

Storage account type: Zone-redundant storage (ZRS)
Storage account access tier: Hot

---

### Answer:

You should use the zone-redundant storage (ZRS) account type. ZRS replicates your data synchronously across three storage clusters in a single region. Each storage cluster is physically separated from the others and is located in its own availability zone (AZ). If a data center becomes unavailable, ZRS is still available.
You should not use the locally-redundant storage (LRS) account type. Although LRS is the least expensive option, it survives only if a server fails. LRS does not survive the failure of the whole data center.
You should not use the geo-redundant storage (GRS) or read-access geo-redundant storage (RA-GRS) account types. Although they provide a sufficient level of redundancy, both are more expensive than ZRS.
You should use the hot access tier because it is cheaper when data is written or accessed frequently. Storing data is a little more expensive than with the cold tier, but the savings will still be much higher in hot tier as compared to cold tier.
You should not use the cool storage account access tier. Although it has a low storage cost option that meets the usage requirements, it will be expensive because of the write operations. You might end up paying more instead of saving in costs.
You should not use the archive access tier. Although archive storage is the least expensive option in terms of storage cost, it does not meet the requirements. Data in the archive tier is not available immediately. The data must be rehydrated first, which takes up to 15 hours.

---

### References

[Use geo-redundancy to design highly available applications](https://learn.microsoft.com/en-us/azure/storage/common/geo-redundant-design)  
[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview) 
[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  


---

## Q085:

You are designing a monitoring strategy for an Azure SQL Database. 
You want metrics to be collected from the database and provide performance reports on a dashboard.
You need to minimize programming effort.

Which logging target should you specify in the design?

Choose the correct answer

- Azure Blob Storage
- Azure Event Hubs
- Azure Log Analytics
- Azure Table Storage

---

### Answer:

You should use Log Analytics in Azure Monitor. You can enable the diagnostic settings on the database and send the metrics to the Azure Log Analytics workspace. To generate the dashboard, you can enable Azure SQL Analytics in Azure Monitor. Azure SQL Analytics can consume the data from the Log Azure Log Analytics workspace to provide the performance reports on a dashboard without any programming efforts.
You should not use Azure Blob Storage. Blob Storage is used for storing large amounts of unstructured object data. Although you can enable the diagnostic settings on the database and send the metrics to the Azure Blob Storage, it would require lots of programming efforts to read the data from blob storage and generate the dashboard.
You should not use Azure Event Hubs. Event Hubs is a Big Data streaming platform. While it could be used by enabling the diagnostic settings on the database and send the metrics to the Azure Event Hubs. It is generally used to ship the data to third-party monitoring solutions or building custom logging and metrics solutions.
You should not use Azure Table Storage. Table Storage is used for storing structured NoSQL data in the cloud. Besides, it is not possible to send the diagnostics data directly to Table Storage.

---

### References

[Overview of Log Analytics in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-overview)  
[Monitor Azure SQL Database using Azure SQL Analytics (Preview)](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/insights/azure-sql)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?WT.mc_id=Portal-Microsoft_Azure_Monitoring&tabs=portal)  
[Introduction to Azure Blob storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction)  
[Azure Event Hubs - A big data streaming platform and event ingestion service](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about) 
[What is Azure Table storage?](https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview)  

---

## Q084:

Your company is in the process of migrating its data operations from an on-premises network to Azure storage. The company has over 65 terabytes of files currently stored in on-premises file servers and it wants to import this data into an Azure cool storage tier.
You need to recommend a solution for transferring the data. The solution must minimize costs and interruptions to the on-premises network.

What solution should you recommend?

Choose the correct answer

- Data Migration Assistant
- AzCopy
- Azure Data Box
- Azure Data Factory

---

### Answer:
- Azure Data Box

You should recommend Azure Data Box. Azure Data Box provides a cost-effective way to migrate large amounts of on-premises data to Azure. Data is transferred from on-premises to a Data Box, Data Box Disk, or Data Box Heavy, which is then sent to Microsoft for data load. You should use:

- Data Box Disk when transferring up to 40 terabytes (35 terabytes of usable capacity)
- Data Box when transferring up to 100 terabytes (80 terabytes of usable capacity)
- Data Box Heavy when transferring up to 1 petabyte (800 terabytes of usable capacity)

You should not recommend Azure Data Factory. Azure Data Factory is a cloud-based extract-transform-load (ETL) and data integration service designed for data movement and large-scale data transformations for cloud-based data.

You should not recommend AzCopy. AzCopy is a command-line utility that you can use to copy blobs and files to or from Azure storage accounts. AzCopy can copy on-premises data, but it is not designed to support the type of data transfer needed in this scenario.

You should not use Azure Data Migration Assistant. Data Migration Assistant is used to identify compatibility issues that can adversely impact data functionality when migrating to a new version of SQL Server or Azure SQL Database.

---

### References

[Azure Data Box for data transfer](https://azure.microsoft.com/en-us/products/databox/data/)  
[Azure Data Box: Frequently Asked Questions](https://learn.microsoft.com/en-us/azure/databox/data-box-faq?tabs=data-box-and-data-box-heavy)  
[Get started with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Overview of Data Migration Assistant](https://learn.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15)  

---

## Q083:

Your company is migrating data, workloads, and applications from Data Lake Storage Gen1 to Data Lake
Storage Gen2 to improve support for big data analytics. You want to accomplish this by using an
incremental copy pattern. You create a destination storage account and prepare the source data for
migration.
You need to determine what you should use to transfer the data.

What should you use?

Choose the correct answer

- Azure Data Box
- Azure Data Migration Assistant
- AzCopy
- Azure Data Factory

---

### Answer:
- Azure Data Factory

You should use Azure Data Factory. This is the recommended method for transferring data between Data Lake Storage Gen1 and Gen2. Azure Data Factory is a cloud-based extract-transform-load (ETL) and data integration service designed for data movement and large scale data transformations.
You should not use AzCopy. AzCopy is a command-line utility that you can use to copy blobs and files to or from Azure storage accounts. AzCopy is able to copy data into Azure Data Lake but it is not designed to support the type of data transfer needed in this scenario.
You should not use Azure Data Box. Azure Data Box provides a cost-effective way to migrate large amounts of on-premises data to Azure. Data is transferred from on-premises to a Data Box, Data Box Disk, or Data Box Heavy, which is then sent to Microsoft for data load.
You should not use Azure Data Migration Assistant. Data Migration Assistant is used to identify compatibility issues that can adversely impact data functionality when migrating to a new version of SQL Server or Azure SQL Database.

---

### References

[Azure Data Lake Storage migration guidelines and patterns](https://learn.microsoft.com/en-us/previous-versions/azure/storage/blobs/data-lake-storage-migrate-gen1-to-gen2)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Azure Data Box for data transfer](https://azure.microsoft.com/en-us/products/databox/data/)  
[Get started with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)  
[Overview of Data Migration Assistant](https://learn.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15)  

---

## Q082:

Your company is planning a merger with another company of a similar size. The other company has an on- premises data infrastructure that consists of proprietary databases, XML files, and PDF files of scanned documents. The merger is about to happen soon, and you need a way to quickly place this data in the cloud for analysis.
You need to choose an appropriate data flow and storage solution.

What should you do?

Choose the correct answer

- Use Data Factory to copy the data from the other company and store it in Azure SQL Database.
- Use Data Lake to copy the data from the other company and store it in HDInsight.
- Use HDInsight to copy the data from the other company and store it in Data Factory.
- Use Data Factory to copy the data from the other company and store it in Data Lake.

---

### Answer:
- Use Data Factory to copy the data from the other company and store it in Data Lake.

You should use Data Factory to copy the data from the other company and store it in Data Lake. Data Factory is a data integration service that allows you to create workflows for transforming and moving data. In this scenario, the data needs to be moved from the other company to Azure Data Lake. Data Lake is a hyper-scale repository for big data workloads. It allows you to capture data of any size and type, which is ideal in this scenario.
You should not use HDInsight to copy the data from the other company and store it in Data Factory. HDInsight is a big data solution that makes it easy to process large amounts of data. It is typically used for batch processing, data warehousing, data science, and streaming of Internet-of-Things (IoT) devices. Also, data cannot be stored in Data Factory. Data Factory is a workflow service.
You should not use Data Factory to copy the data from the other company and store it in Azure SQL Database. Azure SQL Database is a Platform-as-a-Service (PaaS) offering of Microsoft SQL Server, which is a relational database engine. It is not ideal in this scenario because you need to store data of varying types that is not all relational.
You should not use Data Lake to copy the data from the other company and store it in HDInsight. Data Lake stores data. It cannot copy data.

---

### References

[Copy and transform data in Azure Data Lake Storage Gen2 using Azure Data Factory or Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage?source=recommendations&tabs=data-factory)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)  
[What is Apache Hadoop in Azure HDInsight?](https://learn.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-introduction)  
[What is Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview?view=azuresql)  

---

## Q081:

You work for a consumer sales company that has gathered market data from a variety of sources. You have noticed that the data from some sources has schema drift issues.
You need to recommend a data preparation solution to prepare the data for analysis. The solution needs to minimize development effort

Which Azure service should you recommend?

Choose the correct answer

- Azure Functions
- Azure Data Factory
- Azure Logic Apps
- Azure Stream Analytics

---

### Answer:

You should recommend Azure Data Factory because it can help you to develop graphical data transformation logic without writing code. It can also protect against schema drift.
You should not recommend Azure Functions because, for every schema change, you would need to change the function code as well.
You should not recommend Azure Stream Analytics. For every schema change, you would need to change the code of the job.
You should not recommend Azure Logic Apps because any change to the schema of the input objects would require a change to the structure of the application.

---

### References

[Schema drift in mapping data flow](https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift)  
[Azure Stream Analytics documentation](https://learn.microsoft.com/en-us/azure/stream-analytics/)  
[Azure Functions documentation](https://learn.microsoft.com/en-us/azure/azure-functions/)  
[What is Azure Logic Apps?](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview)  

---

## Q080:

You have an Azure Blob storage account containing data accessed several times daily.
You plan to upload a new blob to the Blob storage account. The data in this specific new blob will be viewed infrequently but must be available immediately when accessed.
You must configure the access tier for the new blob. The solution must minimize storage costs.

What should you do?

Choose the correct answer

- Set the default access tier for the storage account to Cool.
- Set the access tier for the new blob to Archive.
- Set the access tier for the new blob to Cool.
- Set the default access tier for the storage account to Hot.

---

### Answer:


You should set the access tier for the new blob to Cool. Cool storage is intended for data accessed infrequently and stored for 30 days or more. Cool storage has a similar time-to-access as Hot data. Although it has a slightly lower availability compared to Hot data, storage costs are lower.
You should not set the access tier for the new blob to Archive. Although Archive storage is the least expensive, it also has several hours of retrieval latency.
You should not set the default access tier for the account to Hot. Usually, the blobs in the storage account are accessed frequently. If the tier for the account is set to Hot, this applies to the new blob as well.
You should not set the default access tier for the account to Cool. The Cool access tier is appropriate specifically for the new blob only. If the default tier for the account is set to Cool, this applies to the next uploaded blobs as well, and these blobs might need to be frequently accessed.

---

### References

[Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)  
[Assign Azure roles using the Azure portal](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal)  

---

## Q079:

You have an Azure Blob storage account containing data accessed several times daily.
You plan to upload a new blob to the Blob storage account. The data in this specific new blob will be viewed infrequently but must be available immediately when accessed.
You must configure the access tier for the new blob. The solution must minimize storage costs.

What should you do?

Choose the correct answer

- Set the default access tier for the storage account to Cool.
- Set the access tier for the new blob to Archive.
- Set the access tier for the new blob to Cool.
- Set the default access tier for the storage account to Hot.

---

### Answer:
- Set the access tier for the new blob to Cool.

You should set the access tier for the new blob to Cool. Cool storage is intended for data accessed infrequently and stored for 30 days or more. Cool storage has a similar time-to-access as Hot data. Although it has a slightly lower availability compared to Hot data, storage costs are lower.
You should not set the access tier for the new blob to Archive. Although Archive storage is the least expensive, it also has several hours of retrieval latency.
You should not set the default access tier for the account to Hot. Usually, the blobs in the storage account are accessed frequently. If the tier for the account is set to Hot, this applies to the new blob as well.
You should not set the default access tier for the account to Cool. The Cool access tier is appropriate specifically for the new blob only. If the default tier for the account is set to Cool, this applies to the next uploaded blobs as well, and these blobs might need to be frequently accessed.

---

### References

[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  

---

## Q078:

You are designing a database solution for the databases hosted in an on-premises environment. Currently, there are 10 databases that are hosted on individual VMs. Each database has varying usage patterns and the resources on the VM are underutilized.
You need to recommend a database solution that minimizes costs and reduces the administrative overhead.

The solution should meet the following requirements:

- The solution should have a 99.99% uptime SLA.
- The resources in the solution can scale dynamically.

The solution should have geo-replication capabilities.
Which solution should you recommend?

Choose the correct answer

- 10 Azure SQL databases
- 10 databases running in SQL server VMs
- 10 Azure SQL Managed Instances
- An Azure SQL database elastic pool hosting 10 databases

---

### Answer:

You should recommend an Azure SQL database elastic pool hosting 10 databases. Elastic pools allow putting multiple databases on the same Azure SQL server instance. Since the 10 databases have varying usage patterns and underutilized resources on the VM, an elastic pool can greatly reduce the cost and maintenance by hosting them together. In an elastic pool, the resources are shared among all the databases. You can scale dynamically based on the usage anytime. Since it is an Azure managed service, it is backed up with a minimum of 99.99% uptime SLA. You can also enable geo-replication for individual databases to prevent them from any disasters.
You should not recommend 10 Azure SQL databases. This solution almost meets all the requirements, but the databases would still be underutilized and you would need to manage 10 Azure SQL databases. This solution would also increase the costs, since you would pay for 10 Azure SQL databases.
You should not recommend 10 databases running in SQL server VMs. This would be a very similar setup that the one you are currently running on the on-premises environment. Since its a laaS service, you need to design for high availability by yourself using VM scale sets or availability sets, which can provide 99.99% uptime SLA, however, the cost to run the solution would be high. Replicating databases to another region would also become your responsibility.
You should not recommend 10 Azure SQL Managed Instances. Managed Instance is also a managed service that fulfills the requirements, however, it would be costly to run and would leave resources underutilized.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  
[High availability for Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla-local-zone-redundancy?view=azuresql&tabs=azure-powershell)  
[Service Level Agreements (SLA) for Online Services](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?lang=1)  
[Business continuity and HADR for SQL Server on Azure Virtual Machines](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/business-continuity-high-availability-disaster-recovery-hadr-overview?view=azuresql)  

---

## Q077:

You are designing a solution for hosting a business critical application in the cloud. For the application, you expect low I/O latency from the database.
You need to recommend a solution that meets the following requirements:

- The solution should have ability to scale individual components like compute and storage.
- The solution should provide secondary read-only replica for running analytics queries.

Which database service tier should you recommend?

Choose the correct answer

- DTU-based Premium
- DTU-based Standard
- vCore-based General Purpose
- vCore-based Business Critical 

---

### Answer:
- vCore-based Business Critical

You should recommend the vCore-based Business Critical database service tier. With the vCore-based purchasing model, you can scale the components individually, like compute and storage. This allows greater flexibility and control to the user. With the Critical Business service tier, you get low I/O latency for read- write operations. It also provides a free-of-charge secondary replica where you can run the analytics queries without affecting the performance of the database.
You should not recommend the vCore-based General Purpose database service tier. You can use General Purpose for generic database workloads. It provides higher latency than the Business Critical service tier and no secondary read replicas.
You should not recommend the DTU-based Premium or the DTU-based Standard service tiers. In the database transaction unit (DTU)-based purchasing model, the user has no flexibility to choose the compute and storage. DTU is a combination of CPU, memory, read and writes. You have to select a predefined SKU when using the DTU model.

---

### References

[Compare vCore and DTU-based purchasing models of Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/purchasing-models?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  
[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  

---

## Q076:

You have been tasked with designing security measures for a relational database solution.
You recommend enabling transparent data encryption (TDE) on the databases to protect data at rest.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Database backups are also encrypted when TDE is enabled.
Yes

Enabling TDE will require changes to be made to the application.
No

TDE is enabled by default for new databases.
Yes

---

### Answer:

Database backups are also encrypted when transparent data encryption (TDE) is enabled. The backup process copies the pages from the encrypted database file to the backup device. The data is never decrypted during the backup process.
Enabling TDE will not require changes to be made to the application. The application can perform real-time encryption and decryption without requiring any changes to be made.
TDE is enabled by default for new databases. All newly created databases are encrypted by default using the service-managed transparent data encryption.

---

### References

[Transparent data encryption for SQL Database, SQL Managed Instance, and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&tabs=azure-portal)    
[Design security for data at rest, data in motion, and data in use](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/7-design-security-for-data-at-rest-data-transmission-data-use)  

---

## Q075:

You are designing a monitoring strategy of user sign-ins for a web application.
You need to evaluate Microsoft Entra activity logs as a possible solution.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

You can configure Microsoft Entra sign-in logs to be routed to an Azure Storage account for archiving.
Yes

An Azure Log Analytics workspace is required to enable sending Microsoft Entra activity logs to Azure Monitor.
Yes

Microsoft Entra Password Protection is a prerequisite to enable Microsoft Entra activity logs.
No

---

### Answer:

You can configure Microsoft Entra sign-in logs to be routed to an Azure Storage account for archiving. This option is useful if you want to store Activity Log events for more than 90 days, which is the default number of days Activity Log events are stored on the Azure platform.
An Azure Log Analytics workspace is required to enable sending Microsoft Entra activity logs to Azure Monitor.
Microsoft Entra Password Protection is not a prerequisite to enable Microsoft Entra activity logs. This feature is used to enforce password policies in an organization. Microsoft Entra activity logs can be used even if Microsoft Entra Password Protection is not deployed.

---

### References

[What are the Microsoft Entra activity log integration options?](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/concept-log-monitoring-integration-options-considerations)   
[Monitor and review logs for on-premises Microsoft Entra Password Protection environments](https://learn.microsoft.com/en-us/entra/identity/authentication/howto-password-ban-bad-on-premises-monitor)  


---

## Q074:

A company is looking to enable its engineers to adopt Azure for a Software-as-a-Service (SaaS) product they are creating. It will be a multi-tenant application that would require lots of databases for each tenant.
You need to recommend a relational database solution that meets the following requirements:
The solution must be cost-efficient.
The team does not have a database administrator, so the management overhead of the database must be kept to a minimum.

The database must support geo-replication and auto-failovers.

Which database solution should you recommend?

Choose the correct answer

- An Azure SQL Database elastic pool
- An Azure SQL Database single database
- SQL Server on Azure virtual machines
- Azure SQL Managed Instance

---

### Answer:

You should recommend an Azure SQL Database elastic pool for the services. Azure SQL is a Platform-as-a- Service (PaaS) solution that is completely managed by Azure, so you do not have to perform any database administration work to manage the servers. With an elastic pool, you can put multiple databases in a single logical SQL server and all the databases will share the resources configured on that SQL server. This is cost- efficient as resources are being shared and there is no underutilization of resources. It also supports geo- replication and auto-failovers in the event of outages.
You should not select an Azure SQL Database single database. This provides the same features as an elastic pool, but it is not as cost-efficient since you will be creating a single database for each service. This means the cost of your database will increase.
You should not select SQL Server on Azure virtual machines. This is an Infrastructure-as-a-Service (laaS) solution where an SQL server is deployed on the virtual machine. This will require a database administrator who can manage the aspects of the server. You would need to replicate data by yourself and no failover options are available.
You should not select Azure SQL Managed Instance. SQL Managed Instance is also a Paas solution, like Azure SQL, but it provides more control on how the server is configured. To configure it correctly, you would need a database administrator, which will make it more costly than an SQL database elastic pool. It provides an automated backups feature, but no auto-failover capabilities.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  
[What is a single database in Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-overview?view=azuresql)  
[What is Azure SQL Managed Instance?](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql)
[What is SQL Server on Windows Azure Virtual Machines?](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/sql-server-on-azure-vm-iaas-what-is-overview?view=azuresql)  

---

## Q073:

Your company is developing a new data-intensive application. The initial database will contain approximately 1 TB of data. After the production version of the database is released, it is projected to grow by 300 to 500 GB per year.

All development will take place in Azure.

You need to recommend an Azure database solution that meets the following requirements:

- Microsoft SQL Server compatibility
- Minimal management requirements
- Minimal cost
- Support for DTU-based purchasing model
- Support for widely varying processing requirements during development
- Which storage solution should you recommend?

Choose the correct answer

- Azure SQL Database elastic pool
- Azure SQL managed instance
- SQL Server on Azure VM
- Azure SQL Database single database

---

### Answer:

You should recommend an Azure SQL Database single database. This database solution meets all the scenario requirements. However, if required, in future, with the growth of the database that is 300-500GB per year, an additional database can be added to accommodate the needed capacity and the databases can be added to an elastic pool.
You should not recommend Azure SQL Database elastic pool. This is used when you have multiple databases with different user requirements and peak use periods to provide cost savings by having the database share resources. In this scenario, a single database that does not need an Azure SQL Database elastic pool is needed.
You should not recommend Azure SQL managed instance. Managed instance meets most of the scenario requirements but does not support a DTU-based pricing model. Pricing support is for vCore only.
You should not recommend SQL Server on Azure VM. This option has the greatest compatibility with the SQL Server but it also has the highest management overhead. This is because you are responsible for managing the operating system and database engine.

---

### References

[What is a single database in Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-overview?view=azuresql)  
[What is Azure SQL?](https://learn.microsoft.com/en-us/azure/azure-sql/azure-sql-iaas-vs-paas-what-is-overview?view=azuresql&WT.mc_id=thomasmaurer-blog-thmaure)  
[Dynamically scale database resources with minimal downtime](https://learn.microsoft.com/en-us/azure/azure-sql/database/scale-resources?view=azuresql)  
[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  

---

## Q072:

Your Azure environment includes an Azure SQL Managed Instance that hosts a single database. The database includes data columns that contain especially sensitive data. Access to this data must be strictly controlled, including limiting access to administrators and other privileged users.
You are investigating the use of the Always Encrypted feature to provide additional protection.
You need to identify the features and functionality of Always Encrypted.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Always Encrypted encryption keys are never exposed to the SQL Managed Instance and are stored in an Azure Key Vault only
No

Always Encrypted can be used to prevent cloud admins and db_owners from viewing protected data.
Yes

Always Encrypted encrypts data at rest, in memory, and in use for protected columns.
Yes

Always Encrypted allows you to identify sensitive data columns before deploying a production database.
Yes

---

### Answer:

Always Encrypted encryption keys are never exposed to the SQL Managed Instance. However, encryption keys can be stored in Azure Key Vault or in Windows Certificate Store. Microsoft recommends using Azure Key Vault for key storage because this makes it easier to manage the encryption keys.
Always Encrypted can be used to prevent cloud admins and db_owners from viewing protected data. Preventing access to the data by privileged users is a primary reason for implementing Always Encrypted. This includes administrators, cloud admins, and privileged database users.
Always Encrypted encrypts data at rest, in memory, and in use for protected columns. However, it is not meant as a replacement for Transparent Data Encryption (TDE) or Transport Layer Security (TLS) for data protection, but as a supplement for your most sensitive data.
Always Encrypted allows you to identify sensitive data columns before deploying a production database. This is recommended so you can avoid having to make changes to your applications after moving to the production database.

---

### References

[Azure encryption overview](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-overview)  
[An overview of Azure SQL Database and SQL Managed Instance security capabilities](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-overview?view=azuresql)  
[Playbook for addressing common security requirements with Azure SQL Database and Azure SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-best-practice?view=azuresql)  

---

## Q071:

Your company has a hybrid network with an on-premises Active Directory (AD) and Microsoft Entra ID. A database server named DB1 is being migrated from an on-premises SQL Server to an Azure SQL Database.
You need to ensure that data in transit between on-premises users and the SQL Database is encrypted. The negative impact on database and communication performance should be kept to a minimum.
What should you recommend to meet the requirement?

Choose the correct answer

- Dynamic data masking
- Transparent Data Encryption (TDE)
- Transport Layer Security (TLS)
- Always Encrypted

---
### Answer:

- Transport Layer Security (TLS)
You should recommend the Transport Layer Security (TLS) protocol to protect data in transit. Data is encrypted when transmitted either way between the database and the client. TLS provides strong authentication, message privacy, and data integrity controls. It is relatively easy to deploy and implement.
You should not recommend the Transparent Data Encryption (TDE) feature to secure data in transit. TDE is used to protect data at rest from unauthorized or offline access to the data. Data in the database and data backups are encrypted. TDE is enabled by default when you create a new Azure SQL Database or Azure SQL Managed Instance.
You should not recommend Always Encrypted. Always Encrypted provides a higher level of protection for data in your database and can be applied to individual columns within the database. Data is encrypted at rest, in memory, and in use. It is used when you need added protection such as protecting the data from access by database administrators and other privileged users. Always Encrypted is not meant for use as a substitute for TLS or TDE, and incurs significant processing overhead. It is designed for use as a supplement to TLS and TDE for especially sensitive data.
You should not recommend Dynamic data masking. This is used as a way to ensure that only a select portion of the data in a data column is displayed in clear text; for example, only the last four digits of a telephone number.

---

### References

[Azure encryption overview](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-overview)   
[An overview of Azure SQL Database and SQL Managed Instance security capabilities](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-overview?view=azuresql)  
[Playbook for addressing common security requirements with Azure SQL Database and Azure SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-best-practice?view=azuresql)  

---

## Q070:

You plan to migrate an on-premises database to an Azure SQL database. The database size is now 250 GB.
You need to recommend a DTU-based service tier that meets the following requirements:
• Backup retention must be at least 14 days.
• In-memory OLTP must be supported.
Columnstore indexes must be supported.
There must be at least 8 GB in-memory OLTP storage per pool guaranteed.
Which tier and number of DTUs should you recommend? To answer, select the appropriate options from the drop-down menus.

Choose the correct options

---------------------------

Premium | Basic | Standard
125 | 500 | 1000

---------------------------

Tier: Premium
DTUs per pool: 1000

---

### Answer:

You should recommend the Premium Database Transaction Unit-based (DTU-based) service tier. It supports in-memory online transaction processing (OLTP) and columnstore indexes. It also has the lowest latency because it is built on solid-state drives.
You should not recommend the Basic DTU-based service tier. It does not support in-memory OLTP and columnstore indexes. It is mostly used for development environments and should not be used for critical production environments.
You should not recommend the Standard DTU-based service tier. It does not support in-memory OLTP. It is
used for development and production environments and should not be used on critical production environments.
You should recommend 1000 DTUs so that you can have up to 10 GB for in-memory OLTP storage per pool guaranteed.
You should not recommend 500 DTUs. This allows up to only 4 GB for in-memory OLTP storage per pool guaranteed.
You should not recommend 125 DTUs. This only guarantees up to 1 GB for in-memory OLTP storage per pool.

---

### References

[Resources limits for elastic pools using the DTU purchasing model](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-elastic-pools?view=azuresql)  
[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  

---

## Q069:

You manage an Azure SQL Database managed instance that stores critical business data.
You need to design a protection strategy for the data. The solution must protect the entire database while at rest and require minimal administrative effort to implement.
What solution should you recommend?
Choose the correct answer
Dynamic Data Masking
Always Encrypted
Transparent Data Encryption
Azure Storage encryption
▲ Explanation
You should recommend Transparent Data Encryption because it encrypts the data at rest. All data that is stored on the disk is encrypted, as well as backups.
You should not recommend Always Encrypted. This option encrypts both data in transit and data at rest, but it requires greater effort to implement. You should only encrypt the most sensitive data using this feature because it requires you to specify a column to be encrypted.
You should not recommend Azure Storage encryption. This is a default encryption method that protects a storage account, including protecting backup files. It does not protect data and log files while in use.
You should not recommend Dynamic Data Masking because it does not protect data on the disk. This option masks sensitive data when it is displayed in the application.


---

### Answer:

---

### References

[Azure Data Encryption at rest](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest)  
[Tutorial: Getting started with Always Encrypted](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-tutorial-getting-started?view=sql-server-ver16&tabs=ssms)  
[Azure Storage encryption for data at rest](https://learn.microsoft.com/en-us/azure/storage/common/storage-service-encryption)  
[Dynamic Data Masking](https://learn.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16)  

---

## Q068:

You have several general purpose Azure SQL databases that have capacities of 400 database transaction units (DTUs) each. These databases have varying and unpredictable usage demands. Half of the databases use less than 200 DTUs on a regular basis.

You need to optimize the cost of managing and scaling the databases.

What solution should you recommend?

Choose the correct answer

Standard service tier

- v-Core based model
- Elastic pools
- Hyperscale service tier

---

### Answer:

You should recommend elastic pools because they are a cost-effective solution to manage and scale multiple databases that have varying and unpredictable usage demands. The databases will share the resources at a set price. They must be on the same server.
You should not recommend the Hyperscale service tier because you will not be able to control how the resources are managed by each of the databases. The Hyperscale service tier is a highly scalable storage and compute performance tier.
You should not recommend the Standard/General Purpose service tier because you will not be able to control how the resources are managed by each of the databases. This tier is a default service tier in Azure SQL Database that is designed for generic workloads.
You should not recommend the v-Core based model because you will not be able to control how the resources are managed by each of the databases. The vCore-based purchasing model enables you to independently scale compute and storage resources, match on-premises performance, and optimize price.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  

---

## Q067:

You are the lead architect for a new data analytics startup in the Dallas-Fort Worth metropolitan area. You decide on a cloud-only infrastructure in Azure. You currently have data in approximately 50 SQL databases that must be analyzed. Usage patterns vary between databases. The solution is expected to use several databases that will remain in use for at least two years.
You need to contain costs as much as possible and look for ways to reduce expenditures.

How should you consider deploying the solution to minimize costs? For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statements:

Using SQL elastic pools to manage your databases will save costs.
Yes

Configuring the databases to have the Price: Low tag will save costs.
No

Using reserved capacity instead of pay-as-you-go will save costs.
Yes

---

### Answer:

Using SQL elastic pools to manage your databases will save costs. With elastic pools, multiple databases are on a single server and can share resources, which saves costs.
Configuring the databases to have the Price: Low tag will not save costs. Tags are used for adding metadata to Azure resources and to logically organize them into a taxonomy. Tags do not directly affect resource cost.
Using reserved capacity instead of pay-as-you-go will save costs. Pre-paying to reserve a database for one or three years is much more cost-effective than a pay-as-you-go arrangement.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)    

[Save costs for resources with reserved capacity Azure SQL Database & SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview?view=azuresql)   

[Acquire, provision, and manage Azure reserved VM instances (RI) + server subscriptions for customers](https://learn.microsoft.com/en-us/partner-center/azure-ri-server-subscriptions)   

---

## Q066:

Company1 relies heavily on its Azure cloud infrastructure to deliver services to clients worldwide. As the company continues to grow, managing access to its critical resources, applications, and data has become a significant challenge.
You need to recommend a solution for controlling and monitoring access to important roles and enforcing just-in-time elevation of rights.

What should you recommend?

Choose the correct answer

- Privileged Identity Management (PIM)
- Access reviews
- Entitlement management
- Activity logs and auditing

---

### Answer:
- Privileged Identity Management (PIM)

You should recommend Privileged Identity Management (PIM). PIM is a feature within Microsoft Entra Identity Governance that specifically addresses the need to control and monitor access to important roles. It enforces just-in-time (JIT) elevation of privileges for users who need them to perform specific tasks. This aligns perfectly with the requirement to manage access to critical resources and ensure that privileges are granted only when necessary.
You should not recommend access reviews. While access reviews are important for periodically auditing and reviewing access to resources, they do not directly enforce JIT elevation of privileges, which is a specific requirement mentioned in the scenario.
You should not recommend entitlement management. Entitlement management helps to define and manage fine-grained access packages, but it may not be the primary choice for enforcing JIT elevation of privileges.
You should not recommend activity logs and auditing. While activity logs and auditing are essential for monitoring and maintaining security, they do not provide the same level of real-time control and enforcement of privileged access as PIM.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  

[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)  

[What is entitlement management?](https://learn.microsoft.com/en-us/entra/id-governance/entitlement-management-overview)  

[Azure security logging and auditing](https://learn.microsoft.com/en-us/azure/security/fundamentals/log-audit)  

---

## Q065:

You are requested to provide resources for a marketing campaign that internal users can use to self-service request without the need for approval. There is a requirement for access to expire within 30 days.

What should you recommend?

Choose the correct answer

- Lifecycle workflow
- Security group
- Access package
- Access review

---

### Answer:
- Access package

You should recommend an access package. An access package is designed to provide a curated set of resources that users can request access to without the need for approval. Access packages allow users to self-service their access requests based on predefined permissions and policies. Additionally, you can set an expiration period for the access, which aligns with the requirement for user access to expire within 30 days. This feature provides an efficient and controlled way to manage temporary access to resources for specific campaigns or projects.

You should not recommend security groups, access reviews, or lifestyle workflows. These are not directly aligned with the requirement of providing resources that internal users can self-service request without approval and with an expiration time frame.

Security groups are used for grouping users, computers, or other security principals for applying permissions, but they do not provide the self-service request or expiration features mentioned in the scenario.

Access reviews are processes used to periodically review and manage user access to resources, but they do not fit the scenario's requirement for self-service access and a specific 30-day expiration time frame.
Lifecycle workflows often involve processes related to the creation, modification, or deletion of identities or resources, but they might not be suitable for managing temporary access requests and their expiration periods as described in the scenario.

---

### References

[Tutorial: Manage access to resources in entitlement management](https://learn.microsoft.com/en-us/entra/id-governance/entitlement-management-access-package-first)   

[Active Directory security groups](https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/understand-security-groups)  

[Review recommendations for Access reviews](https://learn.microsoft.com/en-us/entra/id-governance/review-recommendations-access-reviews)  

[What are lifecycle workflows?](https://learn.microsoft.com/en-us/entra/id-governance/what-are-lifecycle-workflows)  

---

## Q064:

Your organization is preparing a landing zone to move on-premises resources to Azure cloud. You are requested to recommend a hierarchical level where policies must be placed so that they are applied to all workloads that require the same security, compliance, connectivity, and feature settings.
You need to provide a recommendation.

What should you recommend?

Choose the correct answer

- Management group
- Resources
- Resource group
- Subscription

---

### Answer:

You should recommend a management group level. A management group in Azure is a container for organizing subscriptions and applying policies across multiple subscriptions. Placing a policy at the management group level ensures that it is inherited by all the subscriptions and resources within those management groups. This is the recommended approach when you want consistent policies to apply to a set of resources across different subscriptions.
You should not recommend a resource group level. A resource group is a logical container for grouping related resources within a single subscription. While policies can be applied at the resource group level, this would be suitable when you want policies to apply to resources within a specific project or application, rather than across multiple workloads requiring the same settings.
You should not recommend a subscription level. A subscription is a basic unit of organization within Azure. Policies can be applied at the subscription level, but this might not be the best choice for managing workloads that require consistent settings across multiple subscriptions.
You should not recommend a resources level. Applying policies directly to individual resources (such as virtual machines, databases, etc.) is not efficient for managing a larger scale environment with uniform settings. This approach can lead to policy management complexity and difficulty in maintaining consistent settings.

---

### References

[Cloud governance guides](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/govern/)  
[Design for governance](https://learn.microsoft.com/en-us/training/modules/design-governance/2-design-for-governance)  
[Design for management groups](https://learn.microsoft.com/en-us/training/modules/design-governance/3-design-for-management-groups)  
[Design for subscriptions](https://learn.microsoft.com/en-us/training/modules/design-governance/4-design-for-subscriptions)  
[Design for resource groups](https://learn.microsoft.com/en-us/training/modules/design-governance/5-design-for-resource-groups)  
[What is Azure Resource Manager?](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/overview)  

---

## Q063:

Your company has an Azure subscription. The company wants a way to enforce organizational standards and to assess compliance at the subscription level and have the standards apply throughout the organization.
If compliance standards change, you should be able to update the standards and bring resources into compliance through bulk remediation. Remediation for new resources should be automatic.

You need to recommend a solution to meet these requirements.

What should you recommend?

Choose the correct answer

- Azure Resource Manager (ARM) templates
- Azure Managed Identities
- Azure Policy
- Azure Conditional Access

---

### Answer:
- Azure Policy

You should recommend Azure Policy. Azure Policy lets you enforce organizational standards at any point in your hierarchy, from management groups down to the individual resource level. You can define policies to implement governance for resource consistency and compliance. You can run remediation tasks on existing resources and implement automatic remediation for new resources.
You should not recommend Azure Conditional Access. Conditional Access is a feature of Microsoft Entra security that can be used to set requirements to allow or block access to resources.
You should not recommend Azure Managed Identities. Managed Identities is a special type of security principal that is used to manage secure access by Azure resources.
You should not recommend Azure Resource Manager (ARM) templates. ARM templates contain the definition for creating resource groups and resources, but are not used to manage compliance or remediate compliance issues.


---

### References

[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  
[What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)    
[What are ARM templates?](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/overview)  

---

## Q062:

Your company plans to use Azure Blueprints to ensure that sets of Azure resources implement and adhere to organizational standards, patterns, and requirements. The blueprints being designed include:

- Role Assignments
- Policy Assignments
- Azure Resource Manager templates (ARM templates)
- Resource Groups

You need to identify features and functionalities of Azure Blueprints.

For each of the following statements, select Yes if the statement is true. Otherwise, select No.

When a resource is deployed from a blueprint, a connection remains between the blueprint and the resource.
Yes

RBAC role assignments can be defined for a subscription or individual resource groups in a blueprint.
Yes

A subscription owner can override blueprint Read Only and Do Not Delete resource locking options.
No

---

### Answer:

When a resource is deployed from a blueprint, a connection remains between the blueprint and the resource. This is a major difference between using an ARM template to deploy resources and using an ARM template from within an Azure Blueprint. This connection makes for improved tracking and auditing of deployments.
Role-based access control (RBAC) role assignments can be defined for a subscription or individual resource groups in a blueprint. RBAC roles assignments can be included as artifacts in an Azure Blueprint with defined role assignments.
A subscription owner cannot override blueprint Read Only and Do Not Delete resource locking options. A subscription owner can unassign a blueprint that they are assigned at the subscription level, which removes any locking. To prevent the subscription owner from doing this, you should assign blueprints at the management group level.

---

### References

[What is Azure Blueprints (Preview)?](https://learn.microsoft.com/en-us/azure/governance/blueprints/overview)  
[Understand the lifecycle of an Azure Blueprint](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/lifecycle)  
[Understand the deployment sequence in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/sequencing-order)  
[Understand resource locking in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/resource-locking)  

---

## Q061:

Your company is using Azure Blueprints to ensure that sets of Azure resources implement and adhere to organizational standards, patterns, and requirements. You create and publish a blueprint that you then assign to your subscription.
The blueprint completes the following tasks:

- Update select existing resources.
- Create new resources.
- Apply resource locks.
- Add existing groups to select RBAC roles.

You prepare to unassign the blueprint so that you can replace it with a different blueprint. You need to determine the expected impact of unassigning the blueprint.
What will happen when you unassign the blueprint?

Choose the correct answer

- RBAC role assignments are removed.
- Updates to existing resources are rolled back.
- Any new resources created by the blueprint are deleted.
- All blueprint resource locking is removed.

---

### Answer:
- All blueprint resource locking is removed.

All blueprint resource locking is removed when you unassign a blueprint. This is the only impact to resources and resource groups in the blueprint's scope. New resources will remain in place, updates to existing resources are maintained, and RBAC role assignments do not change. Resources remain in place, but they are no longer protected by the blueprint.

---

### References

[What is Azure Blueprints (Preview)?](https://learn.microsoft.com/en-us/azure/governance/blueprints/overview)  
[Understand the lifecycle of an Azure Blueprint](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/lifecycle)  
[Understand the deployment sequence in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/sequencing-order)  
[Understand resource locking in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/resource-locking)  

---

## Q060:

Your company has an Azure subscription that hosts most of its computing resources. The company is evaluating the possible use of resource tags to better organize and manage its resources.
You need to identify features and functionalities of using resource tags.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement

You can use Azure Policy, PowerShell cmdlets, CLI commands, ARM templates, and Azure portal to apply tags.
Yes

Tags applied at the resource group are automatically inherited when you create a new resource.
No

Tags can be applied at the resource group and individual resource hierarchical levels only.
No

---

### Answer:

You can use Azure Policy, PowerShell cmdlets, CLI commands, ARM templates, and Azure portal to apply tags. Azure portal, PowerShell, and CLI are primarily used to make manual changes to resource tags. Azure Policy provides a way to automate the assignment and updating of tags.
Tags applied at the resource group are not automatically inherited when you create a new resource. No inheritance is supported by default, but it can be configured through Azure Policy.
Tags cannot be applied at the resource group and individual resource hierarchical levels only. You can apply tags directly at the subscription level. You cannot apply tags at management group levels. You can apply Azure policies that manage resource tags at management group levels.


---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources) 
[Assign policy definitions for tag compliance](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-policies)  

---

## Q059:

Your company has an Azure subscription that includes multiple resource groups used to organize resources. Currently, you are not using resource tags.
You need to apply resource tags to resource groups, and ensure that the same tags apply to any resources that the group contains. When tags change at the resource group, the change should update its resources. You want to minimize the effort that is required to implement and maintain the solution.

What should you do?

Choose the correct answer

- Manually create and update tags individually for the resources.

- Create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group.

- Create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group. Initiate a remediation task when specified tags are added or updated at the resource group.

- Create and modify the tags at the resource group.

---

### Answer:
- Create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group. Initiate a remediation task when specified tags are added or updated at the resource group.

You should create an Azure Policy to apply specified tags from the resource group to resources, link the policy to the resource group, and initiate a remediation task whenever tags are added or updated at the resource group. Resource tags applied at the resource group are not inherited by its resources. You can specify this action through Azure Policy. After applying the policy, or making changes to the resource group, you can run a remediation task to update the resources. Specified tags are also applied when you create a new resource, or update an existing resource. You can also use an Azure Policy to force resources to inherit tags from a subscription.
You should not just create and modify the tags at the resource group. The tags are not inherited by resources by default.
You should not manually create and update tags individually for the resource groups and resources. This would require the most effort to implement and maintain, and it would probably introduce inadvertent
errors.
You should not create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group without initiating a remediation process. Remediation is a required part of the process for existing resources. Specified tags are applied when you create a new resource, or update an existing resource.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  
[Tutorial: Create and manage policies to enforce compliance](https://learn.microsoft.com/en-us/azure/governance/policy/tutorials/create-and-manage)  

---

## Q058:

You are designing a monitoring solution for a gaming website hosted in an Azure Web App.
You have the following monitoring requirements:
Track how often users return to the website in a specified time period.
Measure how specific events influence user activities.
View user activity by region.
You need to provide a solution that minimizes administrative effort.

Which monitoring solution should you use?

Choose the correct answer

- Application Insights
- Azure Monitor Alerts
- Azure Al Services
- Time Series Insights environments

---

### Answer:
- Application Insights

You should use Application Insights as it includes a retention feature that enables tracking of user behavior. You can configure custom events so that you can analyze user reactions. You can query user activity by time range as well as by filtering according to region or country.
You should not use Azure Monitor Alerts. Data collected by Azure Monitor helps in detecting potential problems with the application or infrastructure. You can configure alerts for notifying your operation teams before end users face issues. It cannot track the user activity of your application.
You should not use Time Services Insights environments. This service provides fully managed lot analytics solution in the cloud. It provides visualization to your loT events, but does not track events back to user satisfaction.
You should not use Azure Al Services. These services consist of APIs and images that developers can use to integrate popular artificial intelligence (Al) features into their applications. They do not provide monitoring functions.


---

### References

[Application Insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview?tabs=net)  
[User retention analysis for web applications with Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/usage-retention)  
[What is Azure Time Series Insights Gen2](https://learn.microsoft.com/en-us/azure/time-series-insights/overview-what-is-tsi)  
[What are Azure Al services?](https://learn.microsoft.com/en-us/azure/ai-services/what-are-ai-services)  
[Azure Monitor overview](https://learn.microsoft.com/en-us/azure/azure-monitor/overview)  
[What are Azure Monitor Alerts?](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview)  

---

## Q057:

Your company has an Azure subscription with Microsoft Entra ID. Your company's development team members will provision Azure VMs to support special projects.
You need to recommend a solution that meets the following additional requirements:

- Limit VMs to specific sizes only.
- Limit VMs to specific regions.

What should you use?

Choose the correct answer

- Azure Role-Based Access Control (Azure RBAC)
- Azure Policy
- Azure Conditional Access
- Azure Resource Tags

---

### Answer:
- Azure Policy

You should use Azure Policy to meet the requirements. Azure Policy lets you enforce organizational standards at any point in your hierarchy, from management groups down to the individual resource level. You can define policies to implement governance for resource consistency, including which VMs can be created, and in which regions.
You should not use Azure Resource Tags. Resource Tags is a way to organize and manage resources, and it does not control resource governance or the types of resources that can be created. Common uses of resource tags include implementing support for regulatory governance, workload optimization, and cost
controls.
You should not use Azure Role-Based Access Control (RBAC), Azure RBAC roles give you control over what you can do when you access a resource, but they are not used to impose resource consistency for creating resources.
You should not use Azure Conditional Access. Conditional Access is a feature of Microsoft Entra security that can be used to set requirements to allow or block access to resources.

---

### References

[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources?tabs=json)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  
[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  

---

## Q056:

Your Azure organizational hierarchy has one management group with two subscriptions and four resource groups in each subscription. You plan to implement Azure policies to help you to organize and manage resources in your organization.
You plan to implement a custom policy to:
Restrict the types of VMs that can be deployed.
Deny the creation of resources without a Department tag when they are created.
You need to determine the minimum number of custom policies and policy assignments required.

What is the minimum number required in each scenario? To answer, select the correct number from the drop-down menus.

Choose the correct options
1 | 2 | 4 | 8 | 16

Minimum number of custom policies required: 1
Minimum number of policy assignments required: 1

---

### Answer:

To assign the custom policy to the management group, you need to create a minimum of one custom policy and make one policy assignment. Only one policy is needed because you can include multiple conditions in a custom policy to meet the policy requirements by using the allof logical operator. It is important to note that a policy should produce a single effect. If you need, for example, to create a policy to restrict a VM type and also append an automatic tag to new resources, you will need to create two policies, the first one with the deny effect, and the second policy with the append effect.
Policies that are applied at a hierarchical level are automatically inherited; therefore, applying the policy to the management group applies the policies to all subscriptions and resource groups.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)
[Azure Policy definition structure](https://learn.microsoft.com/en-us/azure/governance/policy/concepts/definition-structure-basics)  
[Understand Azure Policy effects](https://learn.microsoft.com/en-us/azure/governance/policy/concepts/effect-basics)  
[Tutorial: Create and manage policies to enforce compliance](https://learn.microsoft.com/en-us/azure/governance/policy/tutorials/create-and-manage)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  

---

## Q055:

You decide to use tagging to organize your Azure resources.
You need to make sure that new resources are tagged as soon as possible and that manual tagging is not required.

What should you do?

Choose the correct answer

- Create an Azure policy to apply tagging when new resources are created.
- Add tags through the Azure portal.
- Write an Azure CLI script to tag every new resource once a day.
- Write a PowerShell script to tag everything once a week.

---

### Answer:

You should create an Azure policy to apply tagging when you create new resources. An Azure policy can be predefined and applied to all new resources.
You should not write a PowerShell script to tag everything once a week. PowerShell is a method that can apply tagging, but it uses manual procedure.
You should not write an Azure CLI script to tag every new resource once a day. Azure CLI is a method that can apply tagging, but it uses a manual procedure.
You should not add tags through the Azure portal. Although you can add tags in the portal after creating a resource, it is still a manual process.

---

### References

[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  
[Tagging a VM using the portal](https://learn.microsoft.com/en-us/previous-versions/azure/virtual-machines/tag-portal)  

---

## Q054:

You are implementing a conditional access solution in your Microsoft Entra environment. During the pilot phase of the implementation, the Security Operation Team (SOC) raised concerns regarding the delay between modifications in user conditions and the enforcement of policy changes.
You need to recommend a mechanism that enables a conversation between the token issuer (Microsoft Entra ID) and the relying party (enlightened app) to address policy violations or security issues.

What should you recommend?

Choose the correct answer

- Two-factor authentication (2FA)
- Continuous Access Evaluation (CAE)
- OAuth 2.0 Authorization Code Flow
- OpenID Connect (OIDC)

---

### Answer:
- Continuous Access Evaluation (CAE)

You should recommend Continuous Access Evaluation (CAE). The scenario discusses implementing a conditional access solution in a Microsoft Entra environment, with concerns raised by the Security Operations Team about delays between user condition modifications and the enforcement of policy changes. The situation calls for a mechanism that facilitates a "conversation" between the token issuer (Microsoft Entra ID) and the relying party (enlightened app) to handle policy violations or security issues. CAE is the most appropriate mechanism for this purpose. CAE is designed to provide real-time or near real- time evaluation of access conditions and allows for ongoing monitoring and assessment of user and device conditions, ensuring that policy changes are promptly enforced. If conditions change for a user (as expressed in the scenario), CAE can trigger an evaluation and interaction between the token issuer and the relying party to address potential policy violations or security concerns.
You should not recommend Two-factor authentication (2FA), OAuth 2.0 Authorization Code Flow, or OpenID. Connect (OIDC). These are authentication and authorization mechanisms, but they do not specifically address the continuous monitoring and evaluation aspect required by the scenario.

---

### References

[Continuous access evaluation](https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-continuous-access-evaluation)  
[Design for conditional access](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/6-design-for-conditional-access)  
[How to use two-step verification with your Microsoft account](https://support.microsoft.com/en-us/account-billing/how-to-use-two-step-verification-with-your-microsoft-account-c7910146-672f-01e9-50a0-93b4585e7eb4)  
[Microsoft identity platform and OAuth 2.0 authorization code flow](https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-auth-code-flow)  
[OpenID Connect on the Microsoft identity platform](https://learn.microsoft.com/en-us/entra/identity-platform/v2-protocols-oidc)  

---

## Q053:

You organization plans on moving all its organizational data from on-premises to the cloud. Your Data Protection Officer (DPO) requested the highest possible level of security for data encryption keys is implemented.
You need to recommend a solution that guarantees a high level of tamper resistance and protection against attacks for cryptographic material, ensuring that stored keys never leave the storage boundary.

What solution should you recommend?

Choose the correct answer

- Hardware Security Module (HSM)
- Trusted Platform Modules (TPM)
- Key Vault
- Encrypted file system

---

### Answer:
- Hardware Security Module (HSM)

You should recommend a Hardware Security Module (HSM) is a specialized hardware device designed to provide secure storage and management of cryptographic keys. HSMs are known for their high level of tamper resistance and protection against attacks. They store keys in a dedicated hardware component, ensuring that the keys never leave the secure boundary of the HSM. This is particularly crucial for meeting the requirement of not letting cryptographic material leave the storage boundary. HSMs offer a robust combination of physical security, tamper resistance, and cryptographic operations, making them a suitable choice for scenarios that demand the highest level of security.
You should not recommend encrypted file systems. Encrypted file systems are designed to protect data at rest on storage devices by encrypting the data stored in files. However, they do not inherently provide the level of protection needed for cryptographic keys. Additionally, encrypted file systems focus on protecting the data itself rather than the keys used to encrypt the data. Since the question specifies the need to protect cryptographic material and ensure it does not leave the storage boundary, encrypted file systems are not the most suitable option.
You should not recommend trusted platform modules (TPMS). TPMS are hardware chips on computer motherboards that provide a secure environment for cryptographic operations. While TPMs can securely store keys, they are typically more focused on local device security and platform integrity. They might not be as specialized as HSMs in offering the highest level of protection and tamper resistance for cryptographic keys.
You should not recommend Key Vaults. Key vaults are cloud-based services designed to securely manage cryptographic keys and secrets. While they offer good security for key management, they are not necessarily hardware based like HSMs. Additionally, the question specifies the need for cryptographic material to never leave the storage boundary, and key vaults could potentially involve data leaving the immediate storage boundary for cloud-based management unless they utilize HSMS.

---

### References

[What is Azure Key Vault Managed HSM?](https://learn.microsoft.com/en-us/azure/key-vault/managed-hsm/overview)  
[Import HSM-protected keys to Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/keys/hsm-protected-keys)  
[File Encryption](https://learn.microsoft.com/en-us/windows/win32/fileio/file-encryption)  
[What is TPM?](https://support.microsoft.com/en-us/topic/what-is-tpm-705f241d-025d-4470-80c5-4feeb24fa1ee)   
[About Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview)  

---

## Q052:

You are requested to implement a solution to manage sensitive information used across services and applications in your Azure environment.
You need to recommend a placement of sensitive information within Azure Key Vault.
Which storage destination within Azure Key Vault should you recommend? To answer, drag the appropriate storage type to each sensitive information object. A storage type may be used once, more than once, or not at all.

Drag and drop the answers

Tags
Secrets
Certficates
Keys

Information about rotation configuration: Tags
RSA object: Keys
JSON object: Secrets
X.509 object: Certficates

---

### Answer:

To effectively manage sensitive information utilized across services and applications within your Azure environment, it is crucial to store each type of sensitive information in its appropriate supported storage destination within Azure Key Vault. The recommended storage types for each sensitive information object are:
• Information about rotation configuration: Tags. Tags are metadata labels that you can assign to resources in Azure, including items within Azure Key Vault. They provide a way to categorize, organize, and manage resources. For sensitive information related to rotation configurations, using tags can help you to label and identify items with specific rotation-related properties or metadata.
• RSA Object: Keys. Rivest-Shamir-Adleman (RSA) is a widely used asymmetric encryption algorithm that involves the use of public and private key pairs. If you have RSA objects that you need to manage, you should store them as cryptographic keys within Azure Key Vault. Keys in Azure Key Vault are used for internal encryption, decryption, and signing operations.
• JSON Object: Secrets. JavaScript Object Notation (JSON) is a data interchange format often used to store structured data. If you have sensitive information stored in JSON format, such as API keys, connection strings, or passwords, you should store them as secrets within Azure Key Vault. Secrets are encrypted and provide an additional layer of security compared to storing them directly in code or configuration files.
• X.509 object: Certificates. X.509 is a standard format for public key certificates, commonly used for secure communication and authentication. If you have X.509 certificates, such as SSL certificates for securing websites, you should store them as certificates within Azure Key Vault. Key Vault's certificate management capabilities allow you to securely store, manage, and use X.509 certificates within your applications and services.

---

### References

[About Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview)  
[The Relationship Between Keys, Secrets and Certificates in Azure Key Vault](https://michaelhowardsecure.blog/2021/04/29/the-relationship-between-keys-secrets-and-certificates-in-azure-key-vault/)  
[About Azure Key Vault certificates](https://learn.microsoft.com/en-us/azure/key-vault/certificates/about-certificates)  
[About Azure Key Vault secrets](https://learn.microsoft.com/en-us/azure/key-vault/secrets/about-secrets)  
[About keys](https://learn.microsoft.com/en-us/azure/key-vault/keys/about-keys)  
[Best practices for secrets management in Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/secrets/secrets-best-practices)  

---

## Q051:

You are tasked with storing sensitive information in Azure Key Vault.
You need to identify which types of sensitive information can be stored in your Azure key vault.
Which three of the following can you store in your Azure key vault? Each correct answer provides a complete solution.

Choose the correct answers

- Blueprints
- Secrets
- Certificates
- Tags
- Keys

---

### Answer:
- Secrets
- Certificates
- Keys

Azure Key Vault is a secure and centralized cloud service provided by Microsoft for managing cryptographic keys, secrets, and certificates used by cloud applications and services. It helps to safeguard sensitive information and provides secure access control.
Secrets are small-sized data elements, such as passwords, connection strings, and API keys. Storing secrets in Azure Key Vault ensures that sensitive data remains secure and accessible only to authorized users and applications.
Certificates are digital files that provide authentication and secure communication. Azure Key Vault allows you to store X.509 certificates and their private keys, enabling applications to securely access these certificates for authentication and encryption purposes.
Keys refer to cryptographic keys used for encryption, decryption, signing, and verification. Azure Key Vault supports the storage of keys, including symmetric keys and asymmetric key pairs. By storing keys in Azure Key Vault, you enhance the security of cryptographic operations.
Tags are not stored in Azure Key Vault. Instead, they are key-value pairs that you can assign to resources like Azure resources, resource groups, or other objects to help organize and categorize them.
Blueprints are not stored in Azure Key Vault. They are used to define a repeatable set of Azure resources that adhere to an organization's standards, patterns, and requirements.

---

### References

[About Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  
[What is Azure Blueprints (Preview)?](https://learn.microsoft.com/en-us/azure/governance/blueprints/overview)  

---

## Q50:

You are responsible for administering a Microsoft Entra environment. To alleviate the workload on your IT department in managing external users, you have been asked to enable self-service account management for your customers.
You need to recommend the most suitable solution.
What solution should you recommend to fulfill this requirement?

Choose the correct answer

- Active Directory Federation Service (AD FS)
- Azure AD B2C
- Microsoft Entra B2B direct connect
- Microsoft Entra B2B

---

### Answer:
- Azure AD B2C

You should recommend Azure Active Directory (Azure AD) B2C. Azure AD business-to-consumer (B2C) is designed for scenarios involving external consumers, such as customers. Azure AD B2C provides robust self-service capabilities, allowing customers to register, manage their profiles, reset passwords, and engage in other account-related activities on their own. It is a suitable choice if you want to provide a self-service functionality for your external customers to manage their accounts.
You should not recommend Microsoft Entra B2B. Microsoft Entra B2B enables external users to utilize their favored credentials for logging into your Microsoft applications and various enterprise software, including software as a service (SaaS) and custom-developed applications. B2B collaboration participants are reflected within your directory, usually designated as guest users. While it facilitates access to resources for external users, it does not provide the same level of self-service account management as Azure AD B2C. Microsoft Entra B2B is more about allowing external entities to access your organization's resources rather than enabling self-service for your customers.
You should not recommend Microsoft Entra B2B direct connect. With this solution you can create a bidirectional and mutually trusted connection with another Microsoft Entra organization to facilitate effortless collaboration. B2B direct connect presently offers support for Teams shared channels, allowing external users to conveniently access your resources using their native Teams instances. While using this solution you can collaborate with external users and grant them access to certain resources, the self-service aspect is more limited compared to Azure AD B2C.
You should not recommend Active Directory Federation Service (AD FS). On-premises Active Directory (AD) with Federation involves using your organization's on-premises AD in conjunction with federation services. It is not directly related to enabling self-service account management for external users, and it might not provide the desired self-service capabilities for customers.

---

### References

[Design for Azure Active Directory B2C (business-to-customer)](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/5-design-business-customer)  

[Design for Microsoft Entra business-to-business (B2B)](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/4-design-business-business)  

[Introduction to Microsoft Entra External ID](https://learn.microsoft.com/en-us/entra/external-id/external-identities-overview)  

[AD FS Overview](https://learn.microsoft.com/en-us/windows-server/identity/ad-fs/ad-fs-overview)   

---

## Q049:

You are managing the Microsoft Entra environment for your organization.
You need to enable secure access for applications and services to Azure resources or applications.
What account or identity should you create for this purpose?

Choose the correct answer

- group Managed Service account (gMSA)
- Service principal identity
- Device identity
- User account

---

### Answer:
- Service principal identity

You should create a service principal identity to enable secure access for applications and services to Azure resources or applications. A service principal is an application identity that you create in Microsoft Entra to provide secure authentication and authorization for applications and services. Service principals allow applications and services to interact with Azure resources using OAuth2 tokens while adhering to controlled access through roles and permissions. They are designed to represent applications, services, or automation tools and play a critical role in enabling secure and controlled access to Azure environment. Service principals are commonly used for scenarios like deploying resources through Azure Resource Manager (ARM) templates, granting permissions to applications, and enabling secure access to APIs and other Azure services.
You should not create a user account, a service account, or a device identity.
User accounts are intended for individual human users, group Managed Service accounts (gMSAs) refer to a traditional concept in on-premises Active Directory, and device identities primarily concern device management in Microsoft Entra ID, rather than authenticating applications or services.

---

### References

[Securing cloud-based service accounts](https://learn.microsoft.com/en-us/entra/architecture/secure-service-accounts)  
[Governing Microsoft Entra service accounts](https://learn.microsoft.com/en-us/entra/architecture/govern-service-accounts)  
[What are user accounts in Microsoft Entra ID?](https://learn.microsoft.com/en-us/training/modules/create-users-and-groups-in-azure-active-directory/2-user-accounts-azure-ad)  
[Secure group managed service accounts](https://learn.microsoft.com/en-us/entra/architecture/service-accounts-group-managed)  
[What is a device identity?](https://learn.microsoft.com/en-us/entra/identity/devices/overview)  

---

## Q048:

A development team is developing a web application App1, which is running on a Azure virtual machine (VM) VM1. App1 stores the data in Azure SQL database DB1. The source code of App1 is stored in a GitHub code repository. A recently performed Static Application Security Testing (SAST) has identified clear text SQL database connection string stored within the code.
You need to recommend a solution to remediate this vulnerability. The solution must minimize costs as well as administrative and development effort.

What should you recommend?

Choose the correct answer

- Azure Storage account
- Microsoft Entra ID
- Azure Key Vault
- BitLocker

---

### Answer:
- Azure Key Vault

You should recommend Azure Key Vault. Azure Key Vault is used to securely store secrets, SQL database connection strings, and certificates. Usually, multi-tier applications need to be authenticated to back-end services. Storing authentication secrets in clear text directly in the code is risky and vulnerable to be revealed to, and misused by, an ineligible user. It is good security practice to separate valuable secrets from application code. Azure Key Vault provides a solution to securely store and manage all the secrets your applications need. By using Azure Key Vault in this scenario you will follow the best security practice.
You should not recommend BitLocker. BitLocker is a solution to encrypt computer hard drives. It provides data protection in case a computer is lost, stolen, or inappropriately decommissioned. In this scenario, the source code is stored in source control solution GitHub. As the SQL database secure string is stored within the code, which is stored in the source code control solution GitHub, encrypting computer hard drives does not meet the requirement
You should not recommend Microsoft Entra ID. Microsoft Entra is Microsoft's Cloud Identity and Access Management Solution. Although you could use Microsoft Entra to allow web applications to authenticate to SQL database, in this scenario, it would require additional administrative and development effort.
You should not recommend Azure Storage account. Azure Storage account provides a container to store data, such as files, queues, tables, and disks. The data can be accessed securely from anywhere around the world. However, Azure Storage account is not designed to hold an application's secrets. You should use Azure Key Vault in this scenario.

---

### References

[Design service principals for applications](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/9-two-design-service-principals)  
[Design for Azure Key Vault](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/10-design-for-azure-key-vault)  
[BitLocker overview](https://learn.microsoft.com/en-us/windows/security/operating-system-security/data-protection/bitlocker/)  
[Use Microsoft Entra authentication](https://learn.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview?view=azuresql)  

---

## Q047:

Your company has a Microsoft Entra subscription. Recent security breaches have resulted from inappropriate or outdated access privilege assignments. You are designing access policies for different departments throughout your company to ease the problem. You want to implement a solution that:

- Provides permissions only when needed
- Lets you set start and end dates for permission assignments
- Sends notifications when privileged roles are activated

You need to identify the requirements for implementing this security design.
What should you recommend? To answer, select the correct recommendations from the drop-down menus.

Choose the correct options

Microsoft Entra ID minimum license:
P2

Microsoft Entra feature:
Privileged Identity Management

---

### Answer:

You should recommend P2 as the Microsoft Entra ID license implementation of the Privileged Identity Management (PIM) feature. PIM lets you manage, control, and monitor access to important resources through features that include:

- Just-in-time (JIT) privileged access control
- Time-bound access to resources with start and end dates
- Required approval to activate privileged roles
- Notification when privileged roles are activated

This gives you the just-in-time and time-sensitive controls needed with the desired notifications. Full implementation of PIM requires Microsoft Entra P2.
You should not recommend Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to help protect against identity-based threats, including monitoring for risks and implementing remediation steps.
You should not recommend Microsoft Entra Connect. Microsoft Entra Connect is used to provide synchronization between on-premises AD and Microsoft Entra ID.
You should not recommend Conditional Access. Conditional Access is used to block or allow access to domain resources based on qualifying criteria such as:

- User account or group membership
- Physical location
- Accessing device

Conditional access does not provide the types of protection needed. Conditional access can identify some risky behaviors and force actions such as password changes or multi-factor authentication.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  

[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  

[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  

[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  

---

## Q046:

Your company has a Microsoft Entra ID P2 environment that hosts over 10,000 licensed users. The domain supports over 100 business-critical applications. Your company is concerned about its ability to recognize and respond to external threats for the applications.
You need to recommend a solution that supports identifying sign-in risks and provides remediation strategies.

What should you recommend?

Choose the correct answer

- Microsoft Entra Connect Health
- Azure Policies
- Microsoft Entra ID Protection
- Microsoft Entra Privileged Identity Management (PIM)

---

### Answer:
- Microsoft Entra ID Protection

You should recommend Microsoft Entra ID Protection. Identity Protection enables an organization to:
Automate detection and remediation of identity-based risks
Investigate risks based on portal data
Export risk data for additional analysis
You can use Identity Protection to detect risks like anonymous IP address use for a user or leaked credentials, and implement remediation steps. Full implementation of Identity Protection requires a Microsoft Entra ID P2 license.
You should not recommend Microsoft Entra Privileged Identity Management (PIM). PIM lets you manage, control, and monitor access to important resources through features that include:
Just-in-time (JIT) privileged access control
Time-bound access to resources with start and end dates
Required approval to activate privileged roles
Notification when privileged roles are activated
Access audit history
PIM does not provide for the protection and remediation needed in this scenario.
You should not recommend Microsoft Entra Connect Health. Connect Health is not a security monitoring and remediation solution, though it provides monitoring of your on-premises identity infrastructure when supporting both an on-premises AD and Microsoft Entra ID. It provides information such as performance monitoring and usage analytics.
You should not recommend Azure Policies. Azure Policy is used to ensure and assess compliance to defined standards. This can include regulatory compliance, resource consistency, security, costs, and so forth. Azure Policy works by comparing resource properties with policy definitions that act as business rules.


---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  
[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)  

---

## Q045:

Company1 has a Microsoft Entra tenant named company1.com.
Company1 uses groups to provide access to resources. Each employee has a user in company1.com. Employees frequently change company positions and often still have access to resources they no longer need.
You need to recommend a solution to automatically remove users from groups that they no longer need to be in.

What should you recommend?

Choose the correct answer

- Microsoft Entra ID Protection
- Conditional access
- Group expiration
- Access review


---

### Answer:
- Access review

You should recommend access review. Access review can be performed once or periodically and you can configure who should perform the review, either the group owner or the group members themselves. At the end of the evaluation period, users who no longer need membership can be automatically removed from the group. Based on the configuration, users for whom the review is not performed can also be removed from the group automatically.
You should not recommend conditional access. Conditional access enables you to control access to cloud apps. You can specify who can access the app, from which devices, which threat level is allowed, and any other conditions that must be met before the user can access the app.
You should not recommend group expiration. Group expiration can be configured only for Office 365 groups, not security groups. Only security groups can be granted access to resources, not Office 365 groups.
You should not recommend Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to detect and prevent risky sign-ins. It cannot be used to remove users from groups automatically.

---

### References

[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)   
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  
[Configure the expiration policy for Microsoft 365 groups](https://learn.microsoft.com/en-us/entra/identity/users/groups-lifecycle)  

---

## Q044:

You create an Azure subscription for your company. You plan to create a resource group for each department in your company. You want to allow only members of a particular department to create resources in the resource group assigned to their department.
You need to identify an Azure feature to support this design.
Which Azure feature should you use?

Choose the correct answer

- Policies
- Locks
- Role-based access control (RBAC)
- Initiatives

---

### Answer:
- Role-based access control (RBAC)

You should use Role-based access control (RBAC). RBAC helps you control which users or groups have access to resources, and which permissions they have on those resources. You can apply RBAC access to a management group, a subscription, a resource group, or a resource. Permissions are inherited from higher scopes to lower scopes.

You should not use locks. Locks allow you to prevent resources from being modified or deleted. This helps you prevent unexpected or accidental changes. The two types of lock levels are CanNotDelete (Delete in the Portal) and ReadOnly (Read-only in the Portal).

You should not use policies. Policies allow you to configure rules that control the types of Azure resources that are allowed in a subscription or resource group. For example, you can prevent VMs of a certain size from being deployed.
You should not use initiatives. Initiatives are groups of policies. Initiatives allow you to manage a set of policies together.

---

### References

[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)  
[Lock your resources to protect your infrastructure](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json)  
[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  

---

## Q043:

A medium-sized company is using Microsoft Entra ID to control access to their applications and services that are deployed in Azure. A recent security audit shows that the Global Administrator group is populated with people who do not need such broad access to the resources.
You need to restrict the resources appropriately. In addition, you want to be able to grant elevated access only for specific periods of time, such as for only an hour or a day or for situations when a person needs temporary access for a specific task.
What two actions should you perform? Each correct answer presents part of the solution.

Choose the correct answers

- Assign more granular roles to the administrators according to their functions.
- Use managed identities to further restrict access to the resources.
- Use Privileged Identity Management (PIM) to create additional rules for access.
- Add conditional access policies to your current access restrictions.

---

### Answer:
- Assign more granular roles to the administrators according to their functions.
- Use Privileged Identity Management (PIM) to create additional rules for access.

You should assign more granular roles to the administrators according to their functions. Unless your organization is very small, the administrators are responsible for specific aspects of a system or only for certain resources. The tasks that are performed by particular administrators should map directly to their roles. You should always assign the least possible privilege that allows users to perform their assigned tasks.
You should also use Privileged Identity Management (PIM) to create additional rules for access, including just-in-time (JIT) privileged access to Azure resources. This includes activating access for only a specific length of time, such as one hour or one day.
You should not use managed identities to further restrict access to the resources. Managed identities provide access for resources, not for users. For example, if an application needs to retrieve secrets from a key vault, it can use its managed identity to retrieve a token.
You should not add conditional access policies to your current access restrictions. Conditional access policies are intended to address issues that are related to remote access and bring your own device (BYOD) workplaces. You can, for example, allow or block access for certain locations or device types.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[Securing privileged access for hybrid and cloud deployments in Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/identity/role-based-access-control/security-planning)  
[What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)  
[What is Conditional Access?](https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview)  

---

## Q042:

Your organization has 10 Azure subscriptions, one for each branch office. Five offices are located in the eastern region, and the other five are located in the western region. Each region is grouped into a management group. Each subscription has multiple resource groups, and each resource group has multiple resources. You want to ensure that only the IT personnel in each region have permission to create or modify the Azure resources for that region.
You need to assign role-based access control (RBAC) permissions to the IT personnel. Your solution must require minimal maintenance.
Which scope should you apply the permissions to?

Choose the correct answer

- Management group
- Resource group
- Subscription
- Resource

---

### Answer:
- Management group

You should apply role-based access control (RBAC) permissions to the management group scope. A management group allows you to group multiple subscriptions for RBAC assignment. In this scenario, each region has multiple subscriptions, and only the IT personnel in a region should be able to create and modify the Azure resources for that region. By creating two management groups, one for each region, you can apply RBAC permissions to a management group and have them affect all the resources in all subscriptions for that management group.
You should not apply RBAC permissions to the subscription scope. This would require you to manage RBAC permissions for 10 subscriptions instead of two management groups.
You should not apply RBAC permissions to the resource group scope. This would require you to manage multiple RBAC permission assignments for multiple resource groups across 10 subscriptions.
You should not apply RBAC permissions to the resource scope. This would require you to manage multiple RBAC permission assignments for every resource group across 10 subscriptions.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q041:

Your organization has four Azure subscriptions, one for each branch office. Each subscription has multiple resource groups, and each resource group has multiple resources. You want to ensure that only the IT personnel in each branch office have permission to create or modify the Azure resources for that office.
You need to assign role-based access control (RBAC) permissions to the IT personnel. Your solution must require minimal maintenance.
Which scope should you apply the permissions to?

Choose the correct answer

- Management group
- Resource group
- Resource
- Subscription

---

### Answer:
- Subscription

You should apply RBAC permissions to the subscription scope. Each office has its own subscription, and only the IT personnel in a branch office should be able to create and modify the Azure resources for that office. By applying the permissions to a subscription, you allow those permissions to affect only that subscription and the resources in that subscription.
You should not apply RBAC permissions to the management group scope. A management group allows you
to group multiple subscriptions for RBAC assignment. In this scenario, RBAC permissions should be assigned to each subscription, not across multiple subscriptions.
You should not apply RBAC permissions to the resource group scope. This would require you to manage multiple RBAC permission assignments for multiple resource groups in a single subscription.
You should not apply RBAC permissions to the resource scope. This would require you to manage multiple RBAC permission assignments for every resource group in a subscription.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q040:

Your organization has one Azure subscription. You plan to deploy four SQL Server virtual machines (VMs) to that subscription. Developers and IT personnel must be able to connect to these VMs. However, only the IT personnel should have permission to modify or delete the SQL Server VMs.
You need to design an authorization strategy. Your solution should follow the principle of least privilege.

Which strategy should you use?

Choose the correct answer

- Deploy the SQL Server VMs in a separate resource group. Add the developers to a Developers group and the IT personnel to an IT group. Assign the Reader role to the Developers group and the Owner role to the IT group at the resource group level.

- Deploy the SQL Server VMs in a separate resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the resource group level.

- Deploy the SQL Server VMs in an existing resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the subscription level.

- Deploy the SQL Server VMs in an existing resource group. Add the developers to a Developers group and the IT personnel to an IT group. Assign the Reader role to the Developers group and the Owner role to the IT group at the resource group level.

---

### Answer:
- Deploy the SQL Server VMs in a separate resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the resource group level.

You should deploy the SQL Server VMs in a separate resource group and delegate permissions at the resource group level. Permissions will be inherited by virtual machines in the resource group. The Virtual Machine Contributor role enables you to manage virtual machines. If you delegate this role at the resource group level, you will be able to manage virtual machines in the resource group. Users do not need any additional permissions to be able to connect to existing virtual machines, so you should not assign Developers to any role.
You should not assign the Developers group to the Reader role. This action would assign permissions to the developers that are not required to perform their task of connecting to the virtual machine, and would not conform to the principle of least privilege.
You should not deploy SQL Server VMs to the existing resource group and delegate permissions at the
resource group level. If you take this action, permissions would be inherited by all objects in the resource group, not only by SQL Server VMs. This action would not conform to the principle of least privilege.
You should not assign the Virtual Machine Contributor role to the IT group at the subscription level. If you take this action, permissions would be inherited by all resource groups in the subscription and by all VMs. This action would assign the IT group permissions that are not required and would not conform to the principle of least privilege.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q039:

Your company has a Microsoft Entra infrastructure. Your company wants to securely share applications and services with guest users from another organization. The guest organization has an on-premises Active Directory (AD) domain but it does not support Microsoft Entra ID.
You need to implement a solution that gives guest users access to select resources while minimizing any additional user account management overhead. You want to minimize introducing any additional security risks.
What should you do?

Choose the correct answer

- Require the guest organization to implement Microsoft Entra ID.
- Create a user account in your Microsoft Entra for each guest user.
- Implement business-to-business (B2B) collaboration.
- Configure synchronization between the guest organization's on-premises AD and your Microsoft Entra ID.

---

### Answer:
- Implement business-to-business (B2B) collaboration.

You should implement business-to-business (B2B) collaboration. With B2B collaboration the guest users must have valid email addresses but an IT infrastructure for user management, such as Microsoft Entra, is not required. The guest organization has full responsibility for managing the external guest users. Your company will invite guest users through a simple invitation and redemption process. You configure conditional access policies to control access to your company content..
You should not create a user account in your Microsoft Entra for each guest user. This is not necessary and would require additional management overhead for your organization.
You should not configure synchronization between the guest organization's on-premises AD and your Microsoft Entra ID. There is no need for any type of synchronization between the organizations.
You should not require the guest organization to implement Microsoft Entra ID. This is not a requirement for the guest organization.

---

### References

[B2B collaboration overview](https://learn.microsoft.com/en-us/entra/external-id/what-is-b2b)  
[Add Microsoft Entra B2B collaboration users in the Microsoft Entra admin center](https://learn.microsoft.com/en-us/entra/external-id/add-users-administrator)  
[Microsoft Entra B2B collaboration FAQS](https://learn.microsoft.com/en-us/entra/external-id/faq)  

---

## Q038:

Your company has an on-premises Active Directory (AD) forest and a Microsoft Entra ID P1 tenant. All Microsoft Entra users are assigned a P1 license. You plan to have users use the same usernames and passwords for on-premises and Microsoft Entra authentication. Password changes are currently managed through the helpdesk to ensure that passwords remain synchronized.
Your company is considering deploying Microsoft Entra Connect on the on-premises network. You need to identify features from this action that will help to reduce the management overhead for your network infrastructure and helpdesk personnel.
Which two features could you use? Each correct answer presents a complete solution.

Choose the correct answers

- Privileged Identity Management (PIM)
- Identity Protection
- Password writeback
- Periodic access review
- Self-service password reset

---

### Answer:
- Password writeback
- Self-service password reset

You should not use periodic access review. This does not require Microsoft Entra Connect and would not reduce the helpdesk workload. Access review is an aid to manage group memberships, access to enterprise applications, and role assignments.
You should not use Privileged Identity Management (PIM). PIM is not a feature specific to a hybrid infrastructure using Microsoft Entra Connect. PIM lets you manage, control, and monitor access to important resources through features that include:
Just-in-time (JIT) privileged access control
Time-bound access to resources with start and end dates
Required approval to activate privileged roles
Notification when privileged roles are activated
Access audit history
You should not use Identity Protection. This is not a feature specific to a hybrid infrastructure using Microsoft Entra Connect. Identity Protection provides a way to detect and remediate identity-based risks. It can generate reports to identify risky users and risky sign-ins. It also supports a risk detection report. Identity Protection requires a Microsoft Entra P2 license.


---

### References

[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  
[How does self-service password reset writeback work in Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/authentication/concept-sspr-writeback)  
[Self-service password reset frequently asked questions](https://learn.microsoft.com/en-us/entra/identity/authentication/passwords-faq) 
[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)  

---

## Q037:

You have a hybrid identity environment that includes an on-premises Active Directory environment and a Microsoft Entra ID.
You need to create an authentication solution that verifies the following:

- That on-premises AD security policies are applied.
- That passwords are validated against your on-premises AD.
- That passwords are not stored in the cloud.

What solution should you use?

Choose the correct answer

- Password hash synchronization
- Pass-through authentication
- Federated authentication
- Single sign-on (SSO)

---

### Answer:
- Pass-through authentication

You should use pass-through authentication. This authentication method does not store passwords in the cloud (Microsoft Entra ID). Instead, passwords are stored only in the on-premises AD. Consequently, when an authentication request is made, only the on-premises AD is used to validate the request. A lightweight agent is used on-premises to communicate with Microsoft Entra ID.
You should not use federated authentication. Federation allows an external, third-party system to authenticate users. This can include biometrics, smart-cards, and so on. Federation does not authenticate against on-premises AD.
You should not use password hash synchronization. This is the most basic and least-effort solution for a hybrid authentication system. Passwords are stored in the cloud (Microsoft Entra ID). This method is best for simple scenarios where users need access only to Microsoft Entra resources, such as Office 365 and SaaS apps.
You should not use single sign-on (SSO). SSO is an umbrella term that means users only have to sign in once, after that they are automatically authenticated to additional resources. You should not use SSO when you have to sync or store passwords.

---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[User sign-in with Microsoft Entra pass-through authentication](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/how-to-connect-pta)  

---

## Q036:

Your company has an on-premises Active Directory (AD) domain that uses AD Connect to access Microsoft Entra ID. Employees use single sign-on (SSO) from corporate devices or their own devices to gain access to resources to do their jobs. IT administrators reported that the Remote Desktop Protocol (RDP) port of the on-premises AD server was open to perform a one-time administrative task.
After reports of identity theft at a partner company, management is particularly concerned about identity security.
You need to strengthen your security policy to mitigate risks associated with identity compromise.
Which two actions should you perform? Each correct answer presents part of the solution.

Choose the correct answers

- Enable Microsoft Entra conditional access.
- Disable password hash synchronization.
- Close Remote Desktop Protocol (RDP) ports on the on-premises AD server.
- Require strong passwords that expire each month.

---

### Answer:
- Enable Microsoft Entra conditional access.
- Close Remote Desktop Protocol (RDP) ports on the on-premises AD server.

You should enable Microsoft Entra conditional access. This ensures that users gain access to corporate resources only from devices that meet security standards.
You should also close Close Remote Desktop Protocol (RDP) ports on the on-premises AD server. This prevents unauthorized remote access to the server. The only ports that should be open are those that are required for the connection between the AD server and AD Connect.
You should not disable password hash synchronization. You should enable this to provide an extra measure of security. AD Connect compares password hashes to known compromised passwords to make sure that a user's password has not been compromised.
You should not require strong passwords with very frequent expiration. Microsoft research has shown that this restrictive policy causes users to choose passwords that are easy to guess.

---

### References

[Five steps to securing your identity infrastructure](https://learn.microsoft.com/en-us/azure/security/fundamentals/steps-secure-identity)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection) 
[What is password hash synchronization with Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-phs)  
[Hybrid Identity Required Ports and Protocols](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/reference-connect-ports)  

---

## Q035:

Your company has a Microsoft Entra tenant named Company.com. The company has a Marketing, Finance and Research department.
You are designing multi-factor authentication (MFA) for Company.com.
You need to ensure that MFA is only implemented for users in the Research department.
Which requirement should you include in the design?

Choose the correct answer

- Create a conditional access policy.
- Configure authentication methods.
- Implement Microsoft Entra ID Protection.
- Implement Microsoft Entra Privileged Identity Management (PIM).

---

### Answer:
- Create a conditional access policy.

You should include a requirement to create a conditional access policy. A conditional access policy is used to grant access to cloud apps. A conditional access policy can be targeted to users and groups. One of the access controls that you can require in a conditional access policy is the use of multi-factor authentication (MFA).
You should not include a requirement to configure authentication methods. In authentication methods, you can configure custom smart lockout, custom banned passwords and password protection for Active Directory Domain Services. You cannot enable or configure MFA in authentication methods.
You should not include a requirement to implement Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to detect and prevent risky sign-ins. One of the Microsoft Entra ID Protection features is the ability to require MFA for sign-in, but only if sign-in is detected as risky.
You should not implement Microsoft Entra Privileged Identity Management (PIM). PIM is used for just-in- time activation of privileged roles. You can use PIM to require MFA if a user wants to activate privileged role membership. PIM cannot be used to require MFA at user sign in.

---

### References

[What authentication and verification methods are available in Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/authentication/concept-authentication-methods)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  


---

## Q034:

You are designing a hybrid identity solution for your organization. It consists of an on-premises Active Directory (AD) domain and a Microsoft Entra tenant. Your solution must meet the following requirements:
Allow a user who logs in to their on-premises account to automatically authenticate in Microsoft Entra to access Azure services.
Minimize administrative effort to deploy and maintain.
You need to set up authentication.

Which mechanism should you use?

Choose the correct answer

- Single sign-on (SSO) with password hash sync
- Federation without password hash sync
- Federation with password hash sync
- Single sign-on (SSO) with pass-through authentication

---

### Answer:
- Single sign-on (SSO) with password hash sync

You should use single sign-on (SSO) with password hash sync. This requires the least amount of administrative effort for deployment and maintenance. The on-premises AD domain services stores the user password as a hash. AD Connect then retrieves the user password hash and hashes that hash. It then sends the second hash to Microsoft Entra ID. When the user logs in to Microsoft Entra ID, Microsoft Entra ID compares the password hashes.
You should not use SSO with pass-through authentication. This requires you to install a pass-through authentication agent on an on-premises server.
You should not use federation with or without password hash sync. With this authentication method,
Microsoft Entra delegates authentication to a separate trusted authentication system, such as AD Federation
Services (FS). This requires you to deploy and maintain additional software.

---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[What is password hash synchronization with Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-phs)  
[Implement password hash synchronization with Microsoft Entra Connect Sync](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/how-to-connect-password-hash-synchronization)  

---

## Q033:

A recent audit of your Microsoft Entra environment has resulted in a requirement to retain your activity logs for 15 years.
You need to meet the requirement, while minimizing complexity and costs.

What should you recommend?

Choose the correct answer

- Azure Service Bus
- Azure Log Analytics workspace
- Azure Event Hubs
- Azure storage account

---

### Answer:
- Azure storage account

You should recommend an Azure storage account. Azure storage accounts provide a cost-effective and relatively simple way to retain logs for a long period while minimizing complexity. You can configure your logs to be stored in an Azure storage account, which offers durable and scalable storage capabilities. Azure Storage provides a straightforward solution for long-term retention without the need for complex configurations or additional services. For the specific scenario of retaining logs for a long duration with minimal complexity and costs, an Azure storage account is the recommended choice.
You should not recommend an Azure Log Analytics workspace. An Azure Log Analytics workspace is a service designed for collecting, analyzing, and visualizing data from your resources. While it provides powerful capabilities for log analysis and monitoring, it is not suited for long-term storage.
You should not recommend Azure Event Hubs. Azure Event Hubs is a scalable event streaming platform that can ingest and process large volumes of data in real-time. While it provides flexibility, it is more geared towards real-time data streaming rather than long-term storage.
You should not recommend Azure Service Bus. Azure Service Bus is a messaging service designed for reliable communication between different applications and services. It provides features like queues and topics for asynchronous messaging patterns. However, for the specific requirement of retaining activity logs for 15 years while minimizing complexity and costs, Azure Service Bus is not the most suitable choice.

---

### References

[How to archive Microsoft Entra activity logs to an Azure storage account](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/howto-archive-logs-to-storage-account)  
[Microsoft Entra data retention](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/reference-reports-data-retention)  
[Azure Event Hubs - A real-time data streaming platform with native Apache Kafka support](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  
[Log Analytics workspace overview](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  


---

## Q032:

You are managing an Azure hybrid environment with resources on premises and in the cloud. You are requested to collect all diagnostic and audit logs centrally in an external third-party security information and event management (SIEM) system.
You need to recommend the most suitable solution to route resource logs to an external third-party SIEM. The solution must provide maximum flexibility and customization in terms of real-time processing and routing logs to different destinations.

What should you recommend?

Choose the correct answer

- Azure Event Hubs
- Azure storage account
- Azure Log Analytics workspace
- Azure Service Bus

---

### Answer:
- Azure Event Hubs

You should recommend Azure Event Hubs. Azure Event Hubs is designed for high-throughput, real-time data streaming and provides the flexibility to process and route logs to various destinations, including an external third-party SIEM system. You can configure your resources to send their diagnostic and audit logs to an Azure Event Hubs and then use custom applications or integrations to transform and forward the logs to your external SIEM. This approach allows you to implement custom processing logic, filtering, and formatting before sending the logs to the SIEM, ensuring that they meet the requirements of the third-party system. Azure Event Hubs' scalability, data distribution capabilities, and real-time processing make it a strong candidate for managing logs in a hybrid environment and integrating them with external SIEM solutions.
While Azure Log Analytics and an Azure storage account have their use cases, neither provides the same level of flexibility and real-time processing capabilities as Azure Event Hubs for sending logs to an external third-party SIEM system, especially when the goal is to process and route logs to different destinations. A partner solution could also be a strong choice if it aligns well with your requirements and provides the necessary customization and integration options.
Azure Log Analytics workspace is a service that helps you to collect, analyze, and visualize data from your resources. It is commonly used for monitoring and managing Azure resources, as well as on-premises resources through agents. You can configure diagnostic and audit logs from your resources to be sent to a Log Analytics workspace. Once collected, you can perform queries, create alerts, and build custom dashboards to monitor and troubleshoot your environment. While Log Analytics is powerful for Azure monitoring, it might not be the best option for sending logs to an external third-party SIEM system.
Azure storage accounts allow you to store various types of data, including logs, in different storage services like Blobs, Tables, Queues, and Files. While you could store your resource logs in an Azure storage account, it is primarily a storage solution. You would need to implement a mechanism or application to regularly fetch logs from the storage and send them to your third-party SIEM system. This approach would require more custom development and might not provide the same level of log processing and real-time integration as Azure Event Hubs.
Azure Service Bus is primarily designed for reliable messaging and communication between applications. While Azure Service Bus is a good choice for facilitating information exchange among various applications and services through messages, it may not be the optimal solution for the task of centralizing logs. In this context, a more fitting solution is Azure Event Hubs. In the broader sense, a message takes on the form of a container adorned with metadata, serving as a vessel for encapsulating data. This data has the capacity to encompass a variety of information types, including structured data encoded in prevalent formats such as JSON, XML, Apache Avro, and Plain Text.

---

### References

[Azure Event Hubs: A real-time data streaming platform with native Apache Kafka support](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)   
[Log Analytics workspace overview](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview)    
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  

---

## Q031:

You are planning the migration of a large on-premises IT system to Azure. You want to ensure that proper alerting and metric-collection systems are configured for all resources once they are deployed and in production.
To ensure that the correct Azure functionality is used within the overall monitoring infrastructure, you need to identify the most appropriate feature or tool for each requirement.
What solution should you use for each scenario? To answer, drag the appropriate tool or feature to each requirement. A solution may be used once, more than once, or not at all.
Drag and drop the answers

Tool or feature:

Activity Logs
Azure Monitor
Log Analytics Workspace
Resource Manager Templates
Azure Dashboards
Smart Groups

Requirement:

You need to create standardized deployments of Azure resources that can ensure that standard alerting rules are put into place:
Resource Manager Templates

You need a service to centralize all metrics from Azure resources and create alerts based on those metrics:
Azure Monitor

You need subscription-level events that occur within Azure and which can be used to identify changes that are made to resources:
Activity Logs

You need to do manual analysis of collected Azure log data using the Kusto query language:
Log Analytics Workspace

---

### Answer:

You should use Azure Resource Manager (ARM) templates to support standardized deployments. ARM templates are scripts that can be authored to deploy nearly any resource type within Azure. Deploying a standardized alert configuration as part of the resource at the time of original deployment is a powerful automation feature of Azure.
You should use Azure Monitor to centralize all metrics from Azure resources and create alerts based on those metrics. Azure Monitor collects all log and metric data that is collected within Azure. This provides you with a centralized location for collecting the data.
You should use Activity logs to obtain subscription-level events that occur within Azure and that can be
used to identify changes that are made to resources. Core Azure alerts are surfaced through Activity Logs.
These include service-level alerts, in addition to logging all activities made on resources themselves, and
support alerts at the subscription scope. You should use Log Analytics workspaces to do manual analysis of collected Azure log data using the Kusto
query language. This is a powerful tool that is useful for performing a detailed analysis of historical metrics
or activities for systems within Azure.

---

### References

[Understand the structure and syntax of ARM templates](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/syntax)  
[Azure Monitor Documentation](https://learn.microsoft.com/en-us/azure/azure-monitor/)  
[Azure Monitor data platform](https://learn.microsoft.com/en-us/azure/azure-monitor//data-platform)  
[Overview of Azure platform logs](https://learn.microsoft.com/en-us/azure/azure-monitor/data-sources)  
[Log queries in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-query-overview)  


---

## Q030:

You are designing a monitoring solution for a microservice solution hosted in an Azure Kubernetes Service (AKS) cluster.
You need to recommend a monitoring solution that meets the following requirements:
Measure the memory consumption of cluster nodes.
Monitor the health of pods and deployments.
Create alerts when persistent volumes are more than 80% full.
Visualize the metrics and dashboards within the Azure Portal.

What should you recommend?

Choose the correct answer

- Grafana
- Container insights
- Application Insights
- VM insights

---

### Answer:
- Container insights

You should recommend Container insights. You can use Container insights to monitor workloads running Kubernetes-based solutions in Azure. You can monitor and create alerts for all components in the cluster, including node memory and processor usage, pod and deployment health, and persistent volume usage. You can access the Container insights dashboards and metrics within the Azure Portal or integrate them with external monitoring tools, such as Prometheus or Grafana.
You should not recommend VM insights. You can use VM insights to monitor the performance and health of virtual machines (VMs). It should be possible to use VM insights to monitor some cluster nodes; however, it is not possible to monitor the internal resources in the cluster, like pods and persistent volumes.
You should not recommend Application Insights. You can use Application Insights to monitor application
performance, error rate caused by exceptions, and how users interact with the application, including most- visited pages and other metrics.
You should not recommend Grafana. You can use Grafana to visualize and create dashboards based on multiple sources of metrics. You can use the Azure Monitor data source plugin to query data from Container insights to create custom dashboards. However, you cannot monitor an AKS cluster only with Grafana.

---

### References

[Azure Monitor overview](https://learn.microsoft.com/en-us/azure/azure-monitor/overview)  
[Container insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-overview)  
[Overview of VM insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-overview)  
[Application Insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)  
[Monitor your Azure services in Grafana](https://learn.microsoft.com/en-us/azure/azure-monitor/visualize/grafana-plugin)  

---

## Q029:

You are migrating your on-premises workloads running on Windows and Linux virtual machines to Azure. During the migration, the applications will run on a hybrid model, with some components running in both Azure and on-premises servers to ensure no downtime for the application during the migration process.
You need to design a monitoring strategy solution so that the logs and metrics are available in a centralized location in Azure. The logs should be stored for at least 18 months and must be easy to view and query.
Which solution should you configure?

Choose the correct answer

- A Storage Account
- Log Analytics
- Event Hubs

---

### Answer:
- Log Analytics

You should configure Log Analytics for this scenario. Log Analytics provides you with capabilities to use the Kusto query language to read the logs and metrics. The logs and metrics can be collected using Log Analytics agents, which can be deployed on all the virtual machines running on-premises or in Azure. The data collected can be retained for a maximum of two years in the workspace. You can create a single workspace for all the logs to centralize the management.
You should not recommend a storage account. This is a cheap solution to store the logs of Azure resources through diagnostic settings. It cannot ship logs from virtual machines directly to the storage account. Even if you can get the logs to the storage account, it is not easy to query them.
You should not configure Event Hubs. Event Hubs is a useful solution when you need to send the logs and metrics to other third-party solutions. Azure diagnostic logs and metrics can be sent to Event Hubs. You can directly send logs to the event hub from the virtual machines using the Linux diagnostic extension 4.0 for Linux virtual machines and Windows Azure Diagnostic extension (WAD) for Windows virtual machines. Despite this, the logs cannot be viewed from the event hub directly and it would need to be integrated with another solution that can ingest the logs and provide querying capabilities. Also, event hubs do not have permanent storage; the retention is short, with seven days maximum for the standard tier.

---

### References

[Azure Monitor Logs cost calculations and options](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/cost-logs)  
[Log Analytics tutorial](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-tutorial)  
[Log Analytics agent overview](https://learn.microsoft.com/en-us/azure/azure-monitor/agents/log-analytics-agent)  
[Use the Linux diagnostic extension 4.0 to monitor metrics and logs](https://learn.microsoft.com/en-us/azure/virtual-machines/extensions/diagnostics-linux?tabs=azcli)  
[Send data from Windows Azure diagnostics extension to Azure Event Hubs](https://learn.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-stream-event-hubs)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?tabs=CMD)  

---

## Q026-028:

Your company has a Line of Business (LOB) application running in the Azure cloud. The application should be integrated with an enterprise resource planning (ERP) system running from your company's on-premises main office. Due to compliance reasons, these applications should not receive network traffic from the public internet. Your company's main office has 500 workstations and 100 employees working from home with company-managed laptops that use the LOB application.

You need to configure a connectivity solution that meets the following requirements:

- Provide all employees working from home with secure connectivity to cloud-based and on-premises services.
- Connect the on-premises main office to the Azure network.

---

### References

[VPN Gateway design](https://learn.microsoft.com/en-us/azure/vpn-gateway/design)  
[What is Azure VPN Gateway?](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways)  
[What is Azure Express Route?](https://learn.microsoft.com/en-us/azure/expressroute/expressroute-introduction)  
[About VPN devices and IPsec/IKE parameters for Site-to-Site VPN Gateway connections](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpn-devices)  

---

## Q028:

Solution: You configure a Point-to-Site (P2S) VPN connection for the remote employees and an Express Route between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should provide a connection to give remote employees access to the LOB application through a Point-to-Site (P2S) VPN tunnel. Additionally, you should use an Express Route connection to connect the on-premises head office to the Azure cloud network.

---

## Q027:

Solution: You configure an Express Route connection for the remote employees and between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
No


---

## Q026:

Solution: You configure a Point-to-Site (P2S) VPN connection for remote employees and a Site-to-Site VPN connection between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should use a P2S VPN tunnel to provide a connection for remote employees to access the LOB application and a Site-to-Site VPN connection between the on-premises main office and the Azure network. Access to the applications would be only available inside the on-premises network.

---

## Q022-025:

You are assisting a startup that is planning to launch an online retail platform that sells a wide range of products. The platform's success depends on providing a seamless shopping experience to customers while efficiently managing unpredictable traffic spikes during promotions and sales events. The platform requires additional operating system configuration and control over the server resources.
You need to recommend a solution to meet requirements for significantly increased traffic during Black Friday promotion sales, so that the system can automatically handle the increased load in the most efficient way.

---

### References

---

## Q025:

Solution: You recommend implementing a VM autoscaling mechanism.

Does this solution meet the goal?


---

### Answer:
Yes

This solution meets the goal. Implementing an autoscaling mechanism enables the gradual reduction of the number of virtual machines (VMs) and containers as traffic increases and then returns to normal levels after the promotion. Autoscaling mechanisms automatically adjust resources based on demand, helping to conserve resources and optimize costs. Azure Virtual Machines autoscaling is a feature that allows you to dynamically adjust the number of VM instances based on demand, ensuring optimal performance and cost efficiency. It automates the process of scaling up or down based on predefined rules and conditions, so you can accommodate varying workloads without manual intervention. This feature is particularly useful when your application experiences fluctuations in traffic or resource utilization.

---

###  References

[Virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/overview)
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)  

---

## Q024:

Solution: You recommend microservice architecture to run each service in its own isolated containers.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The microservice architecture recommended allows different services, such as inventory updates, cart calculations, and payment processing, to run independently in isolated containers. This promotes modularity, scalability, and efficient resource utilization. Microservice architecture is an architectural style used in software development to design and build applications as a collection of small, independent, and loosely coupled services. In contrast to monolithic architectures, where the entire application is developed as a single unit, microservices break down the application into smaller, independently deployable services that communicate with each other through well-defined APIs. Each service in a microservices architecture focuses on a specific business capability and can be developed, deployed, and scaled independently. Microservices, each running in its own isolated container, offering control over server resources. Containers allow for customization of the operating system configuration and resource allocation. You can scale individual microservices independently.

---

###  References

[Microservices architecture design](https://learn.microsoft.com/en-us/azure/architecture/microservices/)  
[Microservices assessment and readiness](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/microservices-assessment)  

---

## Q023:

Solution: You recommend to request your IT department to provision additional virtual machines as needed.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. Although requesting additional virtual machines could help to scale resources, it might not be as automatic and dynamic as needed to handle sudden traffic spikes efficiently. Manually provisioning VMs can be time consuming and may not respond quickly to sudden traffic spikes. 

---

###  References

[Virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/overview)
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)  

---

## Q022:

Solution: You recommend serverless computing
Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. Serverless computing, often referred to as "serverless", is a cloud computing model that allows developers to build and run applications without the need to manage underlying infrastructure or servers. In a serverless architecture, developers focus on writing code to implement specific functions or tasks, and the cloud provider takes care of provisioning, scaling, and managing the necessary compute resources. This abstraction of infrastructure management frees developers from the operational complexities of traditional server management. While serverless platforms can automatically scale resources to handle traffic spikes, they do not provide server control, which might not align with the requirement for additional operating system configuration and control over server resources.

---

###  References

[Serverless Computing](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-serverless-computing)  
[Create serverless applications](https://learn.microsoft.com/en-us/training/paths/create-serverless-applications/)  

---

## Q017-021:

> Overview

You work for CompanyA, a large-scale online retail store company which sells electronic products. 
The company has operations in the North America and Europe regions.

> on-premise Environment

The company hosts the application from two data centers located in New York and London. The data centers are connected using high-speed VPN connections. The application is a monolith application deployed on multiple virtual machines hosted on Hyper-V. SQL Database is used to store the application's data. The product asset files, like images and videos, are stored in Network Attached Storage (NAS) file shares.

> Planned Changes

CompanyA wants to modernize applications and data to deliver an improved experience to users. It has decided to use Azure as a cloud service provider and build cloud-native applications. The core platform features of the online store will be built as microservices that will be published as APIs for consumption. A fully-managed relational database will be used by the APIs to store the state. Azure storage accounts will be used for the product assets, along with other documents that will be generated by invoice and billing APIs for purchases.

> Resiliency Requirements

- The APIs should run in an active-active environment from multiple Azure regions.
- The compute infrastructure hosting the APIs must be spread across the availability zone.
- User requests to the APIs should automatically failover in cases of outages.
- The database must have geo-replication enabled.

> Business Requirements

- Administrative effort must be minimized to maintain the solution.
- Cost should be minimized.
- Developers should be able to easily add or remove new microservices to the solution with least effort. 

> Network Requirements

- The API endpoints should be available from a single domain using path-based routing.
- Requests to the APIs should be sent to the region with the lowest latency.
- The database must be available privately for the APIs.

> Security Requirements

- The APIs must be available only on the HTTPS protocol.
- Access to Azure services from APIs must be provided securely.

---

## Q021:

The invoice and billing API is required to send information to the document generator API whenever a transaction is made.
You need to identify which communication type and communications service are required to establish a connection between these APIs to send the transaction details.

Which communication type and communications service should you identify? To answer, select the appropriate options in the answer area.

Communication type: Asynchronous
Communications service: Service Bus queues

---

### Answer:

You should identify the asynchronous communication type. Generally, microservices APIs operate independently and should not be coupled with any other service. To establish communication between the services, asynchronous messaging is the best solution. There is a publisher who sends the details to a messaging solution and a subscriber can see those messages and act on them. The billing and invoice API is a publisher that sends a message, while the document generator API is the subscriber that reads the messages and creates the document. This allows decoupling of the services and results in a pure microservice architecture.
You should not identify the synchronous communication type. This is the direct approach where the API communicates by calling the other API endpoint. This solution looks the easiest but can cause too many problems. For example, if the endpoint is unreachable, the documents will not be generated. The number of requests sent directly to the endpoint can affect the performance of the application. The API cannot be scaled horizontally based on metrics. You would need to implement a retry feature in other APIs in the event of failures. This is also an example of tight coupling between services. Ideally, you should decouple the communication aspect of microservices so that they can work individually.
You should identify Service Bus queues to send the transaction details. Azure Service Bus queues allow you to decouple your microservices API for asynchronous messaging solutions. The invoice and billing API should send a message with all the transaction details to generate the documents for the purchase. Service Bus queues provide functionalities like duplicate message-detection, they support At-Most-Once and At- Least-Once delivery guarantees, they provide atomic operation support, (meaning that all the messages in a transaction are considered as a single unit), as well as dead-letter queues for failed message-delivery. With messaging architecture, you can also scale your APIs properly. Depending on the number of messages in queues, you can scale your APIs horizontally in a predictable manner.
You should not identify Azure queue storage to send the transaction details. This is a very simple messaging service, but it does not allow atomicity, cannot detect duplicate messages, and does not include the concept of dead-letter queues. Service Bus queues are a better solution when working with microservices.
You should not identify Service Bus topics to send the transaction details. Service Bus topics are used for a pub/sub messaging model where you need to publish a message to the topic which is read by multiple subscribers. Since you only have a single microservice that will be receiving the messages, you should use Service Bus queues.
You should not identify REST API calls to send the transaction details. APIs can communicate directly by calling each other by sending a request to the endpoint of the API. However, this will have bottlenecks when the number of requests increases, and scaling the application would not be optimal. This is a type of synchronous request and brings a lot more challenges. The APIs are tightly coupled, so if you make any changes to the API, the other API will need to be modified as well. It is best to use a messaging solution to decouple the communication. You can then modify the API and send messages to the queue, without worrying about any implementation changes.

---

### References

[Storage queues and Service Bus queues - compared and contrasted](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted)  

[Service Bus queues, topics, and subscriptions](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions)  

[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  

[Communication in a microservice architecture](https://learn.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/communication-in-microservice-architecture)  

---

## Q020:

The total size of the product assets files (containing images and videos) in the Network Attached Storage (NAS) file share is 35G. You need to migrate the assets over a content delivery network (CDN) to improve the performance of the retail store.
Which solution should you recommend to migrate the files to a storage account?

- Azure File Sync
- Azure Storage Explorer
- Azure Data Box
- AzCopy

---

### Answer:
- AzCopy

You should recommend AzCopy to transfer the files from the Network Attached Storage (NAS) file share to blob storage. Since the files will be served using Azure Content Delivery Network (CDN), they need to be stored in blob storage. CDN only supports serving static files from blob storage. Azcopy is an open-source CLI tool, which can be used to transfer files over the internet. Since the total size of the files is relatively small, transferring them over the internet should be fairly easy. When migrating data with AzCopy, a job is created that keeps track of the files transferred. If, for any reason, the job fails, you can resume the job to reinitiate the transfer.
You should not recommend Azure File Sync. File Sync is a good option to migrate data from NAS file shares to Azure file shares. You can enable sync and all the files will be stored in the Azure file share. However, since the files need to be served from a CDN, which supports blob storage, this solution cannot be used.
You should not recommend Azure Data Box. Data Box is an offline service that can be used to migrate large amounts of data, up to 100TB. It uses standard NAS protocols to transfer the data from on-premises file shares and the physical box can then be shipped to an Azure region where your infrastructure is deployed. Since the asset data is only in GB, using Azure Data Box is not required.
You should not recommend Azure Storage Explorer. This is a graphical tool to work with an Azure storage account. You can upload your files using the options available in the tool. However, you cannot automate the data transfer. Depending on how the file share is structured and how the data will be stored in the blob storage, you would need to do a lot of manual work to upload all the files.

---

### References

[Upload files to Azure Blob storage by using AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-upload)  
[Planning for an Azure File Sync deployment](https://learn.microsoft.com/en-us/azure/storage/file-sync/file-sync-planning)  
[What is Azure Data Box?](https://learn.microsoft.com/en-us/azure/databox/data-box-overview)  
[Choose an Azure solution for data transfer](https://learn.microsoft.com/en-us/azure/storage/common/storage-choose-data-transfer-solution)  

---

## Q019:

Which two networking solutions should you recommend for traffic routing? Each correct answer presents part of the solution.

- Azure Load Balancer
- Azure Traffic Manager
- Azure Front Door
- Azure Application Gateway

---

### Answer:
- Azure Front Door
- Azure Application Gateway

You should recommend Azure Application Gateway. To use Application Gateway to expose the APIs, you need to use Application Gateway Ingress Controller (AGIC), which is a Kubernetes application. AGIC monitors the cluster to update Application Gateway whenever a new service is selected to be exposed to the outside world. Application Gateway allows you to perform path-based routing and SSL offloading, and you can also enable web application firewall (WAF) features to protect against malicious attacks.
You should also recommend Azure Front Door. Since the clusters are deployed in two regions, you need to use another global load-balancing solution. For this, you can use Azure Front Door to route traffic based on the latency routing policy. This will send the traffic requests to the nearest region. The backend pool for Azure Front Door will be the two application gateways deployed in each region. You can perform SSL offloading and path-based routing in Front Door, as well. Another benefit of Azure Front Door that is useful in this scenario is traffic acceleration, which allows the user to establish a connection to an edge location. From the edge location, the connection to the backend pools is established.
You should not recommend Azure Traffic Manager. Even though Azure Front Door internally uses Traffic Manager, it provides additional features like SSL offloading, which makes the connection route through the HTTPS protocol. This also satisfies the security requirements, where the APIs are only made available over the HTTPS protocol. Traffic Manager is useful for non-HTTPS traffic. Traffic Manager is a DNS-based traffic load balancer, which also makes it less efficient than Azure Front Door whenever failover needs to happen due to DNS caching and DNS TTLS.
You should not recommend Azure Load Balancer. Load Balancer is useful when creating a Load Balancer- type service in Kubernetes. When you create a Load Balancer service in Kubernetes, the cloud controller creates a new Azure Load balancer to provide access to the service outside the cluster. Creating a load balancer for the entire microservice will be a costly solution. Instead, an ingress controller like AGIC should be used to reduce the cost as well as configuration.

--- 

### References

[Use Application Gateway Ingress Controller (AGIC) with a multi-tenant Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/aks-agic/aks-agic)  
[Traffic acceleration](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-traffic-acceleration?pivots=front-door-classic)  
[Load-balancing options](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/load-balancing-overview)  
[Traffic routing methods to origin](https://learn.microsoft.com/en-us/azure/frontdoor/routing-methods)  
[What is Application Gateway Ingress Controller?](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview)   
[Use a public standard load balancer in Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/load-balancer-standard)  

---

## Q018:

You need to recommend a solution for the microservices to connect to the Azure SQL database. The traffic should always remain private and must be secure.

What should you do?

- Use service endpoints.
- Use Azure Private Link.
- Define outbound firewall rules.
- Define network access controls.

---

### Answer:
- Use Azure Private Link.

You should use Azure Private Link to connect to the Azure SQL database. With Azure Private Link, you can create a private endpoint for the Azure SQL database. This endpoint is private to the virtual network (VNet). This means that traffic never leaves the VNet boundary. When you create a private link, the resources get a private endpoint in the VNet, which is accessible like any other resource deployed inside a VNet and so you can reach the database using the private IP address. Azure Private Link is designed to provide access to Azure Platform-as-a-Service (PaaS) services inside a VNet.
You should not use service endpoints. With service endpoints, the traffic is routed through the Microsoft backbone network. This allows you to access the resources inside your VNet. However, Azure Private Link is a more secure solution compared to service endpoints, since service endpoints are publicly routable addresses.
You should not define network access controls. This is used when you want to allow access from other Azure resources or allow connections from a specific IP address. The connection is not private, and so the traffic traverses the internet to establish the connection.
You should not define outbound firewall rules. This is required for the outbound connection to storage accounts and other Azure SQL logical servers. This is useful when enabling features like auditing, vulnerability scanning, etc.

---

### References

[Azure Private Link for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/private-endpoint-overview?view=azuresql) 
[Use virtual network service endpoints and rules for servers in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/vnet-service-endpoint-rule-overview?view=azuresql)  
[What is Azure Private Link?](https://learn.microsoft.com/en-us/azure/private-link/private-link-overview)  
[Outbound firewall rules for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/outbound-firewall-rule-overview?view=azuresql)  
[Azure SQL Database and Azure Synapse Analytics network access controls](https://learn.microsoft.com/en-us/azure/azure-sql/database/network-access-controls-overview?view=azuresql)  


---

## Q017:

Which compute solution should you recommend for the APIs? The solution must meet the resiliency requirements.

- Azure Kubernetes Service (AKS)
- Azure Container Instance (ACI)
- Azure Functions
- Azure App Service

---

### Answer:
- Azure Kubernetes Service (AKS)

You should recommend Azure Kubernetes Service (AKS) for the APIs. The scenario suggests that the core platform features of the online store will be built as microservices. Generally, microservices communicate with each other and need some form of orchestration. Kubernetes can handle all the complexity of orchestrating the microservices and therefore reduce the operational overhead. With AKS, the developers do not have to worry about networking and service discovery when adding or removing microservices. Also, the requirements suggest that the compute infrastructure should be spread across availability zones for high availability and fault tolerance. Also, AKS is a managed service that is free; customers only pay for the node pools attached to the cluster. Using AKS, you can also perform active-active deployment in multiple regions.
You should not recommend Azure App Service. Azure App Service is a good solution for REST APIs and web apps. However, as the number of APIs increases, the effort to maintain them starts to increase as well. Each App Service would have its own ARM templates for deployment and application settings. Also, it does not include a feature for service discovery of new APIs.
You should not recommend Azure Functions. Azure Functions is a good solution for event-driven serverless APIs. However, it is not an efficient solution for the requirements given in the scenario. It cannot auto- discover other APIs and it is not easy to orchestrate when the number of services increases. You would need other solutions for the inter-communication between these APIs, like a publish-subscribe model using Azure Service Bus or Azure Event Grid.
You should not recommend Azure Container Instance (ACI). ACI is used for small-scale applications and task automation. Hosting microservices is not feasible using ACI.

---

### References
[Azure Kubernetes Service (AKS) architecture design](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-start-here)  
[Advanced Azure Kubernetes Service (AKS) microservices architecture](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices-advanced)  
[AKS baseline for multiregion clusters](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-multi-region/aks-multi-cluster)  
[Tutorial: Host a RESTful API with CORS in Azure App Service](https://learn.microsoft.com/en-in/azure/app-service/app-service-web-tutorial-rest-api)  
[Building serverless microservices in Azure sample architecture](https://azure.microsoft.com/en-us/blog/building-serverless-microservices-in-azure-sample-architecture/)  
[What is Azure Container Instances?](https://learn.microsoft.com/en-in/azure/container-instances/container-instances-overview)  

---

## Q014-016:

CompanyA is a large online clothing retail store that operates from various datacenters in multiple cities in the US. The IT operations team faces many challenges to meet the high demand whenever a sale is announced for the products. You need to recommend a highly available and fault-tolerant application design that meets the following requirements:

- The application must scale based on traffic and demand.
- The application must be able to self-diagnose and self-heal in the case of a failure.
- The database must be available in case of Azure outages.
- The application must be optimized for static content like photos and videos.
- SSL offloading and certificate management must minimize complexity.

---

### References

[Traffic Manager routing methods](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods)  
[Overview of TLS termination and end to end TLS with Application Gateway](https://learn.microsoft.com/en-us/azure/application-gateway/ssl-overview)  
[Active geo-replication](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql)  
[Availability sets overview](https://learn.microsoft.com/en-us/azure/virtual-machines/availability-set-overview)  

[What is a content delivery network on Azure?](https://learn.microsoft.com/en-in/azure/cdn/cdn-overview)    
[Automatic instance repairs for Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-automatic-instance-repairs?tabs=portal-1%2Cportal-2%2Ccli-3%2Crest-api-4%2Crest-api-5)   
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)   

[Caching with Azure Front Door](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-caching?pivots=front-door-standard-premium)   
[What is Azure Front Door?](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-overview)    

---

## Q016:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. The VM scale sets have an autoscaling rule based on CPU utilization. Use Azure SQL Database with Geo-replication enabled. Use Azure Front Door to perform the SSL offloading and route traffic using the priority routing method. Use the caching feature of Azure Front Door to optimize the application's performance for static content.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale sets deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in case of failures. The autoscaling rule to increase and decrease the number of VMs can manage the changes in demand and traffic. In this way, the IT operations team does not have any overhead to perform manual intervention whenever the traffic requirements change. The autoscaling policy can take care of this requirement very efficiently. The instance repair feature of VM scale sets can monitor the health of the instances and replace any instance that fails the health checks. Thus, this ensures that the system self-heals and recovers automatically from a failure.
The Azure SQL Database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Azure Front Door has many features which can be used to fulfill a number of requirements. Its SSL- offloading capabilities also reduce the overhead from the backend pool VMs. Azure Front Door also has caching capabilities that can serve content from point of presence (PoP) locations to improve the application's performance when using static content.

---

## Q015:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. Enable scheduled VMSS autoscaling for the holiday sale dates throughout the year. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager using priority routing between the two regions. Use Azure Content Delivery Network (CDN) to cache the static content and optimize the application's performance.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale set deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in the event of failures. High availability is achieved using the scheduled autoscaling. For the holiday sale, the autoscaling policy can increase the number of VMs based on the predicted demand. With multi- availability zone deployment, the application will still be available when any availability zone goes down and the VM scale set maintains the number of desired VMs across the zones. Also, with the instance repair feature of VM scale sets, you can monitor the status of the instances and, if any instance fails, the VM scale set replaces that instance with a new one.
Additionally, the Azure SQL database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down. Furthermore, Azure Application Gateway can minimize complexity and the overhead from the VMs via SSL offloading and distributing the traffic to the backend pool of the VMs where the application is deployed.
Azure Traffic Manager with priority routing always sends the request to the active site. The request will automatically failover if the active site becomes unavailable. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes. This fulfills the requirement of self-healing infrastructure, as well. The operation team does not have to take any manual action if the active site goes down. The traffic gets routed to the passive site automatically.
CDN can optimize the application's performance by a lot. The photos and videos can get served from the point of presence (PoP) location nearest to the user. The user will experience low latency when the content is served from the PoP locations.

---

## Q014:

Solution: Deploy the application on Azure VMs with an availability set within two regions in active/passive mode. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager with performance routing between the two regions.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. The VMs would not scale based on demand, and you would need manual intervention to scale the number of VMs based on the traffic. Furthermore, this solution also does not address the requirement of optimizing performance for static content and cannot self-heal in case of failures. The performance aspect can be improved by using some caching mechanism, like an Azure Content Delivery Network (CDN), which can cache the photos and videos to the points of presence (POP) locations around the world for faster access. For self-healing, you would need a mechanism that can check the status of the VM and replace it if the health check fails. This can be done using a VM scale set instance repair feature, but it is not available for VMs.
The VMs deployed in the availability set would ensure high availability in the event of Azure outages or during maintenance, as they would be spread across multiple upgrade domains and fault domains, which maintain availability during a maintenance activity and failures respectively. Azure SQL database with geo- replication enabled would also ensure high availability in the event of an Azure outage. With geo- replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Also, Azure AppGateway could reduce the overhead from the VMs to perform the SSL offloading and distribute the traffic to the backend pool of the VMs where the application is deployed. As well as this, Azure Traffic Manager with performance routing would route traffic to the location's closest region where the network latency is lowest. Since the application is deployed in active/passive mode, the traffic should always route to the active site. It should only send traffic to the passive site when the active one is no longer available. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes.

---

## Q011-013:

Your company has an on-premises SQL Server 2019 instance hosted on a dedicated server running Windows Server 2019 Datacenter edition. The database supports a high-volume transaction processing application with hundreds of users connecting to the database at any time. User connections and transaction volume are expected to grow rapidly as your company expands. Rather than investing in new on-premises resources, the company decides to move the database to the cloud.
The database currently contains 3 TB of data and is expected to grow to no more than 4 TB. The solution should support at least one read-only replica in addition to the primary read-write database.

You need to deploy the database to Azure and meet the requirements.

---

### References

[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql&tabs=azure-portal)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  
[Resource limits for single databases using the DTU purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-single-databases?view=azuresql)  

---

## Q013:


Solution: Deploy an Azure SQL Managed instance under the vCore-based purchasing model and choose the Business Critical service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Managed Instance under the vCore-based purchasing model and choose the Business Critical service tier. The Business Critical service tier supports up to 4 TB storage and multiple read-only replicas. In case of failure of the primary database, one of the read-only replicas is promoted to be the new primary database with read-write support.

---

## Q012:


Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Hyperscale service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Database under the vCore-based purchasing model and choose the Hyperscale service tier. The Hyperscale service tier supports up to 100 TB of storage. It supports up to four read-only replicas in addition to the primary read-write database. This is the most expensive option, but it meets all of the requirements.

---

## Q011:

Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier. This deployment option does not meet the storage requirements for the scenario. Premium service P6 is limited to no more than 500 GB. You would need to select a Premium P11 or Premium P15 deployment to meet the storage requirements. This solution also does not meet the read replica requirements.

---

## Q008-010:

A manufacturing company is looking to improve its manufacturing processes. lot sensors throughout the production line collect real-time data. The company wants to perform real-time data collection and analysis from these devices. The solution must support bi-directional communication to send commands back to the lot sensors.

You need to develop a data flow process to meet this requirement.

---

### References

[IoT Concepts and Azure IoT Hub](https://learn.microsoft.com/en-us/azure/iot-hub/iot-concepts-and-iot-hub)  
[Welcome to Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)  
[Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/compare-messaging-services)  
[Connecting IoT Devices to Azure: IoT Hub and Event Hubs](https://learn.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs)  
[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)
[What is dedicated SQL pool (formerly SQL DW) in Azure Synapse Analytics?](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is)  

---

## Q010:

Solution: You configure an lot hub to receive the data and send the data to Azure Stream Analytics.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal and the scenario requirements. The lot hub provides communication between the lot devices and Azure Stream Analytics. Azure Stream Analytics is a real-time event processing engine that can process a high volume of fast streaming data from a variety of sources. This allows for real-time analysis of telemetry streams for lot devices.

---

## Q009:

Solution: You configure an lot hub to receive the data and send the data to Azure Data Factory.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. An lot hub can be used to receive the data and stream it to a destination, but Azure Data Factory is not used for real-time data analysis. Azure Data Factory is a cloud- based extract-transform-load (ETL) and data integration service designed for data movement and large-scale data transformations.

---

## Q008:

Solution: You configure the lot sensors with an event hub and send the data to Azure Stream Analytics for analysis.

Does this solution meet the goal?

---

### Answer: 
No

This solution does not meet the goal. You can use an event hub for streaming data as an input to Azure Stream Analytics. However, an event hub does not provide bi-directional communication with lot devices.

---

## Q004-Q007:

> Overview

CompanyA sells building materials online and in retail outlets around the world. CompanyA's IT infrastructure is hosted in on-premises data centers in multiple locations. The CEO and CTO realize the potential of Microsoft Azure Cloud to increase the company's competitive advantage and eliminate security concerns.

> Existing Environment

- CompanyA operates public websites serving its online shop.
- An online shop web application is connected to an SQL database server in the backend.

- The company's Microsoft SQL database server holds a 100GB product catalog and online order data. Rapid growth of the database size is not planned.

- The Active Directory (AD) authentication provider is in place. The AD domain name is companya.com.
- Marketing documents, media files, and product manuals are stored on file share servers on-premises at the head office's location.

- Marketing documents are accessed via mapped drives on Windows 10 clients.

> Problem Statement

- The SQL database server is not highly available. Every outage means a drop in sales and loss in customer confidence.

- The online shop website's performance on Black Friday is slow. The Microsoft SQL database has been identified as the bottleneck.

- Searching the site when viewing many items on the page is slow.

- There are security concerns regarding the possible loss of financial and personal data due to the potential unauthorized access by support personnel or administrators of the SQL database content.

- Content on file share servers includes not only current files, but also historical data. This historical data is rarely accessed and is needed only in the case of requests relating to legal and compliance matters. Due to legal requirements, the historical data will have to be kept for seven years. This data occupies additional storage and is costly to manage.

- Some remote locations have a slow internet connection and, therefore, access to marketing documents from remote locations is very slow.
- Privileges and rights granted to users and identities are not supervised.

> Planned Changes

- The online shop web servers will be migrated into Azure Cloud.

- The SQL database servers will be migrated into Azure Cloud with the least amount of effort.

- Data security practices will undergo modernization according to industry standard best practices.

- Marketing documents will be migrated from file share servers onto the cloud.

- A solution to reduce costs for historical data on file shares is to be implemented.

> Technical & Business Requirements

- The SQL database must be highly available. Latency must be reduced. 

- CompanyA requires a solution to obtain real-time business insights about customers' purchasing behavior by analyzing data collected from its own online shop's website, social media channels, and partner websites.

> Security & Policy Requirements

- Legal regulations require customer data stored in Microsoft SQL Database to reside in the corresponding region of customer residence.
- All the data has to be secured at rest, in transit and in use.
- Compliance policy requires the retention of online order data for a minimum of seven years.
- Privileges and rights be granted to users and identities must be supervised.

---

## Q007:

In your solution design you need to recommend a solution to meet the technical requirements for Azure SQL Database.
Which solution for Azure database service tier should you recommend?

- Azure SQL Hyperscale
- Azure SQL Business Critical
- An SQL Server on an Azure Virtual Machine
- Azure SQL General Purpose
- Security and policy requirements

---

### Answer:
- Azure SQL Business Critical

You should recommend Azure SQL Business Critical tier. This Azure database service tier is designed to serve for mission-critical applications that require low latency and minimal downtime. This Azure database service tier utilizes Always On availability groups and high performance direct attached SSD storage. This is the best solution to meet the requirement in this scenario.
You should not recommend the Azure SQL General Purpose. This Azure database service tier provides budget-oriented balance between compute and storage. It utilizes multiple failover nodes with spare capacity. In the case of an outage, spare nodes can create a new SQL Server instance and the workload can be failed over. In addition to a higher latency compared to business critical architecture, this service tier introduces additional downtime to create a new SQL Server instance and failover workloads. In this scenario, especially for Black Friday, it is not acceptable.
You should not recommend the Azure SQL Database Hyperscale solution. This Azure database service tier is most suitable for rapid-changing storage needs, which can be rapidly scaled out up to 100TB. As the SQL database in this scenario is only 100GB and rapid growth is not expected, this service tier is not the solution you should recommend.
You should not recommend SQL Server on Azure Virtual Machine. This type of deployment is similar to what you have on premises, except that SQL server is running on an Azure virtual machine. This deployment is a one-server deployment with no guarantee of the required availability.

---

### References

[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  
[Design for Azure SQL Database](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/2-design-for-azure-sql-database)  
[What is an Always On availability group?](https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  

---

## Q006:

You need to recommend a solution to include in your design to meet the security requirement for Azure SQL Database.

What should you recommend?

- SQL Elastic pools
- Azure Cosmos DB with multi-region writes
- Horizontal scaling by Sharding
- Elastic database tools
- Security and policy requirements

---

### Answer:
- Horizontal scaling by Sharding

You should recommend to include Horizontal scaling by Sharding in your SQL database design. Horizontal scaling by Sharding provides a solution to partition (sharding) a database into multiple databases, which can be scaled independently. The Sharding solution utilizes a special database named shard map manager, which maintains global mapping information about all shards (databases) in a shard set. The application uses this shard map to save data into the proper shard. In this scenario, with Sharding, you meet the legal regulations to maintain customer data in the region of respective customer residence by placing each shard with data in the region of customer residence.
You should not recommend SQL Elastic pools. SQL Elastic pools provide a mechanism to scale computer power up or down for SQL Database as needed. In this scenario, SQL Elastic pools cannot meet the requirement for customer data residence as providing more or fewer compute resources does not change the residence of the data. For this specific requirement, you have to implement Horizontal scaling by Sharding in your SQL database design.
You should not recommend Elastic database tools. Elastic database tools provide the solution to query data across multiple databases and provide output to Microsoft Excel or third-party tools for visualization. Elastic database tools have no influence on the data residency, and as such it cannot meet this requirement.
You should not recommend Azure Cosmos DB with multi-region writes. Azure Cosmos DB is a NoSQL database, which provides real-time access, multi-region writes and data distribution to any Azure region, and it can scale storage and throughput across any Azure region elastically and independently. Azure Cosmos DB can provide the best solution for mobile, gaming and lot applications. Although there are possibilities to migrate data from SQL Server to Azure Cosmos DB and implement data residency restrictions, it would be very expensive in terms of efforts and costs. In the given scenario, it is required for the data to reside in the region of corresponding customer residence and the SQL database migration needs to be executed with the least amount of effort.

---

### References

[Recommend a solution for database scalability](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/5-recommend-database-scalability)  
[Welcome to Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)  
[Understanding distributed NoSQL databases](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql)  

---

## Q005:

You are tasked with remediating the security concerns and implement data security requirements.

Which three encryption solutions should you include in your design? Each correct answer presents part of the solution.

- Always Encrypted
- Server-level firewall
- Network security groups (NSGs)
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security


---

### Answer:

- Always Encrypted
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security

To remediate the security concerns for data-at-rest you should recommend Transparent Data Encryption (TDE). To ensure data privacy, data sovereignty, and data compliance it is indispensable and mandatory to encrypt data at rest. TDE provides such a data encryption capability for data at rest and, as such, the data is protected from the unauthorized access of raw offline database files on a hard disk or a backup copy. The encryption is performed by TDE on a page level (logical partitioning of the data within a data file (.mdf or .ndf) in a database, numbered contiguously from 0 to n), so that data is encrypted as it is written from memory to the disk and decrypted as it is read from the disk to memory.
To remediate the security concerns for data-in-use you should also recommend Always Encrypted. Always Encrypted is a security technique to hide sensitive data stored in specific database columns. The data can be only decrypted while it is loaded into memory of a client computer and processed by a client application. This technique protects data also from administrators or other support personnel that are authorized to access databases for maintenance.
To remediate the security concerns for data-in-transit you should, additionally, recommend SSL/TLS. This encryption provides protection of data from man-in-the-middle (MITM) or side channel attacks during data transportation from the backend SQL database server to the front end application server.
You should not recommend a server-level firewall. Although server-level firewalls protect from unauthorized access by end-users, it does not protect from unauthorized access by support personnel or administrators. Server-level firewalls work on the network layer, which is layer 3 of the International Standardization Organization (OSI) reference model. As an administrator, you must have access to the server in order to be able to pass by firewall for SQL server maintenance.
You should not recommend Network security groups (NSGs). Using an NSG in Azure you can filter network traffic between Azure resources, such as virtual machines within a virtual network (VLAN). The functionality of an NSG is similar to the functionality of a firewall. Similar to firewalls, NSGs can prohibit access by any unauthorized user or resource, but it cannot prohibit access by support personnel or administrators.
References
Design security for data at rest, data in motion, and data in use
Protect data in-transit and at rest
Windows Network Architecture and the OSI Model

---

### References

[Network security groups](https://learn.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview)   

---

## Q004:

You need to recommend a storage design solution on the Azure Cloud platform for the marketing documents. Administrative effort and cost must be minimized.

What should you recommend?

- Azure Files
- Azure Queue Storage
- Azure managed disks
- Azure Blob Storage

---

### Answer:
- Azure Files

You should recommend Azure Files as a storage design solution for the marketing documents. Azure Files is a file share hosted on the Azure Cloud platform. Files hosted in Azure Files can be accessed via an industry- standard Server Message Block (SMB) protocol for file sharing. SMB allows applications to read and write to files in a computer network. Azure Files can be mapped as a shared drive on all main operating systems and thus can replace the company's on-premises file shares. In this scenario, marketing documents are accessed via mapped drives on Windows 10 clients, as such Azure Files is the best replacement solution.
You should not recommend Azure Blob Storage. Azure Blob Storage is an object-oriented storage solution.
It is optimized to store unstructured data, such as large binary files, media files, log files, and backup files. Unlike Azure Files, Azure Blob Storage cannot be mapped as a shared drive, which is the main reason why, in this scenario, it is not the ideal storage solution for the marketing documents.
You should not recommend Azure managed disks. Azure managed disks are block-level storage volumes used by virtual machines to store data. Usually, you create Azure managed disks at the same time as a virtual machine.
You should not recommend Azure Queue Storage. Azure Queue Storage is a service to provide an interface for service-to-service communication. Azure Queue Storage can store a large amount of messages and can be accessed from anywhere using the HTTP or HTTPS protocols.

---

### References

[Design for data storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/2-design-for-data-storage)  
[Design for Azure Files](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files)  
[Design for Azure Blob Storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage)  
[Design for Azure managed disks](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/7-design-for-azure-disk-solutions)  
[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[Overview of file sharing using the SMB 3 protocol in Windows Server](https://learn.microsoft.com/en-us/windows-server/storage/file-server/file-server-smb-overview)  

---

## Q001-Q003:

Your organization is moving its business operations to Azure. Your company is organized into three departments that will deploy Azure app services and databases. A fourth department is responsible for internal operations and will deploy resources as necessary to support those activities.
Your company wants to be cost-conscious in its move to the cloud, and exercise cost and budget controls at the department level. The company plans to use the Azure Resource Usage and Rate Card APIs to colle billing data for analysis.

Initially, you set up your company's Azure with one subscription.

You need to design a solution for cost reporting by department. 
The solution should follow best practices for organizing resources and should minimize the effort required to maintain the solution.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Track costs across business units, environments, or projects](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/track-costs)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  

---

## Q003:

Solution: You create a separate resource group for each type of resource, and tag the resources with department and billing code.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You can create a separate resource group for each type of resource, and tag the resources with department and billing code. Microsoft's best practices recommend various options for organizing resources into resource groups, such as by resource type, by project, by lifecycle, and so forth. The use of tags lets you organize the data, and retrieve data by department for cost accounting and budgeting purposes.

---

## Q002:

Solution: You create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure role-based access control (Azure RBAC).

Does this solution meet the goal?

---

### Answer:
No

The solution does not meet the goal. You should not create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure RBAC. Organizing by department is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information. Azure RBAC is used to control and manage access to resources, but it does not retrieve the information you need.

---

## Q001:

Solution: 
You create a separate subscription for each department, and use resource groups to organize resources by project.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not create a separate subscription for each department, and use resource groups to organize resources by project. Creating separate subscriptions for each department adds administrative overhead. Organizing by project is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information.

---
