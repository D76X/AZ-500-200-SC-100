# AZ-305 Practice Test 169 Questions

---

## Q00X:

---

### Answer:

---

### References

---

## Q017-021:

> Overview

You work for CompanyA, a large-scale online retail store company which sells electronic products. 
The company has operations in the North America and Europe regions.

> on-premise Environment

The company hosts the application from two data centers located in New York and London. The data centers are connected using high-speed VPN connections. The application is a monolith application deployed on multiple virtual machines hosted on Hyper-V. SQL Database is used to store the application's data. The product asset files, like images and videos, are stored in Network Attached Storage (NAS) file shares.

> Planned Changes

CompanyA wants to modernize applications and data to deliver an improved experience to users. It has decided to use Azure as a cloud service provider and build cloud-native applications. The core platform features of the online store will be built as microservices that will be published as APIs for consumption. A fully-managed relational database will be used by the APIs to store the state. Azure storage accounts will be used for the product assets, along with other documents that will be generated by invoice and billing APIs for purchases.

> Resiliency Requirements

- The APIs should run in an active-active environment from multiple Azure regions.
- The compute infrastructure hosting the APIs must be spread across the availability zone.
- User requests to the APIs should automatically failover in cases of outages.
- The database must have geo-replication enabled.

> Business Requirements

- Administrative effort must be minimized to maintain the solution.
- Cost should be minimized.
- Developers should be able to easily add or remove new microservices to the solution with least effort. 

> Network Requirements

- The API endpoints should be available from a single domain using path-based routing.
- Requests to the APIs should be sent to the region with the lowest latency.
- The database must be available privately for the APIs.

> Security Requirements

- The APIs must be available only on the HTTPS protocol.
- Access to Azure services from APIs must be provided securely.

---

### References

---

## Q021:

---

### Answer:


---

## Q020:

---

### Answer:


---

## Q019:

Which two networking solutions should you recommend for traffic routing? Each correct answer presents part of the solution.

- Azure Load Balancer
- Azure Traffic Manager
- Azure Front Door
- Azure Application Gateway

---

### Answer:
- Azure Front Door
- Azure Application Gateway

You should recommend Azure Application Gateway. To use Application Gateway to expose the APIs, you need to use Application Gateway Ingress Controller (AGIC), which is a Kubernetes application. AGIC monitors the cluster to update Application Gateway whenever a new service is selected to be exposed to the outside world. Application Gateway allows you to perform path-based routing and SSL offloading, and you can also enable web application firewall (WAF) features to protect against malicious attacks.
You should also recommend Azure Front Door. Since the clusters are deployed in two regions, you need to use another global load-balancing solution. For this, you can use Azure Front Door to route traffic based on the latency routing policy. This will send the traffic requests to the nearest region. The backend pool for Azure Front Door will be the two application gateways deployed in each region. You can perform SSL offloading and path-based routing in Front Door, as well. Another benefit of Azure Front Door that is useful in this scenario is traffic acceleration, which allows the user to establish a connection to an edge location. From the edge location, the connection to the backend pools is established.
You should not recommend Azure Traffic Manager. Even though Azure Front Door internally uses Traffic Manager, it provides additional features like SSL offloading, which makes the connection route through the HTTPS protocol. This also satisfies the security requirements, where the APIs are only made available over the HTTPS protocol. Traffic Manager is useful for non-HTTPS traffic. Traffic Manager is a DNS-based traffic load balancer, which also makes it less efficient than Azure Front Door whenever failover needs to happen due to DNS caching and DNS TTLS.
You should not recommend Azure Load Balancer. Load Balancer is useful when creating a Load Balancer- type service in Kubernetes. When you create a Load Balancer service in Kubernetes, the cloud controller creates a new Azure Load balancer to provide access to the service outside the cluster. Creating a load balancer for the entire microservice will be a costly solution. Instead, an ingress controller like AGIC should be used to reduce the cost as well as configuration.

--- 

### References

[Use Application Gateway Ingress Controller (AGIC) with a multi-tenant Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/aks-agic/aks-agic)  
[Traffic acceleration](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-traffic-acceleration?pivots=front-door-classic)  
[Load-balancing options](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/load-balancing-overview)  
[Traffic routing methods to origin](https://learn.microsoft.com/en-us/azure/frontdoor/routing-methods)  
[What is Application Gateway Ingress Controller?](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview)   
[Use a public standard load balancer in Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/load-balancer-standard)  

---

## Q018:

You need to recommend a solution for the microservices to connect to the Azure SQL database. The traffic should always remain private and must be secure.

What should you do?

- Use service endpoints.
- Use Azure Private Link.
- Define outbound firewall rules.
- Define network access controls.

---

### Answer:
- Use Azure Private Link.

You should use Azure Private Link to connect to the Azure SQL database. With Azure Private Link, you can create a private endpoint for the Azure SQL database. This endpoint is private to the virtual network (VNet). This means that traffic never leaves the VNet boundary. When you create a private link, the resources get a private endpoint in the VNet, which is accessible like any other resource deployed inside a VNet and so you can reach the database using the private IP address. Azure Private Link is designed to provide access to Azure Platform-as-a-Service (PaaS) services inside a VNet.
You should not use service endpoints. With service endpoints, the traffic is routed through the Microsoft backbone network. This allows you to access the resources inside your VNet. However, Azure Private Link is a more secure solution compared to service endpoints, since service endpoints are publicly routable addresses.
You should not define network access controls. This is used when you want to allow access from other Azure resources or allow connections from a specific IP address. The connection is not private, and so the traffic traverses the internet to establish the connection.
You should not define outbound firewall rules. This is required for the outbound connection to storage accounts and other Azure SQL logical servers. This is useful when enabling features like auditing, vulnerability scanning, etc.

---

### References

[Azure Private Link for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/private-endpoint-overview?view=azuresql) 
[Use virtual network service endpoints and rules for servers in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/vnet-service-endpoint-rule-overview?view=azuresql)  
[What is Azure Private Link?](https://learn.microsoft.com/en-us/azure/private-link/private-link-overview)  
[Outbound firewall rules for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/outbound-firewall-rule-overview?view=azuresql)  
[Azure SQL Database and Azure Synapse Analytics network access controls](https://learn.microsoft.com/en-us/azure/azure-sql/database/network-access-controls-overview?view=azuresql)  


---

## Q017:

Which compute solution should you recommend for the APIs? The solution must meet the resiliency requirements.

- Azure Kubernetes Service (AKS)
- Azure Container Instance (ACI)
- Azure Functions
- Azure App Service

---

### Answer:
- Azure Kubernetes Service (AKS)

You should recommend Azure Kubernetes Service (AKS) for the APIs. The scenario suggests that the core platform features of the online store will be built as microservices. Generally, microservices communicate with each other and need some form of orchestration. Kubernetes can handle all the complexity of orchestrating the microservices and therefore reduce the operational overhead. With AKS, the developers do not have to worry about networking and service discovery when adding or removing microservices. Also, the requirements suggest that the compute infrastructure should be spread across availability zones for high availability and fault tolerance. Also, AKS is a managed service that is free; customers only pay for the node pools attached to the cluster. Using AKS, you can also perform active-active deployment in multiple regions.
You should not recommend Azure App Service. Azure App Service is a good solution for REST APIs and web apps. However, as the number of APIs increases, the effort to maintain them starts to increase as well. Each App Service would have its own ARM templates for deployment and application settings. Also, it does not include a feature for service discovery of new APIs.
You should not recommend Azure Functions. Azure Functions is a good solution for event-driven serverless APIs. However, it is not an efficient solution for the requirements given in the scenario. It cannot auto- discover other APIs and it is not easy to orchestrate when the number of services increases. You would need other solutions for the inter-communication between these APIs, like a publish-subscribe model using Azure Service Bus or Azure Event Grid.
You should not recommend Azure Container Instance (ACI). ACI is used for small-scale applications and task automation. Hosting microservices is not feasible using ACI.

---

### References
[Azure Kubernetes Service (AKS) architecture design](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-start-here)  
[Advanced Azure Kubernetes Service (AKS) microservices architecture](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices-advanced)  
[AKS baseline for multiregion clusters](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-multi-region/aks-multi-cluster)  
[Tutorial: Host a RESTful API with CORS in Azure App Service](https://learn.microsoft.com/en-in/azure/app-service/app-service-web-tutorial-rest-api)  
[Building serverless microservices in Azure sample architecture](https://azure.microsoft.com/en-us/blog/building-serverless-microservices-in-azure-sample-architecture/)  
[What is Azure Container Instances?](https://learn.microsoft.com/en-in/azure/container-instances/container-instances-overview)  

---

## Q014-016:

CompanyA is a large online clothing retail store that operates from various datacenters in multiple cities in the US. The IT operations team faces many challenges to meet the high demand whenever a sale is announced for the products. You need to recommend a highly available and fault-tolerant application design that meets the following requirements:

- The application must scale based on traffic and demand.
- The application must be able to self-diagnose and self-heal in the case of a failure.
- The database must be available in case of Azure outages.
- The application must be optimized for static content like photos and videos.
- SSL offloading and certificate management must minimize complexity.

---

### References

[Traffic Manager routing methods](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods)  
[Overview of TLS termination and end to end TLS with Application Gateway](https://learn.microsoft.com/en-us/azure/application-gateway/ssl-overview)  
[Active geo-replication](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql)  
[Availability sets overview](https://learn.microsoft.com/en-us/azure/virtual-machines/availability-set-overview)  

[What is a content delivery network on Azure?](https://learn.microsoft.com/en-in/azure/cdn/cdn-overview)    
[Automatic instance repairs for Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-automatic-instance-repairs?tabs=portal-1%2Cportal-2%2Ccli-3%2Crest-api-4%2Crest-api-5)   
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)   

[Caching with Azure Front Door](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-caching?pivots=front-door-standard-premium)   
[What is Azure Front Door?](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-overview)    

---

## Q016:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. The VM scale sets have an autoscaling rule based on CPU utilization. Use Azure SQL Database with Geo-replication enabled. Use Azure Front Door to perform the SSL offloading and route traffic using the priority routing method. Use the caching feature of Azure Front Door to optimize the application's performance for static content.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale sets deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in case of failures. The autoscaling rule to increase and decrease the number of VMs can manage the changes in demand and traffic. In this way, the IT operations team does not have any overhead to perform manual intervention whenever the traffic requirements change. The autoscaling policy can take care of this requirement very efficiently. The instance repair feature of VM scale sets can monitor the health of the instances and replace any instance that fails the health checks. Thus, this ensures that the system self-heals and recovers automatically from a failure.
The Azure SQL Database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Azure Front Door has many features which can be used to fulfill a number of requirements. Its SSL- offloading capabilities also reduce the overhead from the backend pool VMs. Azure Front Door also has caching capabilities that can serve content from point of presence (PoP) locations to improve the application's performance when using static content.

---

## Q015:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. Enable scheduled VMSS autoscaling for the holiday sale dates throughout the year. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager using priority routing between the two regions. Use Azure Content Delivery Network (CDN) to cache the static content and optimize the application's performance.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale set deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in the event of failures. High availability is achieved using the scheduled autoscaling. For the holiday sale, the autoscaling policy can increase the number of VMs based on the predicted demand. With multi- availability zone deployment, the application will still be available when any availability zone goes down and the VM scale set maintains the number of desired VMs across the zones. Also, with the instance repair feature of VM scale sets, you can monitor the status of the instances and, if any instance fails, the VM scale set replaces that instance with a new one.
Additionally, the Azure SQL database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down. Furthermore, Azure Application Gateway can minimize complexity and the overhead from the VMs via SSL offloading and distributing the traffic to the backend pool of the VMs where the application is deployed.
Azure Traffic Manager with priority routing always sends the request to the active site. The request will automatically failover if the active site becomes unavailable. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes. This fulfills the requirement of self-healing infrastructure, as well. The operation team does not have to take any manual action if the active site goes down. The traffic gets routed to the passive site automatically.
CDN can optimize the application's performance by a lot. The photos and videos can get served from the point of presence (PoP) location nearest to the user. The user will experience low latency when the content is served from the PoP locations.

---

## Q014:

Solution: Deploy the application on Azure VMs with an availability set within two regions in active/passive mode. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager with performance routing between the two regions.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. The VMs would not scale based on demand, and you would need manual intervention to scale the number of VMs based on the traffic. Furthermore, this solution also does not address the requirement of optimizing performance for static content and cannot self-heal in case of failures. The performance aspect can be improved by using some caching mechanism, like an Azure Content Delivery Network (CDN), which can cache the photos and videos to the points of presence (POP) locations around the world for faster access. For self-healing, you would need a mechanism that can check the status of the VM and replace it if the health check fails. This can be done using a VM scale set instance repair feature, but it is not available for VMs.
The VMs deployed in the availability set would ensure high availability in the event of Azure outages or during maintenance, as they would be spread across multiple upgrade domains and fault domains, which maintain availability during a maintenance activity and failures respectively. Azure SQL database with geo- replication enabled would also ensure high availability in the event of an Azure outage. With geo- replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Also, Azure AppGateway could reduce the overhead from the VMs to perform the SSL offloading and distribute the traffic to the backend pool of the VMs where the application is deployed. As well as this, Azure Traffic Manager with performance routing would route traffic to the location's closest region where the network latency is lowest. Since the application is deployed in active/passive mode, the traffic should always route to the active site. It should only send traffic to the passive site when the active one is no longer available. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes.

---

## Q011-013:

Your company has an on-premises SQL Server 2019 instance hosted on a dedicated server running Windows Server 2019 Datacenter edition. The database supports a high-volume transaction processing application with hundreds of users connecting to the database at any time. User connections and transaction volume are expected to grow rapidly as your company expands. Rather than investing in new on-premises resources, the company decides to move the database to the cloud.
The database currently contains 3 TB of data and is expected to grow to no more than 4 TB. The solution should support at least one read-only replica in addition to the primary read-write database.

You need to deploy the database to Azure and meet the requirements.

---

### References

[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql&tabs=azure-portal)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  
[Resource limits for single databases using the DTU purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-single-databases?view=azuresql)  

---

## Q013:


Solution: Deploy an Azure SQL Managed instance under the vCore-based purchasing model and choose the Business Critical service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Managed Instance under the vCore-based purchasing model and choose the Business Critical service tier. The Business Critical service tier supports up to 4 TB storage and multiple read-only replicas. In case of failure of the primary database, one of the read-only replicas is promoted to be the new primary database with read-write support.

---

## Q012:


Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Hyperscale service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Database under the vCore-based purchasing model and choose the Hyperscale service tier. The Hyperscale service tier supports up to 100 TB of storage. It supports up to four read-only replicas in addition to the primary read-write database. This is the most expensive option, but it meets all of the requirements.

---

## Q011:

Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier. This deployment option does not meet the storage requirements for the scenario. Premium service P6 is limited to no more than 500 GB. You would need to select a Premium P11 or Premium P15 deployment to meet the storage requirements. This solution also does not meet the read replica requirements.

---

## Q008-010:

A manufacturing company is looking to improve its manufacturing processes. lot sensors throughout the production line collect real-time data. The company wants to perform real-time data collection and analysis from these devices. The solution must support bi-directional communication to send commands back to the lot sensors.

You need to develop a data flow process to meet this requirement.

---

### References

[IoT Concepts and Azure IoT Hub](https://learn.microsoft.com/en-us/azure/iot-hub/iot-concepts-and-iot-hub)  
[Welcome to Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)  
[Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/compare-messaging-services)  
[Connecting IoT Devices to Azure: IoT Hub and Event Hubs](https://learn.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs)  
[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)
[What is dedicated SQL pool (formerly SQL DW) in Azure Synapse Analytics?](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is)  

---

## Q010:

Solution: You configure an lot hub to receive the data and send the data to Azure Stream Analytics.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal and the scenario requirements. The lot hub provides communication between the lot devices and Azure Stream Analytics. Azure Stream Analytics is a real-time event processing engine that can process a high volume of fast streaming data from a variety of sources. This allows for real-time analysis of telemetry streams for lot devices.

---

## Q009:

Solution: You configure an lot hub to receive the data and send the data to Azure Data Factory.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. An lot hub can be used to receive the data and stream it to a destination, but Azure Data Factory is not used for real-time data analysis. Azure Data Factory is a cloud- based extract-transform-load (ETL) and data integration service designed for data movement and large-scale data transformations.

---

## Q008:

Solution: You configure the lot sensors with an event hub and send the data to Azure Stream Analytics for analysis.

Does this solution meet the goal?

---

### Answer: 
No

This solution does not meet the goal. You can use an event hub for streaming data as an input to Azure Stream Analytics. However, an event hub does not provide bi-directional communication with lot devices.

---

## Q004-Q007:

> Overview

CompanyA sells building materials online and in retail outlets around the world. CompanyA's IT infrastructure is hosted in on-premises data centers in multiple locations. The CEO and CTO realize the potential of Microsoft Azure Cloud to increase the company's competitive advantage and eliminate security concerns.

> Existing Environment

- CompanyA operates public websites serving its online shop.
- An online shop web application is connected to an SQL database server in the backend.

- The company's Microsoft SQL database server holds a 100GB product catalog and online order data. Rapid growth of the database size is not planned.

- The Active Directory (AD) authentication provider is in place. The AD domain name is companya.com.
- Marketing documents, media files, and product manuals are stored on file share servers on-premises at the head office's location.

- Marketing documents are accessed via mapped drives on Windows 10 clients.

> Problem Statement

- The SQL database server is not highly available. Every outage means a drop in sales and loss in customer confidence.

- The online shop website's performance on Black Friday is slow. The Microsoft SQL database has been identified as the bottleneck.

- Searching the site when viewing many items on the page is slow.

- There are security concerns regarding the possible loss of financial and personal data due to the potential unauthorized access by support personnel or administrators of the SQL database content.

- Content on file share servers includes not only current files, but also historical data. This historical data is rarely accessed and is needed only in the case of requests relating to legal and compliance matters. Due to legal requirements, the historical data will have to be kept for seven years. This data occupies additional storage and is costly to manage.

- Some remote locations have a slow internet connection and, therefore, access to marketing documents from remote locations is very slow.
- Privileges and rights granted to users and identities are not supervised.

> Planned Changes

- The online shop web servers will be migrated into Azure Cloud.

- The SQL database servers will be migrated into Azure Cloud with the least amount of effort.

- Data security practices will undergo modernization according to industry standard best practices.

- Marketing documents will be migrated from file share servers onto the cloud.

- A solution to reduce costs for historical data on file shares is to be implemented.

> Technical & Business Requirements

- The SQL database must be highly available. Latency must be reduced. 

- CompanyA requires a solution to obtain real-time business insights about customers' purchasing behavior by analyzing data collected from its own online shop's website, social media channels, and partner websites.

> Security & Policy Requirements

- Legal regulations require customer data stored in Microsoft SQL Database to reside in the corresponding region of customer residence.
- All the data has to be secured at rest, in transit and in use.
- Compliance policy requires the retention of online order data for a minimum of seven years.
- Privileges and rights be granted to users and identities must be supervised.

---

## Q007:

In your solution design you need to recommend a solution to meet the technical requirements for Azure SQL Database.
Which solution for Azure database service tier should you recommend?

- Azure SQL Hyperscale
- Azure SQL Business Critical
- An SQL Server on an Azure Virtual Machine
- Azure SQL General Purpose
- Security and policy requirements

---

### Answer:
- Azure SQL Business Critical

You should recommend Azure SQL Business Critical tier. This Azure database service tier is designed to serve for mission-critical applications that require low latency and minimal downtime. This Azure database service tier utilizes Always On availability groups and high performance direct attached SSD storage. This is the best solution to meet the requirement in this scenario.
You should not recommend the Azure SQL General Purpose. This Azure database service tier provides budget-oriented balance between compute and storage. It utilizes multiple failover nodes with spare capacity. In the case of an outage, spare nodes can create a new SQL Server instance and the workload can be failed over. In addition to a higher latency compared to business critical architecture, this service tier introduces additional downtime to create a new SQL Server instance and failover workloads. In this scenario, especially for Black Friday, it is not acceptable.
You should not recommend the Azure SQL Database Hyperscale solution. This Azure database service tier is most suitable for rapid-changing storage needs, which can be rapidly scaled out up to 100TB. As the SQL database in this scenario is only 100GB and rapid growth is not expected, this service tier is not the solution you should recommend.
You should not recommend SQL Server on Azure Virtual Machine. This type of deployment is similar to what you have on premises, except that SQL server is running on an Azure virtual machine. This deployment is a one-server deployment with no guarantee of the required availability.

---

### References

[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  
[Design for Azure SQL Database](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/2-design-for-azure-sql-database)  
[What is an Always On availability group?](https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  

---

## Q006:

You need to recommend a solution to include in your design to meet the security requirement for Azure SQL Database.

What should you recommend?

- SQL Elastic pools
- Azure Cosmos DB with multi-region writes
- Horizontal scaling by Sharding
- Elastic database tools
- Security and policy requirements

---

### Answer:
- Horizontal scaling by Sharding

You should recommend to include Horizontal scaling by Sharding in your SQL database design. Horizontal scaling by Sharding provides a solution to partition (sharding) a database into multiple databases, which can be scaled independently. The Sharding solution utilizes a special database named shard map manager, which maintains global mapping information about all shards (databases) in a shard set. The application uses this shard map to save data into the proper shard. In this scenario, with Sharding, you meet the legal regulations to maintain customer data in the region of respective customer residence by placing each shard with data in the region of customer residence.
You should not recommend SQL Elastic pools. SQL Elastic pools provide a mechanism to scale computer power up or down for SQL Database as needed. In this scenario, SQL Elastic pools cannot meet the requirement for customer data residence as providing more or fewer compute resources does not change the residence of the data. For this specific requirement, you have to implement Horizontal scaling by Sharding in your SQL database design.
You should not recommend Elastic database tools. Elastic database tools provide the solution to query data across multiple databases and provide output to Microsoft Excel or third-party tools for visualization. Elastic database tools have no influence on the data residency, and as such it cannot meet this requirement.
You should not recommend Azure Cosmos DB with multi-region writes. Azure Cosmos DB is a NoSQL database, which provides real-time access, multi-region writes and data distribution to any Azure region, and it can scale storage and throughput across any Azure region elastically and independently. Azure Cosmos DB can provide the best solution for mobile, gaming and lot applications. Although there are possibilities to migrate data from SQL Server to Azure Cosmos DB and implement data residency restrictions, it would be very expensive in terms of efforts and costs. In the given scenario, it is required for the data to reside in the region of corresponding customer residence and the SQL database migration needs to be executed with the least amount of effort.

---

### References

[Recommend a solution for database scalability](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/5-recommend-database-scalability)  
[Welcome to Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)  
[Understanding distributed NoSQL databases](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql)  

---

## Q005:

You are tasked with remediating the security concerns and implement data security requirements.

Which three encryption solutions should you include in your design? Each correct answer presents part of the solution.

- Always Encrypted
- Server-level firewall
- Network security groups (NSGs)
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security


---

### Answer:

- Always Encrypted
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security

To remediate the security concerns for data-at-rest you should recommend Transparent Data Encryption (TDE). To ensure data privacy, data sovereignty, and data compliance it is indispensable and mandatory to encrypt data at rest. TDE provides such a data encryption capability for data at rest and, as such, the data is protected from the unauthorized access of raw offline database files on a hard disk or a backup copy. The encryption is performed by TDE on a page level (logical partitioning of the data within a data file (.mdf or .ndf) in a database, numbered contiguously from 0 to n), so that data is encrypted as it is written from memory to the disk and decrypted as it is read from the disk to memory.
To remediate the security concerns for data-in-use you should also recommend Always Encrypted. Always Encrypted is a security technique to hide sensitive data stored in specific database columns. The data can be only decrypted while it is loaded into memory of a client computer and processed by a client application. This technique protects data also from administrators or other support personnel that are authorized to access databases for maintenance.
To remediate the security concerns for data-in-transit you should, additionally, recommend SSL/TLS. This encryption provides protection of data from man-in-the-middle (MITM) or side channel attacks during data transportation from the backend SQL database server to the front end application server.
You should not recommend a server-level firewall. Although server-level firewalls protect from unauthorized access by end-users, it does not protect from unauthorized access by support personnel or administrators. Server-level firewalls work on the network layer, which is layer 3 of the International Standardization Organization (OSI) reference model. As an administrator, you must have access to the server in order to be able to pass by firewall for SQL server maintenance.
You should not recommend Network security groups (NSGs). Using an NSG in Azure you can filter network traffic between Azure resources, such as virtual machines within a virtual network (VLAN). The functionality of an NSG is similar to the functionality of a firewall. Similar to firewalls, NSGs can prohibit access by any unauthorized user or resource, but it cannot prohibit access by support personnel or administrators.
References
Design security for data at rest, data in motion, and data in use
Protect data in-transit and at rest
Windows Network Architecture and the OSI Model

---

### References

[Network security groups](https://learn.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview)   

---

## Q004:

You need to recommend a storage design solution on the Azure Cloud platform for the marketing documents. Administrative effort and cost must be minimized.

What should you recommend?

- Azure Files
- Azure Queue Storage
- Azure managed disks
- Azure Blob Storage

---

### Answer:
- Azure Files

You should recommend Azure Files as a storage design solution for the marketing documents. Azure Files is a file share hosted on the Azure Cloud platform. Files hosted in Azure Files can be accessed via an industry- standard Server Message Block (SMB) protocol for file sharing. SMB allows applications to read and write to files in a computer network. Azure Files can be mapped as a shared drive on all main operating systems and thus can replace the company's on-premises file shares. In this scenario, marketing documents are accessed via mapped drives on Windows 10 clients, as such Azure Files is the best replacement solution.
You should not recommend Azure Blob Storage. Azure Blob Storage is an object-oriented storage solution.
It is optimized to store unstructured data, such as large binary files, media files, log files, and backup files. Unlike Azure Files, Azure Blob Storage cannot be mapped as a shared drive, which is the main reason why, in this scenario, it is not the ideal storage solution for the marketing documents.
You should not recommend Azure managed disks. Azure managed disks are block-level storage volumes used by virtual machines to store data. Usually, you create Azure managed disks at the same time as a virtual machine.
You should not recommend Azure Queue Storage. Azure Queue Storage is a service to provide an interface for service-to-service communication. Azure Queue Storage can store a large amount of messages and can be accessed from anywhere using the HTTP or HTTPS protocols.

---

### References

[Design for data storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/2-design-for-data-storage)  
[Design for Azure Files](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files)  
[Design for Azure Blob Storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage)  
[Design for Azure managed disks](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/7-design-for-azure-disk-solutions)  
[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[Overview of file sharing using the SMB 3 protocol in Windows Server](https://learn.microsoft.com/en-us/windows-server/storage/file-server/file-server-smb-overview)  

---

## Q001-Q003:

Your organization is moving its business operations to Azure. Your company is organized into three departments that will deploy Azure app services and databases. A fourth department is responsible for internal operations and will deploy resources as necessary to support those activities.
Your company wants to be cost-conscious in its move to the cloud, and exercise cost and budget controls at the department level. The company plans to use the Azure Resource Usage and Rate Card APIs to colle billing data for analysis.

Initially, you set up your company's Azure with one subscription.

You need to design a solution for cost reporting by department. 
The solution should follow best practices for organizing resources and should minimize the effort required to maintain the solution.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Track costs across business units, environments, or projects](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/track-costs)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  

---

## Q003:

Solution: You create a separate resource group for each type of resource, and tag the resources with department and billing code.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You can create a separate resource group for each type of resource, and tag the resources with department and billing code. Microsoft's best practices recommend various options for organizing resources into resource groups, such as by resource type, by project, by lifecycle, and so forth. The use of tags lets you organize the data, and retrieve data by department for cost accounting and budgeting purposes.

---

## Q002:

Solution: You create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure role-based access control (Azure RBAC).

Does this solution meet the goal?

---

### Answer:
No

The solution does not meet the goal. You should not create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure RBAC. Organizing by department is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information. Azure RBAC is used to control and manage access to resources, but it does not retrieve the information you need.

---

## Q001:

Solution: 
You create a separate subscription for each department, and use resource groups to organize resources by project.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not create a separate subscription for each department, and use resource groups to organize resources by project. Creating separate subscriptions for each department adds administrative overhead. Organizing by project is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information.

---
