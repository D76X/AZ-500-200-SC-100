# AZ-305 Practice Test 169 Questions

---

## Q04X:

---

### Answer:

---

### References

---

## Q043:

A medium-sized company is using Microsoft Entra ID to control access to their applications and services that are deployed in Azure. A recent security audit shows that the Global Administrator group is populated with people who do not need such broad access to the resources.
You need to restrict the resources appropriately. In addition, you want to be able to grant elevated access only for specific periods of time, such as for only an hour or a day or for situations when a person needs temporary access for a specific task.
What two actions should you perform? Each correct answer presents part of the solution.

Choose the correct answers

- Assign more granular roles to the administrators according to their functions.
- Use managed identities to further restrict access to the resources.
- Use Privileged Identity Management (PIM) to create additional rules for access.
- Add conditional access policies to your current access restrictions.

---

### Answer:
- Assign more granular roles to the administrators according to their functions.
- Use Privileged Identity Management (PIM) to create additional rules for access.

You should assign more granular roles to the administrators according to their functions. Unless your organization is very small, the administrators are responsible for specific aspects of a system or only for certain resources. The tasks that are performed by particular administrators should map directly to their roles. You should always assign the least possible privilege that allows users to perform their assigned tasks.
You should also use Privileged Identity Management (PIM) to create additional rules for access, including just-in-time (JIT) privileged access to Azure resources. This includes activating access for only a specific length of time, such as one hour or one day.
You should not use managed identities to further restrict access to the resources. Managed identities provide access for resources, not for users. For example, if an application needs to retrieve secrets from a key vault, it can use its managed identity to retrieve a token.
You should not add conditional access policies to your current access restrictions. Conditional access policies are intended to address issues that are related to remote access and bring your own device (BYOD) workplaces. You can, for example, allow or block access for certain locations or device types.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[Securing privileged access for hybrid and cloud deployments in Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/identity/role-based-access-control/security-planning)  
[What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)  
[What is Conditional Access?](https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview)  

---

## Q042:

Your organization has 10 Azure subscriptions, one for each branch office. Five offices are located in the eastern region, and the other five are located in the western region. Each region is grouped into a management group. Each subscription has multiple resource groups, and each resource group has multiple resources. You want to ensure that only the IT personnel in each region have permission to create or modify the Azure resources for that region.
You need to assign role-based access control (RBAC) permissions to the IT personnel. Your solution must require minimal maintenance.
Which scope should you apply the permissions to?

Choose the correct answer

- Management group
- Resource group
- Subscription
- Resource

---

### Answer:
- Management group

You should apply role-based access control (RBAC) permissions to the management group scope. A management group allows you to group multiple subscriptions for RBAC assignment. In this scenario, each region has multiple subscriptions, and only the IT personnel in a region should be able to create and modify the Azure resources for that region. By creating two management groups, one for each region, you can apply RBAC permissions to a management group and have them affect all the resources in all subscriptions for that management group.
You should not apply RBAC permissions to the subscription scope. This would require you to manage RBAC permissions for 10 subscriptions instead of two management groups.
You should not apply RBAC permissions to the resource group scope. This would require you to manage multiple RBAC permission assignments for multiple resource groups across 10 subscriptions.
You should not apply RBAC permissions to the resource scope. This would require you to manage multiple RBAC permission assignments for every resource group across 10 subscriptions.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q041:

Your organization has four Azure subscriptions, one for each branch office. Each subscription has multiple resource groups, and each resource group has multiple resources. You want to ensure that only the IT personnel in each branch office have permission to create or modify the Azure resources for that office.
You need to assign role-based access control (RBAC) permissions to the IT personnel. Your solution must require minimal maintenance.
Which scope should you apply the permissions to?

Choose the correct answer

- Management group
- Resource group
- Resource
- Subscription

---

### Answer:
- Subscription

You should apply RBAC permissions to the subscription scope. Each office has its own subscription, and only the IT personnel in a branch office should be able to create and modify the Azure resources for that office. By applying the permissions to a subscription, you allow those permissions to affect only that subscription and the resources in that subscription.
You should not apply RBAC permissions to the management group scope. A management group allows you
to group multiple subscriptions for RBAC assignment. In this scenario, RBAC permissions should be assigned to each subscription, not across multiple subscriptions.
You should not apply RBAC permissions to the resource group scope. This would require you to manage multiple RBAC permission assignments for multiple resource groups in a single subscription.
You should not apply RBAC permissions to the resource scope. This would require you to manage multiple RBAC permission assignments for every resource group in a subscription.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q040:

Your organization has one Azure subscription. You plan to deploy four SQL Server virtual machines (VMs) to that subscription. Developers and IT personnel must be able to connect to these VMs. However, only the IT personnel should have permission to modify or delete the SQL Server VMs.
You need to design an authorization strategy. Your solution should follow the principle of least privilege.

Which strategy should you use?

Choose the correct answer

- Deploy the SQL Server VMs in a separate resource group. Add the developers to a Developers group and the IT personnel to an IT group. Assign the Reader role to the Developers group and the Owner role to the IT group at the resource group level.

- Deploy the SQL Server VMs in a separate resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the resource group level.

- Deploy the SQL Server VMs in an existing resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the subscription level.

- Deploy the SQL Server VMs in an existing resource group. Add the developers to a Developers group and the IT personnel to an IT group. Assign the Reader role to the Developers group and the Owner role to the IT group at the resource group level.

---

### Answer:
- Deploy the SQL Server VMs in a separate resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the resource group level.

You should deploy the SQL Server VMs in a separate resource group and delegate permissions at the resource group level. Permissions will be inherited by virtual machines in the resource group. The Virtual Machine Contributor role enables you to manage virtual machines. If you delegate this role at the resource group level, you will be able to manage virtual machines in the resource group. Users do not need any additional permissions to be able to connect to existing virtual machines, so you should not assign Developers to any role.
You should not assign the Developers group to the Reader role. This action would assign permissions to the developers that are not required to perform their task of connecting to the virtual machine, and would not conform to the principle of least privilege.
You should not deploy SQL Server VMs to the existing resource group and delegate permissions at the
resource group level. If you take this action, permissions would be inherited by all objects in the resource group, not only by SQL Server VMs. This action would not conform to the principle of least privilege.
You should not assign the Virtual Machine Contributor role to the IT group at the subscription level. If you take this action, permissions would be inherited by all resource groups in the subscription and by all VMs. This action would assign the IT group permissions that are not required and would not conform to the principle of least privilege.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q039:

Your company has a Microsoft Entra infrastructure. Your company wants to securely share applications and services with guest users from another organization. The guest organization has an on-premises Active Directory (AD) domain but it does not support Microsoft Entra ID.
You need to implement a solution that gives guest users access to select resources while minimizing any additional user account management overhead. You want to minimize introducing any additional security risks.
What should you do?

Choose the correct answer

- Require the guest organization to implement Microsoft Entra ID.
- Create a user account in your Microsoft Entra for each guest user.
- Implement business-to-business (B2B) collaboration.
- Configure synchronization between the guest organization's on-premises AD and your Microsoft Entra ID.

---

### Answer:
- Implement business-to-business (B2B) collaboration.

You should implement business-to-business (B2B) collaboration. With B2B collaboration the guest users must have valid email addresses but an IT infrastructure for user management, such as Microsoft Entra, is not required. The guest organization has full responsibility for managing the external guest users. Your company will invite guest users through a simple invitation and redemption process. You configure conditional access policies to control access to your company content..
You should not create a user account in your Microsoft Entra for each guest user. This is not necessary and would require additional management overhead for your organization.
You should not configure synchronization between the guest organization's on-premises AD and your Microsoft Entra ID. There is no need for any type of synchronization between the organizations.
You should not require the guest organization to implement Microsoft Entra ID. This is not a requirement for the guest organization.

---

### References

[B2B collaboration overview](https://learn.microsoft.com/en-us/entra/external-id/what-is-b2b)  
[Add Microsoft Entra B2B collaboration users in the Microsoft Entra admin center](https://learn.microsoft.com/en-us/entra/external-id/add-users-administrator)  
[Microsoft Entra B2B collaboration FAQS](https://learn.microsoft.com/en-us/entra/external-id/faq)  

---

## Q038:

Your company has an on-premises Active Directory (AD) forest and a Microsoft Entra ID P1 tenant. All Microsoft Entra users are assigned a P1 license. You plan to have users use the same usernames and passwords for on-premises and Microsoft Entra authentication. Password changes are currently managed through the helpdesk to ensure that passwords remain synchronized.
Your company is considering deploying Microsoft Entra Connect on the on-premises network. You need to identify features from this action that will help to reduce the management overhead for your network infrastructure and helpdesk personnel.
Which two features could you use? Each correct answer presents a complete solution.

Choose the correct answers

- Privileged Identity Management (PIM)
- Identity Protection
- Password writeback
- Periodic access review
- Self-service password reset

---

### Answer:
- Password writeback
- Self-service password reset

You should not use periodic access review. This does not require Microsoft Entra Connect and would not reduce the helpdesk workload. Access review is an aid to manage group memberships, access to enterprise applications, and role assignments.
You should not use Privileged Identity Management (PIM). PIM is not a feature specific to a hybrid infrastructure using Microsoft Entra Connect. PIM lets you manage, control, and monitor access to important resources through features that include:
Just-in-time (JIT) privileged access control
Time-bound access to resources with start and end dates
Required approval to activate privileged roles
Notification when privileged roles are activated
Access audit history
You should not use Identity Protection. This is not a feature specific to a hybrid infrastructure using Microsoft Entra Connect. Identity Protection provides a way to detect and remediate identity-based risks. It can generate reports to identify risky users and risky sign-ins. It also supports a risk detection report. Identity Protection requires a Microsoft Entra P2 license.


---

### References

[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  
[How does self-service password reset writeback work in Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/authentication/concept-sspr-writeback)  
[Self-service password reset frequently asked questions](https://learn.microsoft.com/en-us/entra/identity/authentication/passwords-faq) 
[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)  

---

## Q037:

You have a hybrid identity environment that includes an on-premises Active Directory environment and a Microsoft Entra ID.
You need to create an authentication solution that verifies the following:

- That on-premises AD security policies are applied.
- That passwords are validated against your on-premises AD.
- That passwords are not stored in the cloud.

What solution should you use?

Choose the correct answer

- Password hash synchronization
- Pass-through authentication
- Federated authentication
- Single sign-on (SSO)

---

### Answer:
- Pass-through authentication

You should use pass-through authentication. This authentication method does not store passwords in the cloud (Microsoft Entra ID). Instead, passwords are stored only in the on-premises AD. Consequently, when an authentication request is made, only the on-premises AD is used to validate the request. A lightweight agent is used on-premises to communicate with Microsoft Entra ID.
You should not use federated authentication. Federation allows an external, third-party system to authenticate users. This can include biometrics, smart-cards, and so on. Federation does not authenticate against on-premises AD.
You should not use password hash synchronization. This is the most basic and least-effort solution for a hybrid authentication system. Passwords are stored in the cloud (Microsoft Entra ID). This method is best for simple scenarios where users need access only to Microsoft Entra resources, such as Office 365 and SaaS apps.
You should not use single sign-on (SSO). SSO is an umbrella term that means users only have to sign in once, after that they are automatically authenticated to additional resources. You should not use SSO when you have to sync or store passwords.

---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[User sign-in with Microsoft Entra pass-through authentication](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/how-to-connect-pta)  

---

## Q036:

Your company has an on-premises Active Directory (AD) domain that uses AD Connect to access Microsoft Entra ID. Employees use single sign-on (SSO) from corporate devices or their own devices to gain access to resources to do their jobs. IT administrators reported that the Remote Desktop Protocol (RDP) port of the on-premises AD server was open to perform a one-time administrative task.
After reports of identity theft at a partner company, management is particularly concerned about identity security.
You need to strengthen your security policy to mitigate risks associated with identity compromise.
Which two actions should you perform? Each correct answer presents part of the solution.

Choose the correct answers

- Enable Microsoft Entra conditional access.
- Disable password hash synchronization.
- Close Remote Desktop Protocol (RDP) ports on the on-premises AD server.
- Require strong passwords that expire each month.

---

### Answer:
- Enable Microsoft Entra conditional access.
- Close Remote Desktop Protocol (RDP) ports on the on-premises AD server.

You should enable Microsoft Entra conditional access. This ensures that users gain access to corporate resources only from devices that meet security standards.
You should also close Close Remote Desktop Protocol (RDP) ports on the on-premises AD server. This prevents unauthorized remote access to the server. The only ports that should be open are those that are required for the connection between the AD server and AD Connect.
You should not disable password hash synchronization. You should enable this to provide an extra measure of security. AD Connect compares password hashes to known compromised passwords to make sure that a user's password has not been compromised.
You should not require strong passwords with very frequent expiration. Microsoft research has shown that this restrictive policy causes users to choose passwords that are easy to guess.

---

### References

[Five steps to securing your identity infrastructure](https://learn.microsoft.com/en-us/azure/security/fundamentals/steps-secure-identity)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection) 
[What is password hash synchronization with Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-phs)  
[Hybrid Identity Required Ports and Protocols](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/reference-connect-ports)  

---

## Q035:

Your company has a Microsoft Entra tenant named Company.com. The company has a Marketing, Finance and Research department.
You are designing multi-factor authentication (MFA) for Company.com.
You need to ensure that MFA is only implemented for users in the Research department.
Which requirement should you include in the design?

Choose the correct answer

- Create a conditional access policy.
- Configure authentication methods.
- Implement Microsoft Entra ID Protection.
- Implement Microsoft Entra Privileged Identity Management (PIM).

---

### Answer:
- Create a conditional access policy.

You should include a requirement to create a conditional access policy. A conditional access policy is used to grant access to cloud apps. A conditional access policy can be targeted to users and groups. One of the access controls that you can require in a conditional access policy is the use of multi-factor authentication (MFA).
You should not include a requirement to configure authentication methods. In authentication methods, you can configure custom smart lockout, custom banned passwords and password protection for Active Directory Domain Services. You cannot enable or configure MFA in authentication methods.
You should not include a requirement to implement Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to detect and prevent risky sign-ins. One of the Microsoft Entra ID Protection features is the ability to require MFA for sign-in, but only if sign-in is detected as risky.
You should not implement Microsoft Entra Privileged Identity Management (PIM). PIM is used for just-in- time activation of privileged roles. You can use PIM to require MFA if a user wants to activate privileged role membership. PIM cannot be used to require MFA at user sign in.

---

### References

[What authentication and verification methods are available in Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/authentication/concept-authentication-methods)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  


---

## Q034:

You are designing a hybrid identity solution for your organization. It consists of an on-premises Active Directory (AD) domain and a Microsoft Entra tenant. Your solution must meet the following requirements:
Allow a user who logs in to their on-premises account to automatically authenticate in Microsoft Entra to access Azure services.
Minimize administrative effort to deploy and maintain.
You need to set up authentication.

Which mechanism should you use?

Choose the correct answer

- Single sign-on (SSO) with password hash sync
- Federation without password hash sync
- Federation with password hash sync
- Single sign-on (SSO) with pass-through authentication

---

### Answer:
- Single sign-on (SSO) with password hash sync

You should use single sign-on (SSO) with password hash sync. This requires the least amount of administrative effort for deployment and maintenance. The on-premises AD domain services stores the user password as a hash. AD Connect then retrieves the user password hash and hashes that hash. It then sends the second hash to Microsoft Entra ID. When the user logs in to Microsoft Entra ID, Microsoft Entra ID compares the password hashes.
You should not use SSO with pass-through authentication. This requires you to install a pass-through authentication agent on an on-premises server.
You should not use federation with or without password hash sync. With this authentication method,
Microsoft Entra delegates authentication to a separate trusted authentication system, such as AD Federation
Services (FS). This requires you to deploy and maintain additional software.

---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[What is password hash synchronization with Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-phs)  
[Implement password hash synchronization with Microsoft Entra Connect Sync](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/how-to-connect-password-hash-synchronization)  

---

## Q033:

A recent audit of your Microsoft Entra environment has resulted in a requirement to retain your activity logs for 15 years.
You need to meet the requirement, while minimizing complexity and costs.

What should you recommend?

Choose the correct answer

- Azure Service Bus
- Azure Log Analytics workspace
- Azure Event Hubs
- Azure storage account

---

### Answer:
- Azure storage account

You should recommend an Azure storage account. Azure storage accounts provide a cost-effective and relatively simple way to retain logs for a long period while minimizing complexity. You can configure your logs to be stored in an Azure storage account, which offers durable and scalable storage capabilities. Azure Storage provides a straightforward solution for long-term retention without the need for complex configurations or additional services. For the specific scenario of retaining logs for a long duration with minimal complexity and costs, an Azure storage account is the recommended choice.
You should not recommend an Azure Log Analytics workspace. An Azure Log Analytics workspace is a service designed for collecting, analyzing, and visualizing data from your resources. While it provides powerful capabilities for log analysis and monitoring, it is not suited for long-term storage.
You should not recommend Azure Event Hubs. Azure Event Hubs is a scalable event streaming platform that can ingest and process large volumes of data in real-time. While it provides flexibility, it is more geared towards real-time data streaming rather than long-term storage.
You should not recommend Azure Service Bus. Azure Service Bus is a messaging service designed for reliable communication between different applications and services. It provides features like queues and topics for asynchronous messaging patterns. However, for the specific requirement of retaining activity logs for 15 years while minimizing complexity and costs, Azure Service Bus is not the most suitable choice.

---

### References

[How to archive Microsoft Entra activity logs to an Azure storage account](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/howto-archive-logs-to-storage-account)  
[Microsoft Entra data retention](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/reference-reports-data-retention)  
[Azure Event Hubs - A real-time data streaming platform with native Apache Kafka support](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  
[Log Analytics workspace overview](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  


---

## Q032:

You are managing an Azure hybrid environment with resources on premises and in the cloud. You are requested to collect all diagnostic and audit logs centrally in an external third-party security information and event management (SIEM) system.
You need to recommend the most suitable solution to route resource logs to an external third-party SIEM. The solution must provide maximum flexibility and customization in terms of real-time processing and routing logs to different destinations.

What should you recommend?

Choose the correct answer

- Azure Event Hubs
- Azure storage account
- Azure Log Analytics workspace
- Azure Service Bus

---

### Answer:
- Azure Event Hubs

You should recommend Azure Event Hubs. Azure Event Hubs is designed for high-throughput, real-time data streaming and provides the flexibility to process and route logs to various destinations, including an external third-party SIEM system. You can configure your resources to send their diagnostic and audit logs to an Azure Event Hubs and then use custom applications or integrations to transform and forward the logs to your external SIEM. This approach allows you to implement custom processing logic, filtering, and formatting before sending the logs to the SIEM, ensuring that they meet the requirements of the third-party system. Azure Event Hubs' scalability, data distribution capabilities, and real-time processing make it a strong candidate for managing logs in a hybrid environment and integrating them with external SIEM solutions.
While Azure Log Analytics and an Azure storage account have their use cases, neither provides the same level of flexibility and real-time processing capabilities as Azure Event Hubs for sending logs to an external third-party SIEM system, especially when the goal is to process and route logs to different destinations. A partner solution could also be a strong choice if it aligns well with your requirements and provides the necessary customization and integration options.
Azure Log Analytics workspace is a service that helps you to collect, analyze, and visualize data from your resources. It is commonly used for monitoring and managing Azure resources, as well as on-premises resources through agents. You can configure diagnostic and audit logs from your resources to be sent to a Log Analytics workspace. Once collected, you can perform queries, create alerts, and build custom dashboards to monitor and troubleshoot your environment. While Log Analytics is powerful for Azure monitoring, it might not be the best option for sending logs to an external third-party SIEM system.
Azure storage accounts allow you to store various types of data, including logs, in different storage services like Blobs, Tables, Queues, and Files. While you could store your resource logs in an Azure storage account, it is primarily a storage solution. You would need to implement a mechanism or application to regularly fetch logs from the storage and send them to your third-party SIEM system. This approach would require more custom development and might not provide the same level of log processing and real-time integration as Azure Event Hubs.
Azure Service Bus is primarily designed for reliable messaging and communication between applications. While Azure Service Bus is a good choice for facilitating information exchange among various applications and services through messages, it may not be the optimal solution for the task of centralizing logs. In this context, a more fitting solution is Azure Event Hubs. In the broader sense, a message takes on the form of a container adorned with metadata, serving as a vessel for encapsulating data. This data has the capacity to encompass a variety of information types, including structured data encoded in prevalent formats such as JSON, XML, Apache Avro, and Plain Text.

---

### References

[Azure Event Hubs: A real-time data streaming platform with native Apache Kafka support](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)   
[Log Analytics workspace overview](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview)    
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  

---

## Q031:

You are planning the migration of a large on-premises IT system to Azure. You want to ensure that proper alerting and metric-collection systems are configured for all resources once they are deployed and in production.
To ensure that the correct Azure functionality is used within the overall monitoring infrastructure, you need to identify the most appropriate feature or tool for each requirement.
What solution should you use for each scenario? To answer, drag the appropriate tool or feature to each requirement. A solution may be used once, more than once, or not at all.
Drag and drop the answers

Tool or feature:

Activity Logs
Azure Monitor
Log Analytics Workspace
Resource Manager Templates
Azure Dashboards
Smart Groups

Requirement:

You need to create standardized deployments of Azure resources that can ensure that standard alerting rules are put into place:
Resource Manager Templates

You need a service to centralize all metrics from Azure resources and create alerts based on those metrics:
Azure Monitor

You need subscription-level events that occur within Azure and which can be used to identify changes that are made to resources:
Activity Logs

You need to do manual analysis of collected Azure log data using the Kusto query language:
Log Analytics Workspace

---

### Answer:

You should use Azure Resource Manager (ARM) templates to support standardized deployments. ARM templates are scripts that can be authored to deploy nearly any resource type within Azure. Deploying a standardized alert configuration as part of the resource at the time of original deployment is a powerful automation feature of Azure.
You should use Azure Monitor to centralize all metrics from Azure resources and create alerts based on those metrics. Azure Monitor collects all log and metric data that is collected within Azure. This provides you with a centralized location for collecting the data.
You should use Activity logs to obtain subscription-level events that occur within Azure and that can be
used to identify changes that are made to resources. Core Azure alerts are surfaced through Activity Logs.
These include service-level alerts, in addition to logging all activities made on resources themselves, and
support alerts at the subscription scope. You should use Log Analytics workspaces to do manual analysis of collected Azure log data using the Kusto
query language. This is a powerful tool that is useful for performing a detailed analysis of historical metrics
or activities for systems within Azure.

---

### References

[Understand the structure and syntax of ARM templates](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/syntax)  
[Azure Monitor Documentation](https://learn.microsoft.com/en-us/azure/azure-monitor/)  
[Azure Monitor data platform](https://learn.microsoft.com/en-us/azure/azure-monitor//data-platform)  
[Overview of Azure platform logs](https://learn.microsoft.com/en-us/azure/azure-monitor/data-sources)  
[Log queries in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-query-overview)  


---

## Q030:

You are designing a monitoring solution for a microservice solution hosted in an Azure Kubernetes Service (AKS) cluster.
You need to recommend a monitoring solution that meets the following requirements:
Measure the memory consumption of cluster nodes.
Monitor the health of pods and deployments.
Create alerts when persistent volumes are more than 80% full.
Visualize the metrics and dashboards within the Azure Portal.

What should you recommend?

Choose the correct answer

- Grafana
- Container insights
- Application Insights
- VM insights

---

### Answer:
- Container insights

You should recommend Container insights. You can use Container insights to monitor workloads running Kubernetes-based solutions in Azure. You can monitor and create alerts for all components in the cluster, including node memory and processor usage, pod and deployment health, and persistent volume usage. You can access the Container insights dashboards and metrics within the Azure Portal or integrate them with external monitoring tools, such as Prometheus or Grafana.
You should not recommend VM insights. You can use VM insights to monitor the performance and health of virtual machines (VMs). It should be possible to use VM insights to monitor some cluster nodes; however, it is not possible to monitor the internal resources in the cluster, like pods and persistent volumes.
You should not recommend Application Insights. You can use Application Insights to monitor application
performance, error rate caused by exceptions, and how users interact with the application, including most- visited pages and other metrics.
You should not recommend Grafana. You can use Grafana to visualize and create dashboards based on multiple sources of metrics. You can use the Azure Monitor data source plugin to query data from Container insights to create custom dashboards. However, you cannot monitor an AKS cluster only with Grafana.

---

### References

[Azure Monitor overview](https://learn.microsoft.com/en-us/azure/azure-monitor/overview)  
[Container insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-overview)  
[Overview of VM insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-overview)  
[Application Insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)  
[Monitor your Azure services in Grafana](https://learn.microsoft.com/en-us/azure/azure-monitor/visualize/grafana-plugin)  

---

## Q029:

You are migrating your on-premises workloads running on Windows and Linux virtual machines to Azure. During the migration, the applications will run on a hybrid model, with some components running in both Azure and on-premises servers to ensure no downtime for the application during the migration process.
You need to design a monitoring strategy solution so that the logs and metrics are available in a centralized location in Azure. The logs should be stored for at least 18 months and must be easy to view and query.
Which solution should you configure?

Choose the correct answer

- A Storage Account
- Log Analytics
- Event Hubs

---

### Answer:
- Log Analytics

You should configure Log Analytics for this scenario. Log Analytics provides you with capabilities to use the Kusto query language to read the logs and metrics. The logs and metrics can be collected using Log Analytics agents, which can be deployed on all the virtual machines running on-premises or in Azure. The data collected can be retained for a maximum of two years in the workspace. You can create a single workspace for all the logs to centralize the management.
You should not recommend a storage account. This is a cheap solution to store the logs of Azure resources through diagnostic settings. It cannot ship logs from virtual machines directly to the storage account. Even if you can get the logs to the storage account, it is not easy to query them.
You should not configure Event Hubs. Event Hubs is a useful solution when you need to send the logs and metrics to other third-party solutions. Azure diagnostic logs and metrics can be sent to Event Hubs. You can directly send logs to the event hub from the virtual machines using the Linux diagnostic extension 4.0 for Linux virtual machines and Windows Azure Diagnostic extension (WAD) for Windows virtual machines. Despite this, the logs cannot be viewed from the event hub directly and it would need to be integrated with another solution that can ingest the logs and provide querying capabilities. Also, event hubs do not have permanent storage; the retention is short, with seven days maximum for the standard tier.

---

### References

[Azure Monitor Logs cost calculations and options](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/cost-logs)  
[Log Analytics tutorial](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-tutorial)  
[Log Analytics agent overview](https://learn.microsoft.com/en-us/azure/azure-monitor/agents/log-analytics-agent)  
[Use the Linux diagnostic extension 4.0 to monitor metrics and logs](https://learn.microsoft.com/en-us/azure/virtual-machines/extensions/diagnostics-linux?tabs=azcli)  
[Send data from Windows Azure diagnostics extension to Azure Event Hubs](https://learn.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-stream-event-hubs)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?tabs=CMD)  

---

## Q026-028:

Your company has a Line of Business (LOB) application running in the Azure cloud. The application should be integrated with an enterprise resource planning (ERP) system running from your company's on-premises main office. Due to compliance reasons, these applications should not receive network traffic from the public internet. Your company's main office has 500 workstations and 100 employees working from home with company-managed laptops that use the LOB application.

You need to configure a connectivity solution that meets the following requirements:

- Provide all employees working from home with secure connectivity to cloud-based and on-premises services.
- Connect the on-premises main office to the Azure network.

---

### References

[VPN Gateway design](https://learn.microsoft.com/en-us/azure/vpn-gateway/design)  
[What is Azure VPN Gateway?](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways)  
[What is Azure Express Route?](https://learn.microsoft.com/en-us/azure/expressroute/expressroute-introduction)  
[About VPN devices and IPsec/IKE parameters for Site-to-Site VPN Gateway connections](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpn-devices)  

---

## Q028:

Solution: You configure a Point-to-Site (P2S) VPN connection for the remote employees and an Express Route between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should provide a connection to give remote employees access to the LOB application through a Point-to-Site (P2S) VPN tunnel. Additionally, you should use an Express Route connection to connect the on-premises head office to the Azure cloud network.

---

## Q027:

Solution: You configure an Express Route connection for the remote employees and between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
No


---

## Q026:

Solution: You configure a Point-to-Site (P2S) VPN connection for remote employees and a Site-to-Site VPN connection between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should use a P2S VPN tunnel to provide a connection for remote employees to access the LOB application and a Site-to-Site VPN connection between the on-premises main office and the Azure network. Access to the applications would be only available inside the on-premises network.

---

## Q022-025:

You are assisting a startup that is planning to launch an online retail platform that sells a wide range of products. The platform's success depends on providing a seamless shopping experience to customers while efficiently managing unpredictable traffic spikes during promotions and sales events. The platform requires additional operating system configuration and control over the server resources.
You need to recommend a solution to meet requirements for significantly increased traffic during Black Friday promotion sales, so that the system can automatically handle the increased load in the most efficient way.

---

### References

---

## Q025:

Solution: You recommend implementing a VM autoscaling mechanism.

Does this solution meet the goal?


---

### Answer:
Yes

This solution meets the goal. Implementing an autoscaling mechanism enables the gradual reduction of the number of virtual machines (VMs) and containers as traffic increases and then returns to normal levels after the promotion. Autoscaling mechanisms automatically adjust resources based on demand, helping to conserve resources and optimize costs. Azure Virtual Machines autoscaling is a feature that allows you to dynamically adjust the number of VM instances based on demand, ensuring optimal performance and cost efficiency. It automates the process of scaling up or down based on predefined rules and conditions, so you can accommodate varying workloads without manual intervention. This feature is particularly useful when your application experiences fluctuations in traffic or resource utilization.

---

###  References

[Virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/overview)
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)  

---

## Q024:

Solution: You recommend microservice architecture to run each service in its own isolated containers.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The microservice architecture recommended allows different services, such as inventory updates, cart calculations, and payment processing, to run independently in isolated containers. This promotes modularity, scalability, and efficient resource utilization. Microservice architecture is an architectural style used in software development to design and build applications as a collection of small, independent, and loosely coupled services. In contrast to monolithic architectures, where the entire application is developed as a single unit, microservices break down the application into smaller, independently deployable services that communicate with each other through well-defined APIs. Each service in a microservices architecture focuses on a specific business capability and can be developed, deployed, and scaled independently. Microservices, each running in its own isolated container, offering control over server resources. Containers allow for customization of the operating system configuration and resource allocation. You can scale individual microservices independently.

---

###  References

[Microservices architecture design](https://learn.microsoft.com/en-us/azure/architecture/microservices/)  
[Microservices assessment and readiness](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/microservices-assessment)  

---

## Q023:

Solution: You recommend to request your IT department to provision additional virtual machines as needed.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. Although requesting additional virtual machines could help to scale resources, it might not be as automatic and dynamic as needed to handle sudden traffic spikes efficiently. Manually provisioning VMs can be time consuming and may not respond quickly to sudden traffic spikes. 

---

###  References

[Virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/overview)
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)  

---

## Q022:

Solution: You recommend serverless computing
Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. Serverless computing, often referred to as "serverless", is a cloud computing model that allows developers to build and run applications without the need to manage underlying infrastructure or servers. In a serverless architecture, developers focus on writing code to implement specific functions or tasks, and the cloud provider takes care of provisioning, scaling, and managing the necessary compute resources. This abstraction of infrastructure management frees developers from the operational complexities of traditional server management. While serverless platforms can automatically scale resources to handle traffic spikes, they do not provide server control, which might not align with the requirement for additional operating system configuration and control over server resources.

---

###  References

[Serverless Computing](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-serverless-computing)  
[Create serverless applications](https://learn.microsoft.com/en-us/training/paths/create-serverless-applications/)  

---

## Q017-021:

> Overview

You work for CompanyA, a large-scale online retail store company which sells electronic products. 
The company has operations in the North America and Europe regions.

> on-premise Environment

The company hosts the application from two data centers located in New York and London. The data centers are connected using high-speed VPN connections. The application is a monolith application deployed on multiple virtual machines hosted on Hyper-V. SQL Database is used to store the application's data. The product asset files, like images and videos, are stored in Network Attached Storage (NAS) file shares.

> Planned Changes

CompanyA wants to modernize applications and data to deliver an improved experience to users. It has decided to use Azure as a cloud service provider and build cloud-native applications. The core platform features of the online store will be built as microservices that will be published as APIs for consumption. A fully-managed relational database will be used by the APIs to store the state. Azure storage accounts will be used for the product assets, along with other documents that will be generated by invoice and billing APIs for purchases.

> Resiliency Requirements

- The APIs should run in an active-active environment from multiple Azure regions.
- The compute infrastructure hosting the APIs must be spread across the availability zone.
- User requests to the APIs should automatically failover in cases of outages.
- The database must have geo-replication enabled.

> Business Requirements

- Administrative effort must be minimized to maintain the solution.
- Cost should be minimized.
- Developers should be able to easily add or remove new microservices to the solution with least effort. 

> Network Requirements

- The API endpoints should be available from a single domain using path-based routing.
- Requests to the APIs should be sent to the region with the lowest latency.
- The database must be available privately for the APIs.

> Security Requirements

- The APIs must be available only on the HTTPS protocol.
- Access to Azure services from APIs must be provided securely.

---

## Q021:

The invoice and billing API is required to send information to the document generator API whenever a transaction is made.
You need to identify which communication type and communications service are required to establish a connection between these APIs to send the transaction details.

Which communication type and communications service should you identify? To answer, select the appropriate options in the answer area.

Communication type: Asynchronous
Communications service: Service Bus queues

---

### Answer:

You should identify the asynchronous communication type. Generally, microservices APIs operate independently and should not be coupled with any other service. To establish communication between the services, asynchronous messaging is the best solution. There is a publisher who sends the details to a messaging solution and a subscriber can see those messages and act on them. The billing and invoice API is a publisher that sends a message, while the document generator API is the subscriber that reads the messages and creates the document. This allows decoupling of the services and results in a pure microservice architecture.
You should not identify the synchronous communication type. This is the direct approach where the API communicates by calling the other API endpoint. This solution looks the easiest but can cause too many problems. For example, if the endpoint is unreachable, the documents will not be generated. The number of requests sent directly to the endpoint can affect the performance of the application. The API cannot be scaled horizontally based on metrics. You would need to implement a retry feature in other APIs in the event of failures. This is also an example of tight coupling between services. Ideally, you should decouple the communication aspect of microservices so that they can work individually.
You should identify Service Bus queues to send the transaction details. Azure Service Bus queues allow you to decouple your microservices API for asynchronous messaging solutions. The invoice and billing API should send a message with all the transaction details to generate the documents for the purchase. Service Bus queues provide functionalities like duplicate message-detection, they support At-Most-Once and At- Least-Once delivery guarantees, they provide atomic operation support, (meaning that all the messages in a transaction are considered as a single unit), as well as dead-letter queues for failed message-delivery. With messaging architecture, you can also scale your APIs properly. Depending on the number of messages in queues, you can scale your APIs horizontally in a predictable manner.
You should not identify Azure queue storage to send the transaction details. This is a very simple messaging service, but it does not allow atomicity, cannot detect duplicate messages, and does not include the concept of dead-letter queues. Service Bus queues are a better solution when working with microservices.
You should not identify Service Bus topics to send the transaction details. Service Bus topics are used for a pub/sub messaging model where you need to publish a message to the topic which is read by multiple subscribers. Since you only have a single microservice that will be receiving the messages, you should use Service Bus queues.
You should not identify REST API calls to send the transaction details. APIs can communicate directly by calling each other by sending a request to the endpoint of the API. However, this will have bottlenecks when the number of requests increases, and scaling the application would not be optimal. This is a type of synchronous request and brings a lot more challenges. The APIs are tightly coupled, so if you make any changes to the API, the other API will need to be modified as well. It is best to use a messaging solution to decouple the communication. You can then modify the API and send messages to the queue, without worrying about any implementation changes.

---

### References

[Storage queues and Service Bus queues - compared and contrasted](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted)  

[Service Bus queues, topics, and subscriptions](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions)  

[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  

[Communication in a microservice architecture](https://learn.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/communication-in-microservice-architecture)  

---

## Q020:

The total size of the product assets files (containing images and videos) in the Network Attached Storage (NAS) file share is 35G. You need to migrate the assets over a content delivery network (CDN) to improve the performance of the retail store.
Which solution should you recommend to migrate the files to a storage account?

- Azure File Sync
- Azure Storage Explorer
- Azure Data Box
- AzCopy

---

### Answer:
- AzCopy

You should recommend AzCopy to transfer the files from the Network Attached Storage (NAS) file share to blob storage. Since the files will be served using Azure Content Delivery Network (CDN), they need to be stored in blob storage. CDN only supports serving static files from blob storage. Azcopy is an open-source CLI tool, which can be used to transfer files over the internet. Since the total size of the files is relatively small, transferring them over the internet should be fairly easy. When migrating data with AzCopy, a job is created that keeps track of the files transferred. If, for any reason, the job fails, you can resume the job to reinitiate the transfer.
You should not recommend Azure File Sync. File Sync is a good option to migrate data from NAS file shares to Azure file shares. You can enable sync and all the files will be stored in the Azure file share. However, since the files need to be served from a CDN, which supports blob storage, this solution cannot be used.
You should not recommend Azure Data Box. Data Box is an offline service that can be used to migrate large amounts of data, up to 100TB. It uses standard NAS protocols to transfer the data from on-premises file shares and the physical box can then be shipped to an Azure region where your infrastructure is deployed. Since the asset data is only in GB, using Azure Data Box is not required.
You should not recommend Azure Storage Explorer. This is a graphical tool to work with an Azure storage account. You can upload your files using the options available in the tool. However, you cannot automate the data transfer. Depending on how the file share is structured and how the data will be stored in the blob storage, you would need to do a lot of manual work to upload all the files.

---

### References

[Upload files to Azure Blob storage by using AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-upload)  
[Planning for an Azure File Sync deployment](https://learn.microsoft.com/en-us/azure/storage/file-sync/file-sync-planning)  
[What is Azure Data Box?](https://learn.microsoft.com/en-us/azure/databox/data-box-overview)  
[Choose an Azure solution for data transfer](https://learn.microsoft.com/en-us/azure/storage/common/storage-choose-data-transfer-solution)  

---

## Q019:

Which two networking solutions should you recommend for traffic routing? Each correct answer presents part of the solution.

- Azure Load Balancer
- Azure Traffic Manager
- Azure Front Door
- Azure Application Gateway

---

### Answer:
- Azure Front Door
- Azure Application Gateway

You should recommend Azure Application Gateway. To use Application Gateway to expose the APIs, you need to use Application Gateway Ingress Controller (AGIC), which is a Kubernetes application. AGIC monitors the cluster to update Application Gateway whenever a new service is selected to be exposed to the outside world. Application Gateway allows you to perform path-based routing and SSL offloading, and you can also enable web application firewall (WAF) features to protect against malicious attacks.
You should also recommend Azure Front Door. Since the clusters are deployed in two regions, you need to use another global load-balancing solution. For this, you can use Azure Front Door to route traffic based on the latency routing policy. This will send the traffic requests to the nearest region. The backend pool for Azure Front Door will be the two application gateways deployed in each region. You can perform SSL offloading and path-based routing in Front Door, as well. Another benefit of Azure Front Door that is useful in this scenario is traffic acceleration, which allows the user to establish a connection to an edge location. From the edge location, the connection to the backend pools is established.
You should not recommend Azure Traffic Manager. Even though Azure Front Door internally uses Traffic Manager, it provides additional features like SSL offloading, which makes the connection route through the HTTPS protocol. This also satisfies the security requirements, where the APIs are only made available over the HTTPS protocol. Traffic Manager is useful for non-HTTPS traffic. Traffic Manager is a DNS-based traffic load balancer, which also makes it less efficient than Azure Front Door whenever failover needs to happen due to DNS caching and DNS TTLS.
You should not recommend Azure Load Balancer. Load Balancer is useful when creating a Load Balancer- type service in Kubernetes. When you create a Load Balancer service in Kubernetes, the cloud controller creates a new Azure Load balancer to provide access to the service outside the cluster. Creating a load balancer for the entire microservice will be a costly solution. Instead, an ingress controller like AGIC should be used to reduce the cost as well as configuration.

--- 

### References

[Use Application Gateway Ingress Controller (AGIC) with a multi-tenant Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/aks-agic/aks-agic)  
[Traffic acceleration](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-traffic-acceleration?pivots=front-door-classic)  
[Load-balancing options](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/load-balancing-overview)  
[Traffic routing methods to origin](https://learn.microsoft.com/en-us/azure/frontdoor/routing-methods)  
[What is Application Gateway Ingress Controller?](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview)   
[Use a public standard load balancer in Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/load-balancer-standard)  

---

## Q018:

You need to recommend a solution for the microservices to connect to the Azure SQL database. The traffic should always remain private and must be secure.

What should you do?

- Use service endpoints.
- Use Azure Private Link.
- Define outbound firewall rules.
- Define network access controls.

---

### Answer:
- Use Azure Private Link.

You should use Azure Private Link to connect to the Azure SQL database. With Azure Private Link, you can create a private endpoint for the Azure SQL database. This endpoint is private to the virtual network (VNet). This means that traffic never leaves the VNet boundary. When you create a private link, the resources get a private endpoint in the VNet, which is accessible like any other resource deployed inside a VNet and so you can reach the database using the private IP address. Azure Private Link is designed to provide access to Azure Platform-as-a-Service (PaaS) services inside a VNet.
You should not use service endpoints. With service endpoints, the traffic is routed through the Microsoft backbone network. This allows you to access the resources inside your VNet. However, Azure Private Link is a more secure solution compared to service endpoints, since service endpoints are publicly routable addresses.
You should not define network access controls. This is used when you want to allow access from other Azure resources or allow connections from a specific IP address. The connection is not private, and so the traffic traverses the internet to establish the connection.
You should not define outbound firewall rules. This is required for the outbound connection to storage accounts and other Azure SQL logical servers. This is useful when enabling features like auditing, vulnerability scanning, etc.

---

### References

[Azure Private Link for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/private-endpoint-overview?view=azuresql) 
[Use virtual network service endpoints and rules for servers in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/vnet-service-endpoint-rule-overview?view=azuresql)  
[What is Azure Private Link?](https://learn.microsoft.com/en-us/azure/private-link/private-link-overview)  
[Outbound firewall rules for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/outbound-firewall-rule-overview?view=azuresql)  
[Azure SQL Database and Azure Synapse Analytics network access controls](https://learn.microsoft.com/en-us/azure/azure-sql/database/network-access-controls-overview?view=azuresql)  


---

## Q017:

Which compute solution should you recommend for the APIs? The solution must meet the resiliency requirements.

- Azure Kubernetes Service (AKS)
- Azure Container Instance (ACI)
- Azure Functions
- Azure App Service

---

### Answer:
- Azure Kubernetes Service (AKS)

You should recommend Azure Kubernetes Service (AKS) for the APIs. The scenario suggests that the core platform features of the online store will be built as microservices. Generally, microservices communicate with each other and need some form of orchestration. Kubernetes can handle all the complexity of orchestrating the microservices and therefore reduce the operational overhead. With AKS, the developers do not have to worry about networking and service discovery when adding or removing microservices. Also, the requirements suggest that the compute infrastructure should be spread across availability zones for high availability and fault tolerance. Also, AKS is a managed service that is free; customers only pay for the node pools attached to the cluster. Using AKS, you can also perform active-active deployment in multiple regions.
You should not recommend Azure App Service. Azure App Service is a good solution for REST APIs and web apps. However, as the number of APIs increases, the effort to maintain them starts to increase as well. Each App Service would have its own ARM templates for deployment and application settings. Also, it does not include a feature for service discovery of new APIs.
You should not recommend Azure Functions. Azure Functions is a good solution for event-driven serverless APIs. However, it is not an efficient solution for the requirements given in the scenario. It cannot auto- discover other APIs and it is not easy to orchestrate when the number of services increases. You would need other solutions for the inter-communication between these APIs, like a publish-subscribe model using Azure Service Bus or Azure Event Grid.
You should not recommend Azure Container Instance (ACI). ACI is used for small-scale applications and task automation. Hosting microservices is not feasible using ACI.

---

### References
[Azure Kubernetes Service (AKS) architecture design](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-start-here)  
[Advanced Azure Kubernetes Service (AKS) microservices architecture](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices-advanced)  
[AKS baseline for multiregion clusters](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-multi-region/aks-multi-cluster)  
[Tutorial: Host a RESTful API with CORS in Azure App Service](https://learn.microsoft.com/en-in/azure/app-service/app-service-web-tutorial-rest-api)  
[Building serverless microservices in Azure sample architecture](https://azure.microsoft.com/en-us/blog/building-serverless-microservices-in-azure-sample-architecture/)  
[What is Azure Container Instances?](https://learn.microsoft.com/en-in/azure/container-instances/container-instances-overview)  

---

## Q014-016:

CompanyA is a large online clothing retail store that operates from various datacenters in multiple cities in the US. The IT operations team faces many challenges to meet the high demand whenever a sale is announced for the products. You need to recommend a highly available and fault-tolerant application design that meets the following requirements:

- The application must scale based on traffic and demand.
- The application must be able to self-diagnose and self-heal in the case of a failure.
- The database must be available in case of Azure outages.
- The application must be optimized for static content like photos and videos.
- SSL offloading and certificate management must minimize complexity.

---

### References

[Traffic Manager routing methods](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods)  
[Overview of TLS termination and end to end TLS with Application Gateway](https://learn.microsoft.com/en-us/azure/application-gateway/ssl-overview)  
[Active geo-replication](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql)  
[Availability sets overview](https://learn.microsoft.com/en-us/azure/virtual-machines/availability-set-overview)  

[What is a content delivery network on Azure?](https://learn.microsoft.com/en-in/azure/cdn/cdn-overview)    
[Automatic instance repairs for Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-automatic-instance-repairs?tabs=portal-1%2Cportal-2%2Ccli-3%2Crest-api-4%2Crest-api-5)   
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)   

[Caching with Azure Front Door](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-caching?pivots=front-door-standard-premium)   
[What is Azure Front Door?](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-overview)    

---

## Q016:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. The VM scale sets have an autoscaling rule based on CPU utilization. Use Azure SQL Database with Geo-replication enabled. Use Azure Front Door to perform the SSL offloading and route traffic using the priority routing method. Use the caching feature of Azure Front Door to optimize the application's performance for static content.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale sets deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in case of failures. The autoscaling rule to increase and decrease the number of VMs can manage the changes in demand and traffic. In this way, the IT operations team does not have any overhead to perform manual intervention whenever the traffic requirements change. The autoscaling policy can take care of this requirement very efficiently. The instance repair feature of VM scale sets can monitor the health of the instances and replace any instance that fails the health checks. Thus, this ensures that the system self-heals and recovers automatically from a failure.
The Azure SQL Database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Azure Front Door has many features which can be used to fulfill a number of requirements. Its SSL- offloading capabilities also reduce the overhead from the backend pool VMs. Azure Front Door also has caching capabilities that can serve content from point of presence (PoP) locations to improve the application's performance when using static content.

---

## Q015:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. Enable scheduled VMSS autoscaling for the holiday sale dates throughout the year. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager using priority routing between the two regions. Use Azure Content Delivery Network (CDN) to cache the static content and optimize the application's performance.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale set deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in the event of failures. High availability is achieved using the scheduled autoscaling. For the holiday sale, the autoscaling policy can increase the number of VMs based on the predicted demand. With multi- availability zone deployment, the application will still be available when any availability zone goes down and the VM scale set maintains the number of desired VMs across the zones. Also, with the instance repair feature of VM scale sets, you can monitor the status of the instances and, if any instance fails, the VM scale set replaces that instance with a new one.
Additionally, the Azure SQL database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down. Furthermore, Azure Application Gateway can minimize complexity and the overhead from the VMs via SSL offloading and distributing the traffic to the backend pool of the VMs where the application is deployed.
Azure Traffic Manager with priority routing always sends the request to the active site. The request will automatically failover if the active site becomes unavailable. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes. This fulfills the requirement of self-healing infrastructure, as well. The operation team does not have to take any manual action if the active site goes down. The traffic gets routed to the passive site automatically.
CDN can optimize the application's performance by a lot. The photos and videos can get served from the point of presence (PoP) location nearest to the user. The user will experience low latency when the content is served from the PoP locations.

---

## Q014:

Solution: Deploy the application on Azure VMs with an availability set within two regions in active/passive mode. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager with performance routing between the two regions.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. The VMs would not scale based on demand, and you would need manual intervention to scale the number of VMs based on the traffic. Furthermore, this solution also does not address the requirement of optimizing performance for static content and cannot self-heal in case of failures. The performance aspect can be improved by using some caching mechanism, like an Azure Content Delivery Network (CDN), which can cache the photos and videos to the points of presence (POP) locations around the world for faster access. For self-healing, you would need a mechanism that can check the status of the VM and replace it if the health check fails. This can be done using a VM scale set instance repair feature, but it is not available for VMs.
The VMs deployed in the availability set would ensure high availability in the event of Azure outages or during maintenance, as they would be spread across multiple upgrade domains and fault domains, which maintain availability during a maintenance activity and failures respectively. Azure SQL database with geo- replication enabled would also ensure high availability in the event of an Azure outage. With geo- replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Also, Azure AppGateway could reduce the overhead from the VMs to perform the SSL offloading and distribute the traffic to the backend pool of the VMs where the application is deployed. As well as this, Azure Traffic Manager with performance routing would route traffic to the location's closest region where the network latency is lowest. Since the application is deployed in active/passive mode, the traffic should always route to the active site. It should only send traffic to the passive site when the active one is no longer available. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes.

---

## Q011-013:

Your company has an on-premises SQL Server 2019 instance hosted on a dedicated server running Windows Server 2019 Datacenter edition. The database supports a high-volume transaction processing application with hundreds of users connecting to the database at any time. User connections and transaction volume are expected to grow rapidly as your company expands. Rather than investing in new on-premises resources, the company decides to move the database to the cloud.
The database currently contains 3 TB of data and is expected to grow to no more than 4 TB. The solution should support at least one read-only replica in addition to the primary read-write database.

You need to deploy the database to Azure and meet the requirements.

---

### References

[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql&tabs=azure-portal)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  
[Resource limits for single databases using the DTU purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-single-databases?view=azuresql)  

---

## Q013:


Solution: Deploy an Azure SQL Managed instance under the vCore-based purchasing model and choose the Business Critical service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Managed Instance under the vCore-based purchasing model and choose the Business Critical service tier. The Business Critical service tier supports up to 4 TB storage and multiple read-only replicas. In case of failure of the primary database, one of the read-only replicas is promoted to be the new primary database with read-write support.

---

## Q012:


Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Hyperscale service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Database under the vCore-based purchasing model and choose the Hyperscale service tier. The Hyperscale service tier supports up to 100 TB of storage. It supports up to four read-only replicas in addition to the primary read-write database. This is the most expensive option, but it meets all of the requirements.

---

## Q011:

Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier. This deployment option does not meet the storage requirements for the scenario. Premium service P6 is limited to no more than 500 GB. You would need to select a Premium P11 or Premium P15 deployment to meet the storage requirements. This solution also does not meet the read replica requirements.

---

## Q008-010:

A manufacturing company is looking to improve its manufacturing processes. lot sensors throughout the production line collect real-time data. The company wants to perform real-time data collection and analysis from these devices. The solution must support bi-directional communication to send commands back to the lot sensors.

You need to develop a data flow process to meet this requirement.

---

### References

[IoT Concepts and Azure IoT Hub](https://learn.microsoft.com/en-us/azure/iot-hub/iot-concepts-and-iot-hub)  
[Welcome to Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)  
[Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/compare-messaging-services)  
[Connecting IoT Devices to Azure: IoT Hub and Event Hubs](https://learn.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs)  
[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)
[What is dedicated SQL pool (formerly SQL DW) in Azure Synapse Analytics?](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is)  

---

## Q010:

Solution: You configure an lot hub to receive the data and send the data to Azure Stream Analytics.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal and the scenario requirements. The lot hub provides communication between the lot devices and Azure Stream Analytics. Azure Stream Analytics is a real-time event processing engine that can process a high volume of fast streaming data from a variety of sources. This allows for real-time analysis of telemetry streams for lot devices.

---

## Q009:

Solution: You configure an lot hub to receive the data and send the data to Azure Data Factory.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. An lot hub can be used to receive the data and stream it to a destination, but Azure Data Factory is not used for real-time data analysis. Azure Data Factory is a cloud- based extract-transform-load (ETL) and data integration service designed for data movement and large-scale data transformations.

---

## Q008:

Solution: You configure the lot sensors with an event hub and send the data to Azure Stream Analytics for analysis.

Does this solution meet the goal?

---

### Answer: 
No

This solution does not meet the goal. You can use an event hub for streaming data as an input to Azure Stream Analytics. However, an event hub does not provide bi-directional communication with lot devices.

---

## Q004-Q007:

> Overview

CompanyA sells building materials online and in retail outlets around the world. CompanyA's IT infrastructure is hosted in on-premises data centers in multiple locations. The CEO and CTO realize the potential of Microsoft Azure Cloud to increase the company's competitive advantage and eliminate security concerns.

> Existing Environment

- CompanyA operates public websites serving its online shop.
- An online shop web application is connected to an SQL database server in the backend.

- The company's Microsoft SQL database server holds a 100GB product catalog and online order data. Rapid growth of the database size is not planned.

- The Active Directory (AD) authentication provider is in place. The AD domain name is companya.com.
- Marketing documents, media files, and product manuals are stored on file share servers on-premises at the head office's location.

- Marketing documents are accessed via mapped drives on Windows 10 clients.

> Problem Statement

- The SQL database server is not highly available. Every outage means a drop in sales and loss in customer confidence.

- The online shop website's performance on Black Friday is slow. The Microsoft SQL database has been identified as the bottleneck.

- Searching the site when viewing many items on the page is slow.

- There are security concerns regarding the possible loss of financial and personal data due to the potential unauthorized access by support personnel or administrators of the SQL database content.

- Content on file share servers includes not only current files, but also historical data. This historical data is rarely accessed and is needed only in the case of requests relating to legal and compliance matters. Due to legal requirements, the historical data will have to be kept for seven years. This data occupies additional storage and is costly to manage.

- Some remote locations have a slow internet connection and, therefore, access to marketing documents from remote locations is very slow.
- Privileges and rights granted to users and identities are not supervised.

> Planned Changes

- The online shop web servers will be migrated into Azure Cloud.

- The SQL database servers will be migrated into Azure Cloud with the least amount of effort.

- Data security practices will undergo modernization according to industry standard best practices.

- Marketing documents will be migrated from file share servers onto the cloud.

- A solution to reduce costs for historical data on file shares is to be implemented.

> Technical & Business Requirements

- The SQL database must be highly available. Latency must be reduced. 

- CompanyA requires a solution to obtain real-time business insights about customers' purchasing behavior by analyzing data collected from its own online shop's website, social media channels, and partner websites.

> Security & Policy Requirements

- Legal regulations require customer data stored in Microsoft SQL Database to reside in the corresponding region of customer residence.
- All the data has to be secured at rest, in transit and in use.
- Compliance policy requires the retention of online order data for a minimum of seven years.
- Privileges and rights be granted to users and identities must be supervised.

---

## Q007:

In your solution design you need to recommend a solution to meet the technical requirements for Azure SQL Database.
Which solution for Azure database service tier should you recommend?

- Azure SQL Hyperscale
- Azure SQL Business Critical
- An SQL Server on an Azure Virtual Machine
- Azure SQL General Purpose
- Security and policy requirements

---

### Answer:
- Azure SQL Business Critical

You should recommend Azure SQL Business Critical tier. This Azure database service tier is designed to serve for mission-critical applications that require low latency and minimal downtime. This Azure database service tier utilizes Always On availability groups and high performance direct attached SSD storage. This is the best solution to meet the requirement in this scenario.
You should not recommend the Azure SQL General Purpose. This Azure database service tier provides budget-oriented balance between compute and storage. It utilizes multiple failover nodes with spare capacity. In the case of an outage, spare nodes can create a new SQL Server instance and the workload can be failed over. In addition to a higher latency compared to business critical architecture, this service tier introduces additional downtime to create a new SQL Server instance and failover workloads. In this scenario, especially for Black Friday, it is not acceptable.
You should not recommend the Azure SQL Database Hyperscale solution. This Azure database service tier is most suitable for rapid-changing storage needs, which can be rapidly scaled out up to 100TB. As the SQL database in this scenario is only 100GB and rapid growth is not expected, this service tier is not the solution you should recommend.
You should not recommend SQL Server on Azure Virtual Machine. This type of deployment is similar to what you have on premises, except that SQL server is running on an Azure virtual machine. This deployment is a one-server deployment with no guarantee of the required availability.

---

### References

[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  
[Design for Azure SQL Database](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/2-design-for-azure-sql-database)  
[What is an Always On availability group?](https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  

---

## Q006:

You need to recommend a solution to include in your design to meet the security requirement for Azure SQL Database.

What should you recommend?

- SQL Elastic pools
- Azure Cosmos DB with multi-region writes
- Horizontal scaling by Sharding
- Elastic database tools
- Security and policy requirements

---

### Answer:
- Horizontal scaling by Sharding

You should recommend to include Horizontal scaling by Sharding in your SQL database design. Horizontal scaling by Sharding provides a solution to partition (sharding) a database into multiple databases, which can be scaled independently. The Sharding solution utilizes a special database named shard map manager, which maintains global mapping information about all shards (databases) in a shard set. The application uses this shard map to save data into the proper shard. In this scenario, with Sharding, you meet the legal regulations to maintain customer data in the region of respective customer residence by placing each shard with data in the region of customer residence.
You should not recommend SQL Elastic pools. SQL Elastic pools provide a mechanism to scale computer power up or down for SQL Database as needed. In this scenario, SQL Elastic pools cannot meet the requirement for customer data residence as providing more or fewer compute resources does not change the residence of the data. For this specific requirement, you have to implement Horizontal scaling by Sharding in your SQL database design.
You should not recommend Elastic database tools. Elastic database tools provide the solution to query data across multiple databases and provide output to Microsoft Excel or third-party tools for visualization. Elastic database tools have no influence on the data residency, and as such it cannot meet this requirement.
You should not recommend Azure Cosmos DB with multi-region writes. Azure Cosmos DB is a NoSQL database, which provides real-time access, multi-region writes and data distribution to any Azure region, and it can scale storage and throughput across any Azure region elastically and independently. Azure Cosmos DB can provide the best solution for mobile, gaming and lot applications. Although there are possibilities to migrate data from SQL Server to Azure Cosmos DB and implement data residency restrictions, it would be very expensive in terms of efforts and costs. In the given scenario, it is required for the data to reside in the region of corresponding customer residence and the SQL database migration needs to be executed with the least amount of effort.

---

### References

[Recommend a solution for database scalability](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/5-recommend-database-scalability)  
[Welcome to Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)  
[Understanding distributed NoSQL databases](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql)  

---

## Q005:

You are tasked with remediating the security concerns and implement data security requirements.

Which three encryption solutions should you include in your design? Each correct answer presents part of the solution.

- Always Encrypted
- Server-level firewall
- Network security groups (NSGs)
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security


---

### Answer:

- Always Encrypted
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security

To remediate the security concerns for data-at-rest you should recommend Transparent Data Encryption (TDE). To ensure data privacy, data sovereignty, and data compliance it is indispensable and mandatory to encrypt data at rest. TDE provides such a data encryption capability for data at rest and, as such, the data is protected from the unauthorized access of raw offline database files on a hard disk or a backup copy. The encryption is performed by TDE on a page level (logical partitioning of the data within a data file (.mdf or .ndf) in a database, numbered contiguously from 0 to n), so that data is encrypted as it is written from memory to the disk and decrypted as it is read from the disk to memory.
To remediate the security concerns for data-in-use you should also recommend Always Encrypted. Always Encrypted is a security technique to hide sensitive data stored in specific database columns. The data can be only decrypted while it is loaded into memory of a client computer and processed by a client application. This technique protects data also from administrators or other support personnel that are authorized to access databases for maintenance.
To remediate the security concerns for data-in-transit you should, additionally, recommend SSL/TLS. This encryption provides protection of data from man-in-the-middle (MITM) or side channel attacks during data transportation from the backend SQL database server to the front end application server.
You should not recommend a server-level firewall. Although server-level firewalls protect from unauthorized access by end-users, it does not protect from unauthorized access by support personnel or administrators. Server-level firewalls work on the network layer, which is layer 3 of the International Standardization Organization (OSI) reference model. As an administrator, you must have access to the server in order to be able to pass by firewall for SQL server maintenance.
You should not recommend Network security groups (NSGs). Using an NSG in Azure you can filter network traffic between Azure resources, such as virtual machines within a virtual network (VLAN). The functionality of an NSG is similar to the functionality of a firewall. Similar to firewalls, NSGs can prohibit access by any unauthorized user or resource, but it cannot prohibit access by support personnel or administrators.
References
Design security for data at rest, data in motion, and data in use
Protect data in-transit and at rest
Windows Network Architecture and the OSI Model

---

### References

[Network security groups](https://learn.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview)   

---

## Q004:

You need to recommend a storage design solution on the Azure Cloud platform for the marketing documents. Administrative effort and cost must be minimized.

What should you recommend?

- Azure Files
- Azure Queue Storage
- Azure managed disks
- Azure Blob Storage

---

### Answer:
- Azure Files

You should recommend Azure Files as a storage design solution for the marketing documents. Azure Files is a file share hosted on the Azure Cloud platform. Files hosted in Azure Files can be accessed via an industry- standard Server Message Block (SMB) protocol for file sharing. SMB allows applications to read and write to files in a computer network. Azure Files can be mapped as a shared drive on all main operating systems and thus can replace the company's on-premises file shares. In this scenario, marketing documents are accessed via mapped drives on Windows 10 clients, as such Azure Files is the best replacement solution.
You should not recommend Azure Blob Storage. Azure Blob Storage is an object-oriented storage solution.
It is optimized to store unstructured data, such as large binary files, media files, log files, and backup files. Unlike Azure Files, Azure Blob Storage cannot be mapped as a shared drive, which is the main reason why, in this scenario, it is not the ideal storage solution for the marketing documents.
You should not recommend Azure managed disks. Azure managed disks are block-level storage volumes used by virtual machines to store data. Usually, you create Azure managed disks at the same time as a virtual machine.
You should not recommend Azure Queue Storage. Azure Queue Storage is a service to provide an interface for service-to-service communication. Azure Queue Storage can store a large amount of messages and can be accessed from anywhere using the HTTP or HTTPS protocols.

---

### References

[Design for data storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/2-design-for-data-storage)  
[Design for Azure Files](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files)  
[Design for Azure Blob Storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage)  
[Design for Azure managed disks](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/7-design-for-azure-disk-solutions)  
[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[Overview of file sharing using the SMB 3 protocol in Windows Server](https://learn.microsoft.com/en-us/windows-server/storage/file-server/file-server-smb-overview)  

---

## Q001-Q003:

Your organization is moving its business operations to Azure. Your company is organized into three departments that will deploy Azure app services and databases. A fourth department is responsible for internal operations and will deploy resources as necessary to support those activities.
Your company wants to be cost-conscious in its move to the cloud, and exercise cost and budget controls at the department level. The company plans to use the Azure Resource Usage and Rate Card APIs to colle billing data for analysis.

Initially, you set up your company's Azure with one subscription.

You need to design a solution for cost reporting by department. 
The solution should follow best practices for organizing resources and should minimize the effort required to maintain the solution.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Track costs across business units, environments, or projects](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/track-costs)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  

---

## Q003:

Solution: You create a separate resource group for each type of resource, and tag the resources with department and billing code.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You can create a separate resource group for each type of resource, and tag the resources with department and billing code. Microsoft's best practices recommend various options for organizing resources into resource groups, such as by resource type, by project, by lifecycle, and so forth. The use of tags lets you organize the data, and retrieve data by department for cost accounting and budgeting purposes.

---

## Q002:

Solution: You create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure role-based access control (Azure RBAC).

Does this solution meet the goal?

---

### Answer:
No

The solution does not meet the goal. You should not create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure RBAC. Organizing by department is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information. Azure RBAC is used to control and manage access to resources, but it does not retrieve the information you need.

---

## Q001:

Solution: 
You create a separate subscription for each department, and use resource groups to organize resources by project.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not create a separate subscription for each department, and use resource groups to organize resources by project. Creating separate subscriptions for each department adds administrative overhead. Organizing by project is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information.

---
