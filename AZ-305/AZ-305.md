# AZ-305 Practice Test 169 Questions

---

## Q09X:

---

### Answer:

---

### References

---

## Q093:

A company has two applications named App1 and App2 for a point of sale (POS) system deployed as Azure functions. App1 helps to scan the products and sends a message to App2 when payment is initiated. App2 takes care of the payment and sends a message on the status of the payment to App1. It currently uses Azure Queue Storage to deliver the messages.
In future, the applications will be processing large amounts of data when new stores open.
You need to recommend a solution to replace Azure Queue Storage so that it has high throughput and the data is also stored in a NoSQL database.
Which two solutions should you recommend? Each correct answer presents part of the solution.

Choose the correct answers

- Azure Service Bus
- Azure Data Factory
- Azure Event Hubs
- Azure Stream Analytics

---

### Answer:
- Azure Event Hubs
- Azure Stream Analytics

You should recommend Azure Event Hubs for message delivery. Event Hubs can send millions of messages per second and would be a good solution as the number of stores will be increasing in future. Event Hubs can deliver messages to multiple sources. Functions can be triggered based on the message received in the event hub. So, App1 and App2 can use the event hub to trigger each other.
You should also recommend Azure Stream Analytics for storing messages in the NoSQL database. Stream Analytics is a real-time analytics engine that can retrieve data from multiple sources, an event hub being one of them. It can then process the message and write the data to a NoSQL database like Cosmos DB.
You should not recommend Azure Service Bus. Even though Service Bus has high throughput, you would need additional applications to read the message and write it to the database. Service Bus has two solutions: queues and topics. Queues are for single consumer delivery, while topics can be used for multiple consumers. Azure Service Bus would have been a good choice, but stream analytics does not support Service Bus as an input.
You should not recommend Azure Data Factory. This is used for big data solutions that require Extract, Transform, and Load (ETL) and Extract, Load, and Transform (ELT) operations. It uses pipelines to process the data and can store it in the NoSQL database. However, you cannot send messages directly from the event hub to the data factory. You would need to have another solution in between that can store the data for ingesting it into the data factory. You can do this with Event Hub Capture, with which you can store the data into an Azure storage account. The data factory can then read the captured messages from the storage account and process them to update the database.

---

### References

[Azure Event Hubs trigger for Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=python-v2%2Cisolated-process%2Cnodejs-v4%2Cfunctionsv2%2Cextensionv5&pivots=programming-language-csharp)  
[Azure Event Hubs - A big data streaming platform and event ingestion service](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  
[Understand inputs for Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs)  
[Outputs from Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs)  
[Service Bus queues, topics, and subscriptions](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions)  
[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[Capture events through Azure Event Hubs in Azure Blob Storage or Azure Data Lake Storage](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview)  
[Quickstart: Create a data factory by using the Azure portal](https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory)  

---

## Q092:

You work at a retail company that stores large amounts of data in relational and non-relational databases, and that has file storage in both cloud and an on-premises datacenter. The company decides to analyze some data in real time to update its dashboards. The data will also be used to train the machine-learning model that will be used to provide recommendations in the online store.
You need to recommend a solution to gather actionable insights from the data being collected.
What should you recommend?

Choose the correct answer

- Azure Databricks
- Azure Data Lake
- Azure Data Factory
- Azure Stream Analytics

---

### Answer:

You should recommend Azure Databricks for this scenario. With Databricks, you can run big data pipelines using both batch and real-time data. It can gather data from the Data Factory in batches and, for real-time data, Event Hubs or loT Hub can be used. You can also train machine-learning models inside your Databricks workspaces. It uses Spark, which is a large-scale data-analytics language that can perform all the required actions.
You should not recommend Azure Data Lake. Data Lake is purely a storage solution and it cannot perform any analytics on the data. The data collected is generally unstructured, in the form of blobs and files. To analyze the data, you can use any Hadoop filesystem (HDFS) framework. Data Lake can also store data from real-time services like Event Hubs.
You should not recommend Azure Data Factory. With Data Factory, you can perform big data analytics using data-driven pipelines. It is generally used to orchestrate data movement between various sources and to transform the data. It cannot do real-time processing and machine learning, so it does not fit the requirements in this scenario.
You should not recommend Azure Stream Analytics, since it is a real-time processing engine. It can collect data from multiple sources and provide quick insights. It can store these insights in multiple data storages. However, it cannot train machine-learning models, which are required for recommending the products in the online store.

---

### References

[What is Azure Databricks?](https://learn.microsoft.com/en-gb/azure/databricks/introduction/)
[Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Welcome to Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)  

---

## Q091:

A company hosts an e-commerce website that uses Azure Content Delivery Network (CDN) to serve the photos and videos of the products from the catalog. A storage account named assetstrg is used to store all of the photos and videos. The product catalogue managers use custom scripts to upload the photos and videos of a product.
You need to recommend a solution that provides the product catalog managers with access that only allows them to upload the new files to the storage account from the Company's office network.
Which two security solutions should you recommend? Each correct answer presents part of the solution.

Choose the correct answers

- Use account shared access signature (SAS).
- Use access keys.
- Enable firewall policies and rules.
- Enforce the minimum TLS version.
- Enable secure transfer.
- Use service shared access signature (SAS).

---

### Answer:
- Enable firewall policies and rules.
- Use service shared access signature (SAS).

You should recommend using service shared access signature (SAS). A service SAS provides fine-grained control to access the resources in a storage account. It also allows you to set the start and end date of the token. With service SAS, you can limit access to the blob service in the storage account. Since the product catalog managers only need access to add the new files, a service SAS token for blob service with Add permissions can be created with a short expiry time.
You should also recommend enabling firewall policies and rules. By default, the storage account is available from all networks, including the internet. You can define the containers' public access levels to restrict who can access the blobs in the container. For further security, you can also put network restrictions on the storage account. You can add the IP address range of the company's network such that the storage account is only accessible from the office.
You should not recommend an account shared access signature (SAS). Account SAS is used when access to one or more storage account services is required. This solution can fulfill similar requirements to a service SAS, but it also provides access to other storage account services like queues, tables, and files. A better solution for this scenario is to create a service SAS.
You should not recommend access keys. Access keys are equivalent to super administrator credentials, which allow a user to perform any activity and operation on the storage account. Since the product catalog managers only need to upload the photos and videos, you should follow the principle of least privilege and provide them with upload access only using a service SAS.
You should not recommend enabling secure transfer. This setting is enabled by default and it ensures that any requests made to the storage account are made using HTTPS protocol. The storage account is available via HTTP protocol, as well, but this is an insecure way to communicate with the storage account. Even though it is recommended to use secure communication, it does not meet the scenario's goals.
You should not recommend enforcing the minimum TLS version. Since HTTPS connections are enabled by default, the storage account allows all the TLS versions for backward compatibility for the client connections.
The CDN allows the photos and videos to be served from one of the many points of presence (PoP) locations that are closer to the user accessing the website. The content is cached in these PoP locations from the storage account assetstrg. This improves user experience as the website loads faster, which means greater engagement and value for the business.

---

### References

[Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)  
[Configure Azure Storage firewalls and virtual networks](https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal)  
[Require secure transfer to ensure secure connections](https://learn.microsoft.com/en-us/azure/storage/common/storage-require-secure-transfer)  
[What is a content delivery network on Azure?](https://learn.microsoft.com/en-us/azure/cdn/cdn-overview)  
[Enforce a minimum required version of Transport Layer Security (TLS) for requests to a storage account](https://learn.microsoft.com/en-us/azure/storage/common/transport-layer-security-configure-minimum-version?tabs=portal)  

---

## Q090:

Your company is identifying requirements and preparing implementation designs for moving most of its resources from on-premises to Azure. These requirements include several storage scenarios with various size, performance, access, and long-term storage requirements. You are focused on matching storage account types to appropriate storage requirements.
You need to identify which types of Azure storage accounts support access tiers.
Which two storage account types should you include? Each correct answer presents part of the solution.

Choose the correct answers

- BlockBlobStorage
- FileStorage
- BlobStorage
- General-purpose V1
- General-purpose V2

---

### Answer:
- BlobStorage
- General-purpose V2

You should include General-purpose V2 and BlobStorage. These are the only storage accounts that support hot, cool, and archive access tiers. Archive storage is supported for block blobs only.
General-purpose V2 storage accounts support blob, file, queue, table, disk, and Data Lake Gen2 storage services, and let you choose from standard or premium performance tiers. BlobStorage supports block blobs and append blobs only, and it is limited to the standard performance tier.
You should not use General-purpose V1, BlockBlobStorage, or FileStorage. General-purpose V1, BlockBlobStorage, and FileStorage do not support access tiers.
General-purpose V1 storage accounts support blob, file, queue, table, and disk storage services, and the standard and premium performance tiers.
BlockBlobStorage accounts support block blobs and append blobs only, and they are limited to the premium performance tier.
FileStorage accounts support file services only, and they are limited to the premium performance tier.

---

### References

[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview) 

---

## Q089:

Your company currently sends all Azure log data to a Log Analytics workspace. The company plans to configure an event hub to also send the data to blob storage for long-term storage and trend analysis. Data will be maintained for at least 180 days..
The analysis will compare legacy data with current activity log data periodically. Access latency to the stored data and storage costs should be minimized. The solution should support zone-redundant storage (ZRS). You plan to create a new storage account to hold the log data.
You need to determine the type of storage account that you should create.
What type of storage account should you create?

Choose the correct answer

- General-purpose v2
- General-purpose v1
- FileStorage
- BlockBlobStorage

---

### Answer:
- General-purpose v2

You should create a General-purpose v2 storage account. This account type supports blob storage and lets you choose an access tier. In this scenario, you would choose a cool storage tier to meet the access latency requirement while minimizing costs. General-purpose v2 storage accounts support zone-redundant storage (ZRS).
You should not create a General-purpose v1 account. This account type does not support access tiers nor does it does support ZRS.
You should not create a BlockBlobStorage account. This account type does not support access tiers. It is
not the most cost-effective solution because it only uses the premium storage tier.
You should not create a FileStorage account. A FileStorage account does not support blob storage.


---

### References

[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)  
[Send Azure Monitor activity log data](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)   

---

## Q088:

You are the cloud architect for a newly founded manufacturing company that has built out the entirety of its IT infrastructure in Azure. You are reviewing the company's core business IT systems to identify its data archiving needs.
Due to financial regulations, the accounting system's database backups need to be retained for 10 years in the event of a historical audit. If needed, these backups must be retrieved within one day after the request for a file is received.
The order tracking system has a database that tracks order details and associated materials. The system also generates PDF files of the final build sheets for the company's work. The company must keep the database backups for three months, at which point the backups can be deleted. If needed, database backup of the order tracking system must be available immediately. The generated PDFs must be kept indefinitely with minimal costs.
You need to identify if the Azure storage archive tier meets the requirements for the long-term storage of each IT system.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.


The accounting system backups can be stored in an Azure Storage account that has the access tier set to Archive.
Yes
The order tracking system PDF files can be stored in an Azure Storage account that has the access tier set to Archive.
Yes

The order tracking system database backups can be stored in an Azure Storage account that has the access tier set to Archive.
No

---

### Answer:

The accounting system's backups can be stored in the archive tier. This is the least expensive method to store files that will be kept for at least 180 days. Files stored in the archive tier cannot be accessed immediately, because rehydration can take up to 15 hours. However, this is less than one day, which is the maximum access time for the backups in this scenario.
The order tracking system PDF files can be stored in the archive tier. Files will be kept indefinitely (which is more than 180 days) and the archive tier has the minimum storage cost.
The order tracking system database backups should not be stored in the archive tier because the backups must be available immediately when needed. Files stored in the archive tier cannot be accessed immediately, because rehydration can take up to 15 hours.

---

### References

[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  

---

## Q087:

A company is planning to use Azure Blob storage for a web application which is hosted on an Azure VM.
The data consists of several hundred files that are 1 GB or larger in size. The data is frequently accessed and staged for processing.
You need to provide a solution that minimizes the costs of data access.

Which access tier should you recommend?

Choose the correct answer

- Hot
- Cool
- Archive

---

### Answer:
- Hot

You should recommend the Hot access tier because it is expected to be accessed frequently for both read and write operations. For this storage tier, the cost is higher for the storage and lower for the access methods. This meets the requirement of low access costs.
You should not recommend the Cool access tier because it is designed for storing data that is infrequently accessed and stored for at least 30 days.
You should not recommend the Archive access tier because it is designed for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements.

---

### References

[Azure Archive Storage](https://azure.microsoft.com/en-us/products/storage/)  
[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  

---

## Q086:

Your company wants to configure a storage account for a new application. The storage account must remain available if a single Azure data center fails. The new application should perform more than 95% write operations. When the application needs read access, data must be available immediately.
You need to recommend a solution that offers the lowest storage cost for the required usage pattern.
Which storage account type and access tier should you use? To answer, select the appropriate options from the drop-down menus.

Choose the correct options
ZRS | LRS | GRS | RA-GRS
Hot | Cool | Archive

Storage account type: Zone-redundant storage (ZRS)
Storage account access tier: Hot

---

### Answer:

You should use the zone-redundant storage (ZRS) account type. ZRS replicates your data synchronously across three storage clusters in a single region. Each storage cluster is physically separated from the others and is located in its own availability zone (AZ). If a data center becomes unavailable, ZRS is still available.
You should not use the locally-redundant storage (LRS) account type. Although LRS is the least expensive option, it survives only if a server fails. LRS does not survive the failure of the whole data center.
You should not use the geo-redundant storage (GRS) or read-access geo-redundant storage (RA-GRS) account types. Although they provide a sufficient level of redundancy, both are more expensive than ZRS.
You should use the hot access tier because it is cheaper when data is written or accessed frequently. Storing data is a little more expensive than with the cold tier, but the savings will still be much higher in hot tier as compared to cold tier.
You should not use the cool storage account access tier. Although it has a low storage cost option that meets the usage requirements, it will be expensive because of the write operations. You might end up paying more instead of saving in costs.
You should not use the archive access tier. Although archive storage is the least expensive option in terms of storage cost, it does not meet the requirements. Data in the archive tier is not available immediately. The data must be rehydrated first, which takes up to 15 hours.

---

### References

[Use geo-redundancy to design highly available applications](https://learn.microsoft.com/en-us/azure/storage/common/geo-redundant-design)  
[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview) 
[Azure Storage redundancy](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy)  


---

## Q085:

You are designing a monitoring strategy for an Azure SQL Database. 
You want metrics to be collected from the database and provide performance reports on a dashboard.
You need to minimize programming effort.

Which logging target should you specify in the design?

Choose the correct answer

- Azure Blob Storage
- Azure Event Hubs
- Azure Log Analytics
- Azure Table Storage

---

### Answer:

You should use Log Analytics in Azure Monitor. You can enable the diagnostic settings on the database and send the metrics to the Azure Log Analytics workspace. To generate the dashboard, you can enable Azure SQL Analytics in Azure Monitor. Azure SQL Analytics can consume the data from the Log Azure Log Analytics workspace to provide the performance reports on a dashboard without any programming efforts.
You should not use Azure Blob Storage. Blob Storage is used for storing large amounts of unstructured object data. Although you can enable the diagnostic settings on the database and send the metrics to the Azure Blob Storage, it would require lots of programming efforts to read the data from blob storage and generate the dashboard.
You should not use Azure Event Hubs. Event Hubs is a Big Data streaming platform. While it could be used by enabling the diagnostic settings on the database and send the metrics to the Azure Event Hubs. It is generally used to ship the data to third-party monitoring solutions or building custom logging and metrics solutions.
You should not use Azure Table Storage. Table Storage is used for storing structured NoSQL data in the cloud. Besides, it is not possible to send the diagnostics data directly to Table Storage.

---

### References

[Overview of Log Analytics in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-overview)  
[Monitor Azure SQL Database using Azure SQL Analytics (Preview)](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/insights/azure-sql)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?WT.mc_id=Portal-Microsoft_Azure_Monitoring&tabs=portal)  
[Introduction to Azure Blob storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction)  
[Azure Event Hubs - A big data streaming platform and event ingestion service](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about) 
[What is Azure Table storage?](https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview)  

---

## Q084:

Your company is in the process of migrating its data operations from an on-premises network to Azure storage. The company has over 65 terabytes of files currently stored in on-premises file servers and it wants to import this data into an Azure cool storage tier.
You need to recommend a solution for transferring the data. The solution must minimize costs and interruptions to the on-premises network.

What solution should you recommend?

Choose the correct answer

- Data Migration Assistant
- AzCopy
- Azure Data Box
- Azure Data Factory

---

### Answer:
- Azure Data Box

You should recommend Azure Data Box. Azure Data Box provides a cost-effective way to migrate large amounts of on-premises data to Azure. Data is transferred from on-premises to a Data Box, Data Box Disk, or Data Box Heavy, which is then sent to Microsoft for data load. You should use:

- Data Box Disk when transferring up to 40 terabytes (35 terabytes of usable capacity)
- Data Box when transferring up to 100 terabytes (80 terabytes of usable capacity)
- Data Box Heavy when transferring up to 1 petabyte (800 terabytes of usable capacity)

You should not recommend Azure Data Factory. Azure Data Factory is a cloud-based extract-transform-load (ETL) and data integration service designed for data movement and large-scale data transformations for cloud-based data.

You should not recommend AzCopy. AzCopy is a command-line utility that you can use to copy blobs and files to or from Azure storage accounts. AzCopy can copy on-premises data, but it is not designed to support the type of data transfer needed in this scenario.

You should not use Azure Data Migration Assistant. Data Migration Assistant is used to identify compatibility issues that can adversely impact data functionality when migrating to a new version of SQL Server or Azure SQL Database.

---

### References

[Azure Data Box for data transfer](https://azure.microsoft.com/en-us/products/databox/data/)  
[Azure Data Box: Frequently Asked Questions](https://learn.microsoft.com/en-us/azure/databox/data-box-faq?tabs=data-box-and-data-box-heavy)  
[Get started with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Overview of Data Migration Assistant](https://learn.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15)  

---

## Q083:

Your company is migrating data, workloads, and applications from Data Lake Storage Gen1 to Data Lake
Storage Gen2 to improve support for big data analytics. You want to accomplish this by using an
incremental copy pattern. You create a destination storage account and prepare the source data for
migration.
You need to determine what you should use to transfer the data.

What should you use?

Choose the correct answer

- Azure Data Box
- Azure Data Migration Assistant
- AzCopy
- Azure Data Factory

---

### Answer:
- Azure Data Factory

You should use Azure Data Factory. This is the recommended method for transferring data between Data Lake Storage Gen1 and Gen2. Azure Data Factory is a cloud-based extract-transform-load (ETL) and data integration service designed for data movement and large scale data transformations.
You should not use AzCopy. AzCopy is a command-line utility that you can use to copy blobs and files to or from Azure storage accounts. AzCopy is able to copy data into Azure Data Lake but it is not designed to support the type of data transfer needed in this scenario.
You should not use Azure Data Box. Azure Data Box provides a cost-effective way to migrate large amounts of on-premises data to Azure. Data is transferred from on-premises to a Data Box, Data Box Disk, or Data Box Heavy, which is then sent to Microsoft for data load.
You should not use Azure Data Migration Assistant. Data Migration Assistant is used to identify compatibility issues that can adversely impact data functionality when migrating to a new version of SQL Server or Azure SQL Database.

---

### References

[Azure Data Lake Storage migration guidelines and patterns](https://learn.microsoft.com/en-us/previous-versions/azure/storage/blobs/data-lake-storage-migrate-gen1-to-gen2)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Azure Data Box for data transfer](https://azure.microsoft.com/en-us/products/databox/data/)  
[Get started with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)  
[Overview of Data Migration Assistant](https://learn.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15)  

---

## Q082:

Your company is planning a merger with another company of a similar size. The other company has an on- premises data infrastructure that consists of proprietary databases, XML files, and PDF files of scanned documents. The merger is about to happen soon, and you need a way to quickly place this data in the cloud for analysis.
You need to choose an appropriate data flow and storage solution.

What should you do?

Choose the correct answer

- Use Data Factory to copy the data from the other company and store it in Azure SQL Database.
- Use Data Lake to copy the data from the other company and store it in HDInsight.
- Use HDInsight to copy the data from the other company and store it in Data Factory.
- Use Data Factory to copy the data from the other company and store it in Data Lake.

---

### Answer:
- Use Data Factory to copy the data from the other company and store it in Data Lake.

You should use Data Factory to copy the data from the other company and store it in Data Lake. Data Factory is a data integration service that allows you to create workflows for transforming and moving data. In this scenario, the data needs to be moved from the other company to Azure Data Lake. Data Lake is a hyper-scale repository for big data workloads. It allows you to capture data of any size and type, which is ideal in this scenario.
You should not use HDInsight to copy the data from the other company and store it in Data Factory. HDInsight is a big data solution that makes it easy to process large amounts of data. It is typically used for batch processing, data warehousing, data science, and streaming of Internet-of-Things (IoT) devices. Also, data cannot be stored in Data Factory. Data Factory is a workflow service.
You should not use Data Factory to copy the data from the other company and store it in Azure SQL Database. Azure SQL Database is a Platform-as-a-Service (PaaS) offering of Microsoft SQL Server, which is a relational database engine. It is not ideal in this scenario because you need to store data of varying types that is not all relational.
You should not use Data Lake to copy the data from the other company and store it in HDInsight. Data Lake stores data. It cannot copy data.

---

### References

[Copy and transform data in Azure Data Lake Storage Gen2 using Azure Data Factory or Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage?source=recommendations&tabs=data-factory)  
[What is Azure Data Factory?](https://learn.microsoft.com/en-us/azure/data-factory/introduction)  
[Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)  
[What is Apache Hadoop in Azure HDInsight?](https://learn.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-introduction)  
[What is Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview?view=azuresql)  

---

## Q081:

You work for a consumer sales company that has gathered market data from a variety of sources. You have noticed that the data from some sources has schema drift issues.
You need to recommend a data preparation solution to prepare the data for analysis. The solution needs to minimize development effort

Which Azure service should you recommend?

Choose the correct answer

- Azure Functions
- Azure Data Factory
- Azure Logic Apps
- Azure Stream Analytics

---

### Answer:

You should recommend Azure Data Factory because it can help you to develop graphical data transformation logic without writing code. It can also protect against schema drift.
You should not recommend Azure Functions because, for every schema change, you would need to change the function code as well.
You should not recommend Azure Stream Analytics. For every schema change, you would need to change the code of the job.
You should not recommend Azure Logic Apps because any change to the schema of the input objects would require a change to the structure of the application.

---

### References

[Schema drift in mapping data flow](https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift)  
[Azure Stream Analytics documentation](https://learn.microsoft.com/en-us/azure/stream-analytics/)  
[Azure Functions documentation](https://learn.microsoft.com/en-us/azure/azure-functions/)  
[What is Azure Logic Apps?](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview)  

---

## Q080:

You have an Azure Blob storage account containing data accessed several times daily.
You plan to upload a new blob to the Blob storage account. The data in this specific new blob will be viewed infrequently but must be available immediately when accessed.
You must configure the access tier for the new blob. The solution must minimize storage costs.

What should you do?

Choose the correct answer

- Set the default access tier for the storage account to Cool.
- Set the access tier for the new blob to Archive.
- Set the access tier for the new blob to Cool.
- Set the default access tier for the storage account to Hot.

---

### Answer:


You should set the access tier for the new blob to Cool. Cool storage is intended for data accessed infrequently and stored for 30 days or more. Cool storage has a similar time-to-access as Hot data. Although it has a slightly lower availability compared to Hot data, storage costs are lower.
You should not set the access tier for the new blob to Archive. Although Archive storage is the least expensive, it also has several hours of retrieval latency.
You should not set the default access tier for the account to Hot. Usually, the blobs in the storage account are accessed frequently. If the tier for the account is set to Hot, this applies to the new blob as well.
You should not set the default access tier for the account to Cool. The Cool access tier is appropriate specifically for the new blob only. If the default tier for the account is set to Cool, this applies to the next uploaded blobs as well, and these blobs might need to be frequently accessed.

---

### References

[Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)  
[Assign Azure roles using the Azure portal](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal)  

---

## Q079:

You have an Azure Blob storage account containing data accessed several times daily.
You plan to upload a new blob to the Blob storage account. The data in this specific new blob will be viewed infrequently but must be available immediately when accessed.
You must configure the access tier for the new blob. The solution must minimize storage costs.

What should you do?

Choose the correct answer

- Set the default access tier for the storage account to Cool.
- Set the access tier for the new blob to Archive.
- Set the access tier for the new blob to Cool.
- Set the default access tier for the storage account to Hot.

---

### Answer:
- Set the access tier for the new blob to Cool.

You should set the access tier for the new blob to Cool. Cool storage is intended for data accessed infrequently and stored for 30 days or more. Cool storage has a similar time-to-access as Hot data. Although it has a slightly lower availability compared to Hot data, storage costs are lower.
You should not set the access tier for the new blob to Archive. Although Archive storage is the least expensive, it also has several hours of retrieval latency.
You should not set the default access tier for the account to Hot. Usually, the blobs in the storage account are accessed frequently. If the tier for the account is set to Hot, this applies to the new blob as well.
You should not set the default access tier for the account to Cool. The Cool access tier is appropriate specifically for the new blob only. If the default tier for the account is set to Cool, this applies to the next uploaded blobs as well, and these blobs might need to be frequently accessed.

---

### References

[Access tiers for blob data](https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview)  

---

## Q078:

You are designing a database solution for the databases hosted in an on-premises environment. Currently, there are 10 databases that are hosted on individual VMs. Each database has varying usage patterns and the resources on the VM are underutilized.
You need to recommend a database solution that minimizes costs and reduces the administrative overhead.

The solution should meet the following requirements:

- The solution should have a 99.99% uptime SLA.
- The resources in the solution can scale dynamically.

The solution should have geo-replication capabilities.
Which solution should you recommend?

Choose the correct answer

- 10 Azure SQL databases
- 10 databases running in SQL server VMs
- 10 Azure SQL Managed Instances
- An Azure SQL database elastic pool hosting 10 databases

---

### Answer:

You should recommend an Azure SQL database elastic pool hosting 10 databases. Elastic pools allow putting multiple databases on the same Azure SQL server instance. Since the 10 databases have varying usage patterns and underutilized resources on the VM, an elastic pool can greatly reduce the cost and maintenance by hosting them together. In an elastic pool, the resources are shared among all the databases. You can scale dynamically based on the usage anytime. Since it is an Azure managed service, it is backed up with a minimum of 99.99% uptime SLA. You can also enable geo-replication for individual databases to prevent them from any disasters.
You should not recommend 10 Azure SQL databases. This solution almost meets all the requirements, but the databases would still be underutilized and you would need to manage 10 Azure SQL databases. This solution would also increase the costs, since you would pay for 10 Azure SQL databases.
You should not recommend 10 databases running in SQL server VMs. This would be a very similar setup that the one you are currently running on the on-premises environment. Since its a laaS service, you need to design for high availability by yourself using VM scale sets or availability sets, which can provide 99.99% uptime SLA, however, the cost to run the solution would be high. Replicating databases to another region would also become your responsibility.
You should not recommend 10 Azure SQL Managed Instances. Managed Instance is also a managed service that fulfills the requirements, however, it would be costly to run and would leave resources underutilized.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  
[High availability for Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla-local-zone-redundancy?view=azuresql&tabs=azure-powershell)  
[Service Level Agreements (SLA) for Online Services](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?lang=1)  
[Business continuity and HADR for SQL Server on Azure Virtual Machines](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/business-continuity-high-availability-disaster-recovery-hadr-overview?view=azuresql)  

---

## Q077:

You are designing a solution for hosting a business critical application in the cloud. For the application, you expect low I/O latency from the database.
You need to recommend a solution that meets the following requirements:

- The solution should have ability to scale individual components like compute and storage.
- The solution should provide secondary read-only replica for running analytics queries.

Which database service tier should you recommend?

Choose the correct answer

- DTU-based Premium
- DTU-based Standard
- vCore-based General Purpose
- vCore-based Business Critical 

---

### Answer:
- vCore-based Business Critical

You should recommend the vCore-based Business Critical database service tier. With the vCore-based purchasing model, you can scale the components individually, like compute and storage. This allows greater flexibility and control to the user. With the Critical Business service tier, you get low I/O latency for read- write operations. It also provides a free-of-charge secondary replica where you can run the analytics queries without affecting the performance of the database.
You should not recommend the vCore-based General Purpose database service tier. You can use General Purpose for generic database workloads. It provides higher latency than the Business Critical service tier and no secondary read replicas.
You should not recommend the DTU-based Premium or the DTU-based Standard service tiers. In the database transaction unit (DTU)-based purchasing model, the user has no flexibility to choose the compute and storage. DTU is a combination of CPU, memory, read and writes. You have to select a predefined SKU when using the DTU model.

---

### References

[Compare vCore and DTU-based purchasing models of Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/purchasing-models?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  
[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  

---

## Q076:

You have been tasked with designing security measures for a relational database solution.
You recommend enabling transparent data encryption (TDE) on the databases to protect data at rest.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Database backups are also encrypted when TDE is enabled.
Yes

Enabling TDE will require changes to be made to the application.
No

TDE is enabled by default for new databases.
Yes

---

### Answer:

Database backups are also encrypted when transparent data encryption (TDE) is enabled. The backup process copies the pages from the encrypted database file to the backup device. The data is never decrypted during the backup process.
Enabling TDE will not require changes to be made to the application. The application can perform real-time encryption and decryption without requiring any changes to be made.
TDE is enabled by default for new databases. All newly created databases are encrypted by default using the service-managed transparent data encryption.

---

### References

[Transparent data encryption for SQL Database, SQL Managed Instance, and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&tabs=azure-portal)    
[Design security for data at rest, data in motion, and data in use](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/7-design-security-for-data-at-rest-data-transmission-data-use)  

---

## Q075:

You are designing a monitoring strategy of user sign-ins for a web application.
You need to evaluate Microsoft Entra activity logs as a possible solution.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

You can configure Microsoft Entra sign-in logs to be routed to an Azure Storage account for archiving.
Yes

An Azure Log Analytics workspace is required to enable sending Microsoft Entra activity logs to Azure Monitor.
Yes

Microsoft Entra Password Protection is a prerequisite to enable Microsoft Entra activity logs.
No

---

### Answer:

You can configure Microsoft Entra sign-in logs to be routed to an Azure Storage account for archiving. This option is useful if you want to store Activity Log events for more than 90 days, which is the default number of days Activity Log events are stored on the Azure platform.
An Azure Log Analytics workspace is required to enable sending Microsoft Entra activity logs to Azure Monitor.
Microsoft Entra Password Protection is not a prerequisite to enable Microsoft Entra activity logs. This feature is used to enforce password policies in an organization. Microsoft Entra activity logs can be used even if Microsoft Entra Password Protection is not deployed.

---

### References

[What are the Microsoft Entra activity log integration options?](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/concept-log-monitoring-integration-options-considerations)   
[Monitor and review logs for on-premises Microsoft Entra Password Protection environments](https://learn.microsoft.com/en-us/entra/identity/authentication/howto-password-ban-bad-on-premises-monitor)  


---

## Q074:

A company is looking to enable its engineers to adopt Azure for a Software-as-a-Service (SaaS) product they are creating. It will be a multi-tenant application that would require lots of databases for each tenant.
You need to recommend a relational database solution that meets the following requirements:
The solution must be cost-efficient.
The team does not have a database administrator, so the management overhead of the database must be kept to a minimum.

The database must support geo-replication and auto-failovers.

Which database solution should you recommend?

Choose the correct answer

- An Azure SQL Database elastic pool
- An Azure SQL Database single database
- SQL Server on Azure virtual machines
- Azure SQL Managed Instance

---

### Answer:

You should recommend an Azure SQL Database elastic pool for the services. Azure SQL is a Platform-as-a- Service (PaaS) solution that is completely managed by Azure, so you do not have to perform any database administration work to manage the servers. With an elastic pool, you can put multiple databases in a single logical SQL server and all the databases will share the resources configured on that SQL server. This is cost- efficient as resources are being shared and there is no underutilization of resources. It also supports geo- replication and auto-failovers in the event of outages.
You should not select an Azure SQL Database single database. This provides the same features as an elastic pool, but it is not as cost-efficient since you will be creating a single database for each service. This means the cost of your database will increase.
You should not select SQL Server on Azure virtual machines. This is an Infrastructure-as-a-Service (laaS) solution where an SQL server is deployed on the virtual machine. This will require a database administrator who can manage the aspects of the server. You would need to replicate data by yourself and no failover options are available.
You should not select Azure SQL Managed Instance. SQL Managed Instance is also a Paas solution, like Azure SQL, but it provides more control on how the server is configured. To configure it correctly, you would need a database administrator, which will make it more costly than an SQL database elastic pool. It provides an automated backups feature, but no auto-failover capabilities.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  
[What is a single database in Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-overview?view=azuresql)  
[What is Azure SQL Managed Instance?](https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql)
[What is SQL Server on Windows Azure Virtual Machines?](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/sql-server-on-azure-vm-iaas-what-is-overview?view=azuresql)  

---

## Q073:

Your company is developing a new data-intensive application. The initial database will contain approximately 1 TB of data. After the production version of the database is released, it is projected to grow by 300 to 500 GB per year.

All development will take place in Azure.

You need to recommend an Azure database solution that meets the following requirements:

- Microsoft SQL Server compatibility
- Minimal management requirements
- Minimal cost
- Support for DTU-based purchasing model
- Support for widely varying processing requirements during development
- Which storage solution should you recommend?

Choose the correct answer

- Azure SQL Database elastic pool
- Azure SQL managed instance
- SQL Server on Azure VM
- Azure SQL Database single database

---

### Answer:

You should recommend an Azure SQL Database single database. This database solution meets all the scenario requirements. However, if required, in future, with the growth of the database that is 300-500GB per year, an additional database can be added to accommodate the needed capacity and the databases can be added to an elastic pool.
You should not recommend Azure SQL Database elastic pool. This is used when you have multiple databases with different user requirements and peak use periods to provide cost savings by having the database share resources. In this scenario, a single database that does not need an Azure SQL Database elastic pool is needed.
You should not recommend Azure SQL managed instance. Managed instance meets most of the scenario requirements but does not support a DTU-based pricing model. Pricing support is for vCore only.
You should not recommend SQL Server on Azure VM. This option has the greatest compatibility with the SQL Server but it also has the highest management overhead. This is because you are responsible for managing the operating system and database engine.

---

### References

[What is a single database in Azure SQL Database?](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-overview?view=azuresql)  
[What is Azure SQL?](https://learn.microsoft.com/en-us/azure/azure-sql/azure-sql-iaas-vs-paas-what-is-overview?view=azuresql&WT.mc_id=thomasmaurer-blog-thmaure)  
[Dynamically scale database resources with minimal downtime](https://learn.microsoft.com/en-us/azure/azure-sql/database/scale-resources?view=azuresql)  
[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  

---

## Q072:

Your Azure environment includes an Azure SQL Managed Instance that hosts a single database. The database includes data columns that contain especially sensitive data. Access to this data must be strictly controlled, including limiting access to administrators and other privileged users.
You are investigating the use of the Always Encrypted feature to provide additional protection.
You need to identify the features and functionality of Always Encrypted.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Always Encrypted encryption keys are never exposed to the SQL Managed Instance and are stored in an Azure Key Vault only
No

Always Encrypted can be used to prevent cloud admins and db_owners from viewing protected data.
Yes

Always Encrypted encrypts data at rest, in memory, and in use for protected columns.
Yes

Always Encrypted allows you to identify sensitive data columns before deploying a production database.
Yes

---

### Answer:

Always Encrypted encryption keys are never exposed to the SQL Managed Instance. However, encryption keys can be stored in Azure Key Vault or in Windows Certificate Store. Microsoft recommends using Azure Key Vault for key storage because this makes it easier to manage the encryption keys.
Always Encrypted can be used to prevent cloud admins and db_owners from viewing protected data. Preventing access to the data by privileged users is a primary reason for implementing Always Encrypted. This includes administrators, cloud admins, and privileged database users.
Always Encrypted encrypts data at rest, in memory, and in use for protected columns. However, it is not meant as a replacement for Transparent Data Encryption (TDE) or Transport Layer Security (TLS) for data protection, but as a supplement for your most sensitive data.
Always Encrypted allows you to identify sensitive data columns before deploying a production database. This is recommended so you can avoid having to make changes to your applications after moving to the production database.

---

### References

[Azure encryption overview](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-overview)  
[An overview of Azure SQL Database and SQL Managed Instance security capabilities](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-overview?view=azuresql)  
[Playbook for addressing common security requirements with Azure SQL Database and Azure SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-best-practice?view=azuresql)  

---

## Q071:

Your company has a hybrid network with an on-premises Active Directory (AD) and Microsoft Entra ID. A database server named DB1 is being migrated from an on-premises SQL Server to an Azure SQL Database.
You need to ensure that data in transit between on-premises users and the SQL Database is encrypted. The negative impact on database and communication performance should be kept to a minimum.
What should you recommend to meet the requirement?

Choose the correct answer

- Dynamic data masking
- Transparent Data Encryption (TDE)
- Transport Layer Security (TLS)
- Always Encrypted

---
### Answer:

- Transport Layer Security (TLS)
You should recommend the Transport Layer Security (TLS) protocol to protect data in transit. Data is encrypted when transmitted either way between the database and the client. TLS provides strong authentication, message privacy, and data integrity controls. It is relatively easy to deploy and implement.
You should not recommend the Transparent Data Encryption (TDE) feature to secure data in transit. TDE is used to protect data at rest from unauthorized or offline access to the data. Data in the database and data backups are encrypted. TDE is enabled by default when you create a new Azure SQL Database or Azure SQL Managed Instance.
You should not recommend Always Encrypted. Always Encrypted provides a higher level of protection for data in your database and can be applied to individual columns within the database. Data is encrypted at rest, in memory, and in use. It is used when you need added protection such as protecting the data from access by database administrators and other privileged users. Always Encrypted is not meant for use as a substitute for TLS or TDE, and incurs significant processing overhead. It is designed for use as a supplement to TLS and TDE for especially sensitive data.
You should not recommend Dynamic data masking. This is used as a way to ensure that only a select portion of the data in a data column is displayed in clear text; for example, only the last four digits of a telephone number.

---

### References

[Azure encryption overview](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-overview)   
[An overview of Azure SQL Database and SQL Managed Instance security capabilities](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-overview?view=azuresql)  
[Playbook for addressing common security requirements with Azure SQL Database and Azure SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/security-best-practice?view=azuresql)  

---

## Q070:

You plan to migrate an on-premises database to an Azure SQL database. The database size is now 250 GB.
You need to recommend a DTU-based service tier that meets the following requirements:
• Backup retention must be at least 14 days.
• In-memory OLTP must be supported.
Columnstore indexes must be supported.
There must be at least 8 GB in-memory OLTP storage per pool guaranteed.
Which tier and number of DTUs should you recommend? To answer, select the appropriate options from the drop-down menus.

Choose the correct options

---------------------------

Premium | Basic | Standard
125 | 500 | 1000

---------------------------

Tier: Premium
DTUs per pool: 1000

---

### Answer:

You should recommend the Premium Database Transaction Unit-based (DTU-based) service tier. It supports in-memory online transaction processing (OLTP) and columnstore indexes. It also has the lowest latency because it is built on solid-state drives.
You should not recommend the Basic DTU-based service tier. It does not support in-memory OLTP and columnstore indexes. It is mostly used for development environments and should not be used for critical production environments.
You should not recommend the Standard DTU-based service tier. It does not support in-memory OLTP. It is
used for development and production environments and should not be used on critical production environments.
You should recommend 1000 DTUs so that you can have up to 10 GB for in-memory OLTP storage per pool guaranteed.
You should not recommend 500 DTUs. This allows up to only 4 GB for in-memory OLTP storage per pool guaranteed.
You should not recommend 125 DTUs. This only guarantees up to 1 GB for in-memory OLTP storage per pool.

---

### References

[Resources limits for elastic pools using the DTU purchasing model](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-elastic-pools?view=azuresql)  
[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  

---

## Q069:

You manage an Azure SQL Database managed instance that stores critical business data.
You need to design a protection strategy for the data. The solution must protect the entire database while at rest and require minimal administrative effort to implement.
What solution should you recommend?
Choose the correct answer
Dynamic Data Masking
Always Encrypted
Transparent Data Encryption
Azure Storage encryption
▲ Explanation
You should recommend Transparent Data Encryption because it encrypts the data at rest. All data that is stored on the disk is encrypted, as well as backups.
You should not recommend Always Encrypted. This option encrypts both data in transit and data at rest, but it requires greater effort to implement. You should only encrypt the most sensitive data using this feature because it requires you to specify a column to be encrypted.
You should not recommend Azure Storage encryption. This is a default encryption method that protects a storage account, including protecting backup files. It does not protect data and log files while in use.
You should not recommend Dynamic Data Masking because it does not protect data on the disk. This option masks sensitive data when it is displayed in the application.


---

### Answer:

---

### References

[Azure Data Encryption at rest](https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest)  
[Tutorial: Getting started with Always Encrypted](https://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-tutorial-getting-started?view=sql-server-ver16&tabs=ssms)  
[Azure Storage encryption for data at rest](https://learn.microsoft.com/en-us/azure/storage/common/storage-service-encryption)  
[Dynamic Data Masking](https://learn.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16)  

---

## Q068:

You have several general purpose Azure SQL databases that have capacities of 400 database transaction units (DTUs) each. These databases have varying and unpredictable usage demands. Half of the databases use less than 200 DTUs on a regular basis.

You need to optimize the cost of managing and scaling the databases.

What solution should you recommend?

Choose the correct answer

Standard service tier

- v-Core based model
- Elastic pools
- Hyperscale service tier

---

### Answer:

You should recommend elastic pools because they are a cost-effective solution to manage and scale multiple databases that have varying and unpredictable usage demands. The databases will share the resources at a set price. They must be on the same server.
You should not recommend the Hyperscale service tier because you will not be able to control how the resources are managed by each of the databases. The Hyperscale service tier is a highly scalable storage and compute performance tier.
You should not recommend the Standard/General Purpose service tier because you will not be able to control how the resources are managed by each of the databases. This tier is a default service tier in Azure SQL Database that is designed for generic workloads.
You should not recommend the v-Core based model because you will not be able to control how the resources are managed by each of the databases. The vCore-based purchasing model enables you to independently scale compute and storage resources, match on-premises performance, and optimize price.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  

---

## Q067:

You are the lead architect for a new data analytics startup in the Dallas-Fort Worth metropolitan area. You decide on a cloud-only infrastructure in Azure. You currently have data in approximately 50 SQL databases that must be analyzed. Usage patterns vary between databases. The solution is expected to use several databases that will remain in use for at least two years.
You need to contain costs as much as possible and look for ways to reduce expenditures.

How should you consider deploying the solution to minimize costs? For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statements:

Using SQL elastic pools to manage your databases will save costs.
Yes

Configuring the databases to have the Price: Low tag will save costs.
No

Using reserved capacity instead of pay-as-you-go will save costs.
Yes

---

### Answer:

Using SQL elastic pools to manage your databases will save costs. With elastic pools, multiple databases are on a single server and can share resources, which saves costs.
Configuring the databases to have the Price: Low tag will not save costs. Tags are used for adding metadata to Azure resources and to logically organize them into a taxonomy. Tags do not directly affect resource cost.
Using reserved capacity instead of pay-as-you-go will save costs. Pre-paying to reserve a database for one or three years is much more cost-effective than a pay-as-you-go arrangement.

---

### References

[Elastic pools help you manage and scale multiple databases in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql)    

[Save costs for resources with reserved capacity Azure SQL Database & SQL Managed Instance](https://learn.microsoft.com/en-us/azure/azure-sql/database/reserved-capacity-overview?view=azuresql)   

[Acquire, provision, and manage Azure reserved VM instances (RI) + server subscriptions for customers](https://learn.microsoft.com/en-us/partner-center/azure-ri-server-subscriptions)   

---

## Q066:

Company1 relies heavily on its Azure cloud infrastructure to deliver services to clients worldwide. As the company continues to grow, managing access to its critical resources, applications, and data has become a significant challenge.
You need to recommend a solution for controlling and monitoring access to important roles and enforcing just-in-time elevation of rights.

What should you recommend?

Choose the correct answer

- Privileged Identity Management (PIM)
- Access reviews
- Entitlement management
- Activity logs and auditing

---

### Answer:
- Privileged Identity Management (PIM)

You should recommend Privileged Identity Management (PIM). PIM is a feature within Microsoft Entra Identity Governance that specifically addresses the need to control and monitor access to important roles. It enforces just-in-time (JIT) elevation of privileges for users who need them to perform specific tasks. This aligns perfectly with the requirement to manage access to critical resources and ensure that privileges are granted only when necessary.
You should not recommend access reviews. While access reviews are important for periodically auditing and reviewing access to resources, they do not directly enforce JIT elevation of privileges, which is a specific requirement mentioned in the scenario.
You should not recommend entitlement management. Entitlement management helps to define and manage fine-grained access packages, but it may not be the primary choice for enforcing JIT elevation of privileges.
You should not recommend activity logs and auditing. While activity logs and auditing are essential for monitoring and maintaining security, they do not provide the same level of real-time control and enforcement of privileged access as PIM.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  

[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)  

[What is entitlement management?](https://learn.microsoft.com/en-us/entra/id-governance/entitlement-management-overview)  

[Azure security logging and auditing](https://learn.microsoft.com/en-us/azure/security/fundamentals/log-audit)  

---

## Q065:

You are requested to provide resources for a marketing campaign that internal users can use to self-service request without the need for approval. There is a requirement for access to expire within 30 days.

What should you recommend?

Choose the correct answer

- Lifecycle workflow
- Security group
- Access package
- Access review

---

### Answer:
- Access package

You should recommend an access package. An access package is designed to provide a curated set of resources that users can request access to without the need for approval. Access packages allow users to self-service their access requests based on predefined permissions and policies. Additionally, you can set an expiration period for the access, which aligns with the requirement for user access to expire within 30 days. This feature provides an efficient and controlled way to manage temporary access to resources for specific campaigns or projects.

You should not recommend security groups, access reviews, or lifestyle workflows. These are not directly aligned with the requirement of providing resources that internal users can self-service request without approval and with an expiration time frame.

Security groups are used for grouping users, computers, or other security principals for applying permissions, but they do not provide the self-service request or expiration features mentioned in the scenario.

Access reviews are processes used to periodically review and manage user access to resources, but they do not fit the scenario's requirement for self-service access and a specific 30-day expiration time frame.
Lifecycle workflows often involve processes related to the creation, modification, or deletion of identities or resources, but they might not be suitable for managing temporary access requests and their expiration periods as described in the scenario.

---

### References

[Tutorial: Manage access to resources in entitlement management](https://learn.microsoft.com/en-us/entra/id-governance/entitlement-management-access-package-first)   

[Active Directory security groups](https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/understand-security-groups)  

[Review recommendations for Access reviews](https://learn.microsoft.com/en-us/entra/id-governance/review-recommendations-access-reviews)  

[What are lifecycle workflows?](https://learn.microsoft.com/en-us/entra/id-governance/what-are-lifecycle-workflows)  

---

## Q064:

Your organization is preparing a landing zone to move on-premises resources to Azure cloud. You are requested to recommend a hierarchical level where policies must be placed so that they are applied to all workloads that require the same security, compliance, connectivity, and feature settings.
You need to provide a recommendation.

What should you recommend?

Choose the correct answer

- Management group
- Resources
- Resource group
- Subscription

---

### Answer:

You should recommend a management group level. A management group in Azure is a container for organizing subscriptions and applying policies across multiple subscriptions. Placing a policy at the management group level ensures that it is inherited by all the subscriptions and resources within those management groups. This is the recommended approach when you want consistent policies to apply to a set of resources across different subscriptions.
You should not recommend a resource group level. A resource group is a logical container for grouping related resources within a single subscription. While policies can be applied at the resource group level, this would be suitable when you want policies to apply to resources within a specific project or application, rather than across multiple workloads requiring the same settings.
You should not recommend a subscription level. A subscription is a basic unit of organization within Azure. Policies can be applied at the subscription level, but this might not be the best choice for managing workloads that require consistent settings across multiple subscriptions.
You should not recommend a resources level. Applying policies directly to individual resources (such as virtual machines, databases, etc.) is not efficient for managing a larger scale environment with uniform settings. This approach can lead to policy management complexity and difficulty in maintaining consistent settings.

---

### References

[Cloud governance guides](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/govern/)  
[Design for governance](https://learn.microsoft.com/en-us/training/modules/design-governance/2-design-for-governance)  
[Design for management groups](https://learn.microsoft.com/en-us/training/modules/design-governance/3-design-for-management-groups)  
[Design for subscriptions](https://learn.microsoft.com/en-us/training/modules/design-governance/4-design-for-subscriptions)  
[Design for resource groups](https://learn.microsoft.com/en-us/training/modules/design-governance/5-design-for-resource-groups)  
[What is Azure Resource Manager?](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/overview)  

---

## Q063:

Your company has an Azure subscription. The company wants a way to enforce organizational standards and to assess compliance at the subscription level and have the standards apply throughout the organization.
If compliance standards change, you should be able to update the standards and bring resources into compliance through bulk remediation. Remediation for new resources should be automatic.

You need to recommend a solution to meet these requirements.

What should you recommend?

Choose the correct answer

- Azure Resource Manager (ARM) templates
- Azure Managed Identities
- Azure Policy
- Azure Conditional Access

---

### Answer:
- Azure Policy

You should recommend Azure Policy. Azure Policy lets you enforce organizational standards at any point in your hierarchy, from management groups down to the individual resource level. You can define policies to implement governance for resource consistency and compliance. You can run remediation tasks on existing resources and implement automatic remediation for new resources.
You should not recommend Azure Conditional Access. Conditional Access is a feature of Microsoft Entra security that can be used to set requirements to allow or block access to resources.
You should not recommend Azure Managed Identities. Managed Identities is a special type of security principal that is used to manage secure access by Azure resources.
You should not recommend Azure Resource Manager (ARM) templates. ARM templates contain the definition for creating resource groups and resources, but are not used to manage compliance or remediate compliance issues.


---

### References

[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  
[What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)    
[What are ARM templates?](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/overview)  

---

## Q062:

Your company plans to use Azure Blueprints to ensure that sets of Azure resources implement and adhere to organizational standards, patterns, and requirements. The blueprints being designed include:

- Role Assignments
- Policy Assignments
- Azure Resource Manager templates (ARM templates)
- Resource Groups

You need to identify features and functionalities of Azure Blueprints.

For each of the following statements, select Yes if the statement is true. Otherwise, select No.

When a resource is deployed from a blueprint, a connection remains between the blueprint and the resource.
Yes

RBAC role assignments can be defined for a subscription or individual resource groups in a blueprint.
Yes

A subscription owner can override blueprint Read Only and Do Not Delete resource locking options.
No

---

### Answer:

When a resource is deployed from a blueprint, a connection remains between the blueprint and the resource. This is a major difference between using an ARM template to deploy resources and using an ARM template from within an Azure Blueprint. This connection makes for improved tracking and auditing of deployments.
Role-based access control (RBAC) role assignments can be defined for a subscription or individual resource groups in a blueprint. RBAC roles assignments can be included as artifacts in an Azure Blueprint with defined role assignments.
A subscription owner cannot override blueprint Read Only and Do Not Delete resource locking options. A subscription owner can unassign a blueprint that they are assigned at the subscription level, which removes any locking. To prevent the subscription owner from doing this, you should assign blueprints at the management group level.

---

### References

[What is Azure Blueprints (Preview)?](https://learn.microsoft.com/en-us/azure/governance/blueprints/overview)  
[Understand the lifecycle of an Azure Blueprint](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/lifecycle)  
[Understand the deployment sequence in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/sequencing-order)  
[Understand resource locking in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/resource-locking)  

---

## Q061:

Your company is using Azure Blueprints to ensure that sets of Azure resources implement and adhere to organizational standards, patterns, and requirements. You create and publish a blueprint that you then assign to your subscription.
The blueprint completes the following tasks:

- Update select existing resources.
- Create new resources.
- Apply resource locks.
- Add existing groups to select RBAC roles.

You prepare to unassign the blueprint so that you can replace it with a different blueprint. You need to determine the expected impact of unassigning the blueprint.
What will happen when you unassign the blueprint?

Choose the correct answer

- RBAC role assignments are removed.
- Updates to existing resources are rolled back.
- Any new resources created by the blueprint are deleted.
- All blueprint resource locking is removed.

---

### Answer:
- All blueprint resource locking is removed.

All blueprint resource locking is removed when you unassign a blueprint. This is the only impact to resources and resource groups in the blueprint's scope. New resources will remain in place, updates to existing resources are maintained, and RBAC role assignments do not change. Resources remain in place, but they are no longer protected by the blueprint.

---

### References

[What is Azure Blueprints (Preview)?](https://learn.microsoft.com/en-us/azure/governance/blueprints/overview)  
[Understand the lifecycle of an Azure Blueprint](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/lifecycle)  
[Understand the deployment sequence in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/sequencing-order)  
[Understand resource locking in Azure Blueprints](https://learn.microsoft.com/en-us/azure/governance/blueprints/concepts/resource-locking)  

---

## Q060:

Your company has an Azure subscription that hosts most of its computing resources. The company is evaluating the possible use of resource tags to better organize and manage its resources.
You need to identify features and functionalities of using resource tags.
For each of the following statements, select Yes if the statement is true. Otherwise, select No.

Statement

You can use Azure Policy, PowerShell cmdlets, CLI commands, ARM templates, and Azure portal to apply tags.
Yes

Tags applied at the resource group are automatically inherited when you create a new resource.
No

Tags can be applied at the resource group and individual resource hierarchical levels only.
No

---

### Answer:

You can use Azure Policy, PowerShell cmdlets, CLI commands, ARM templates, and Azure portal to apply tags. Azure portal, PowerShell, and CLI are primarily used to make manual changes to resource tags. Azure Policy provides a way to automate the assignment and updating of tags.
Tags applied at the resource group are not automatically inherited when you create a new resource. No inheritance is supported by default, but it can be configured through Azure Policy.
Tags cannot be applied at the resource group and individual resource hierarchical levels only. You can apply tags directly at the subscription level. You cannot apply tags at management group levels. You can apply Azure policies that manage resource tags at management group levels.


---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources) 
[Assign policy definitions for tag compliance](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-policies)  

---

## Q059:

Your company has an Azure subscription that includes multiple resource groups used to organize resources. Currently, you are not using resource tags.
You need to apply resource tags to resource groups, and ensure that the same tags apply to any resources that the group contains. When tags change at the resource group, the change should update its resources. You want to minimize the effort that is required to implement and maintain the solution.

What should you do?

Choose the correct answer

- Manually create and update tags individually for the resources.

- Create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group.

- Create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group. Initiate a remediation task when specified tags are added or updated at the resource group.

- Create and modify the tags at the resource group.

---

### Answer:
- Create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group. Initiate a remediation task when specified tags are added or updated at the resource group.

You should create an Azure Policy to apply specified tags from the resource group to resources, link the policy to the resource group, and initiate a remediation task whenever tags are added or updated at the resource group. Resource tags applied at the resource group are not inherited by its resources. You can specify this action through Azure Policy. After applying the policy, or making changes to the resource group, you can run a remediation task to update the resources. Specified tags are also applied when you create a new resource, or update an existing resource. You can also use an Azure Policy to force resources to inherit tags from a subscription.
You should not just create and modify the tags at the resource group. The tags are not inherited by resources by default.
You should not manually create and update tags individually for the resource groups and resources. This would require the most effort to implement and maintain, and it would probably introduce inadvertent
errors.
You should not create an Azure Policy to apply specified tags from the resource group to resources, and link the policy to the resource group without initiating a remediation process. Remediation is a required part of the process for existing resources. Specified tags are applied when you create a new resource, or update an existing resource.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  
[Tutorial: Create and manage policies to enforce compliance](https://learn.microsoft.com/en-us/azure/governance/policy/tutorials/create-and-manage)  

---

## Q058:

You are designing a monitoring solution for a gaming website hosted in an Azure Web App.
You have the following monitoring requirements:
Track how often users return to the website in a specified time period.
Measure how specific events influence user activities.
View user activity by region.
You need to provide a solution that minimizes administrative effort.

Which monitoring solution should you use?

Choose the correct answer

- Application Insights
- Azure Monitor Alerts
- Azure Al Services
- Time Series Insights environments

---

### Answer:
- Application Insights

You should use Application Insights as it includes a retention feature that enables tracking of user behavior. You can configure custom events so that you can analyze user reactions. You can query user activity by time range as well as by filtering according to region or country.
You should not use Azure Monitor Alerts. Data collected by Azure Monitor helps in detecting potential problems with the application or infrastructure. You can configure alerts for notifying your operation teams before end users face issues. It cannot track the user activity of your application.
You should not use Time Services Insights environments. This service provides fully managed lot analytics solution in the cloud. It provides visualization to your loT events, but does not track events back to user satisfaction.
You should not use Azure Al Services. These services consist of APIs and images that developers can use to integrate popular artificial intelligence (Al) features into their applications. They do not provide monitoring functions.


---

### References

[Application Insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview?tabs=net)  
[User retention analysis for web applications with Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/usage-retention)  
[What is Azure Time Series Insights Gen2](https://learn.microsoft.com/en-us/azure/time-series-insights/overview-what-is-tsi)  
[What are Azure Al services?](https://learn.microsoft.com/en-us/azure/ai-services/what-are-ai-services)  
[Azure Monitor overview](https://learn.microsoft.com/en-us/azure/azure-monitor/overview)  
[What are Azure Monitor Alerts?](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview)  

---

## Q057:

Your company has an Azure subscription with Microsoft Entra ID. Your company's development team members will provision Azure VMs to support special projects.
You need to recommend a solution that meets the following additional requirements:

- Limit VMs to specific sizes only.
- Limit VMs to specific regions.

What should you use?

Choose the correct answer

- Azure Role-Based Access Control (Azure RBAC)
- Azure Policy
- Azure Conditional Access
- Azure Resource Tags

---

### Answer:
- Azure Policy

You should use Azure Policy to meet the requirements. Azure Policy lets you enforce organizational standards at any point in your hierarchy, from management groups down to the individual resource level. You can define policies to implement governance for resource consistency, including which VMs can be created, and in which regions.
You should not use Azure Resource Tags. Resource Tags is a way to organize and manage resources, and it does not control resource governance or the types of resources that can be created. Common uses of resource tags include implementing support for regulatory governance, workload optimization, and cost
controls.
You should not use Azure Role-Based Access Control (RBAC), Azure RBAC roles give you control over what you can do when you access a resource, but they are not used to impose resource consistency for creating resources.
You should not use Azure Conditional Access. Conditional Access is a feature of Microsoft Entra security that can be used to set requirements to allow or block access to resources.

---

### References

[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources?tabs=json)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  
[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  

---

## Q056:

Your Azure organizational hierarchy has one management group with two subscriptions and four resource groups in each subscription. You plan to implement Azure policies to help you to organize and manage resources in your organization.
You plan to implement a custom policy to:
Restrict the types of VMs that can be deployed.
Deny the creation of resources without a Department tag when they are created.
You need to determine the minimum number of custom policies and policy assignments required.

What is the minimum number required in each scenario? To answer, select the correct number from the drop-down menus.

Choose the correct options
1 | 2 | 4 | 8 | 16

Minimum number of custom policies required: 1
Minimum number of policy assignments required: 1

---

### Answer:

To assign the custom policy to the management group, you need to create a minimum of one custom policy and make one policy assignment. Only one policy is needed because you can include multiple conditions in a custom policy to meet the policy requirements by using the allof logical operator. It is important to note that a policy should produce a single effect. If you need, for example, to create a policy to restrict a VM type and also append an automatic tag to new resources, you will need to create two policies, the first one with the deny effect, and the second policy with the append effect.
Policies that are applied at a hierarchical level are automatically inherited; therefore, applying the policy to the management group applies the policies to all subscriptions and resource groups.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)
[Azure Policy definition structure](https://learn.microsoft.com/en-us/azure/governance/policy/concepts/definition-structure-basics)  
[Understand Azure Policy effects](https://learn.microsoft.com/en-us/azure/governance/policy/concepts/effect-basics)  
[Tutorial: Create and manage policies to enforce compliance](https://learn.microsoft.com/en-us/azure/governance/policy/tutorials/create-and-manage)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  

---

## Q055:

You decide to use tagging to organize your Azure resources.
You need to make sure that new resources are tagged as soon as possible and that manual tagging is not required.

What should you do?

Choose the correct answer

- Create an Azure policy to apply tagging when new resources are created.
- Add tags through the Azure portal.
- Write an Azure CLI script to tag every new resource once a day.
- Write a PowerShell script to tag everything once a week.

---

### Answer:

You should create an Azure policy to apply tagging when you create new resources. An Azure policy can be predefined and applied to all new resources.
You should not write a PowerShell script to tag everything once a week. PowerShell is a method that can apply tagging, but it uses manual procedure.
You should not write an Azure CLI script to tag every new resource once a day. Azure CLI is a method that can apply tagging, but it uses a manual procedure.
You should not add tags through the Azure portal. Although you can add tags in the portal after creating a resource, it is still a manual process.

---

### References

[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  
[Tagging a VM using the portal](https://learn.microsoft.com/en-us/previous-versions/azure/virtual-machines/tag-portal)  

---

## Q054:

You are implementing a conditional access solution in your Microsoft Entra environment. During the pilot phase of the implementation, the Security Operation Team (SOC) raised concerns regarding the delay between modifications in user conditions and the enforcement of policy changes.
You need to recommend a mechanism that enables a conversation between the token issuer (Microsoft Entra ID) and the relying party (enlightened app) to address policy violations or security issues.

What should you recommend?

Choose the correct answer

- Two-factor authentication (2FA)
- Continuous Access Evaluation (CAE)
- OAuth 2.0 Authorization Code Flow
- OpenID Connect (OIDC)

---

### Answer:
- Continuous Access Evaluation (CAE)

You should recommend Continuous Access Evaluation (CAE). The scenario discusses implementing a conditional access solution in a Microsoft Entra environment, with concerns raised by the Security Operations Team about delays between user condition modifications and the enforcement of policy changes. The situation calls for a mechanism that facilitates a "conversation" between the token issuer (Microsoft Entra ID) and the relying party (enlightened app) to handle policy violations or security issues. CAE is the most appropriate mechanism for this purpose. CAE is designed to provide real-time or near real- time evaluation of access conditions and allows for ongoing monitoring and assessment of user and device conditions, ensuring that policy changes are promptly enforced. If conditions change for a user (as expressed in the scenario), CAE can trigger an evaluation and interaction between the token issuer and the relying party to address potential policy violations or security concerns.
You should not recommend Two-factor authentication (2FA), OAuth 2.0 Authorization Code Flow, or OpenID. Connect (OIDC). These are authentication and authorization mechanisms, but they do not specifically address the continuous monitoring and evaluation aspect required by the scenario.

---

### References

[Continuous access evaluation](https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-continuous-access-evaluation)  
[Design for conditional access](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/6-design-for-conditional-access)  
[How to use two-step verification with your Microsoft account](https://support.microsoft.com/en-us/account-billing/how-to-use-two-step-verification-with-your-microsoft-account-c7910146-672f-01e9-50a0-93b4585e7eb4)  
[Microsoft identity platform and OAuth 2.0 authorization code flow](https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-auth-code-flow)  
[OpenID Connect on the Microsoft identity platform](https://learn.microsoft.com/en-us/entra/identity-platform/v2-protocols-oidc)  

---

## Q053:

You organization plans on moving all its organizational data from on-premises to the cloud. Your Data Protection Officer (DPO) requested the highest possible level of security for data encryption keys is implemented.
You need to recommend a solution that guarantees a high level of tamper resistance and protection against attacks for cryptographic material, ensuring that stored keys never leave the storage boundary.

What solution should you recommend?

Choose the correct answer

- Hardware Security Module (HSM)
- Trusted Platform Modules (TPM)
- Key Vault
- Encrypted file system

---

### Answer:
- Hardware Security Module (HSM)

You should recommend a Hardware Security Module (HSM) is a specialized hardware device designed to provide secure storage and management of cryptographic keys. HSMs are known for their high level of tamper resistance and protection against attacks. They store keys in a dedicated hardware component, ensuring that the keys never leave the secure boundary of the HSM. This is particularly crucial for meeting the requirement of not letting cryptographic material leave the storage boundary. HSMs offer a robust combination of physical security, tamper resistance, and cryptographic operations, making them a suitable choice for scenarios that demand the highest level of security.
You should not recommend encrypted file systems. Encrypted file systems are designed to protect data at rest on storage devices by encrypting the data stored in files. However, they do not inherently provide the level of protection needed for cryptographic keys. Additionally, encrypted file systems focus on protecting the data itself rather than the keys used to encrypt the data. Since the question specifies the need to protect cryptographic material and ensure it does not leave the storage boundary, encrypted file systems are not the most suitable option.
You should not recommend trusted platform modules (TPMS). TPMS are hardware chips on computer motherboards that provide a secure environment for cryptographic operations. While TPMs can securely store keys, they are typically more focused on local device security and platform integrity. They might not be as specialized as HSMs in offering the highest level of protection and tamper resistance for cryptographic keys.
You should not recommend Key Vaults. Key vaults are cloud-based services designed to securely manage cryptographic keys and secrets. While they offer good security for key management, they are not necessarily hardware based like HSMs. Additionally, the question specifies the need for cryptographic material to never leave the storage boundary, and key vaults could potentially involve data leaving the immediate storage boundary for cloud-based management unless they utilize HSMS.

---

### References

[What is Azure Key Vault Managed HSM?](https://learn.microsoft.com/en-us/azure/key-vault/managed-hsm/overview)  
[Import HSM-protected keys to Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/keys/hsm-protected-keys)  
[File Encryption](https://learn.microsoft.com/en-us/windows/win32/fileio/file-encryption)  
[What is TPM?](https://support.microsoft.com/en-us/topic/what-is-tpm-705f241d-025d-4470-80c5-4feeb24fa1ee)   
[About Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview)  

---

## Q052:

You are requested to implement a solution to manage sensitive information used across services and applications in your Azure environment.
You need to recommend a placement of sensitive information within Azure Key Vault.
Which storage destination within Azure Key Vault should you recommend? To answer, drag the appropriate storage type to each sensitive information object. A storage type may be used once, more than once, or not at all.

Drag and drop the answers

Tags
Secrets
Certficates
Keys

Information about rotation configuration: Tags
RSA object: Keys
JSON object: Secrets
X.509 object: Certficates

---

### Answer:

To effectively manage sensitive information utilized across services and applications within your Azure environment, it is crucial to store each type of sensitive information in its appropriate supported storage destination within Azure Key Vault. The recommended storage types for each sensitive information object are:
• Information about rotation configuration: Tags. Tags are metadata labels that you can assign to resources in Azure, including items within Azure Key Vault. They provide a way to categorize, organize, and manage resources. For sensitive information related to rotation configurations, using tags can help you to label and identify items with specific rotation-related properties or metadata.
• RSA Object: Keys. Rivest-Shamir-Adleman (RSA) is a widely used asymmetric encryption algorithm that involves the use of public and private key pairs. If you have RSA objects that you need to manage, you should store them as cryptographic keys within Azure Key Vault. Keys in Azure Key Vault are used for internal encryption, decryption, and signing operations.
• JSON Object: Secrets. JavaScript Object Notation (JSON) is a data interchange format often used to store structured data. If you have sensitive information stored in JSON format, such as API keys, connection strings, or passwords, you should store them as secrets within Azure Key Vault. Secrets are encrypted and provide an additional layer of security compared to storing them directly in code or configuration files.
• X.509 object: Certificates. X.509 is a standard format for public key certificates, commonly used for secure communication and authentication. If you have X.509 certificates, such as SSL certificates for securing websites, you should store them as certificates within Azure Key Vault. Key Vault's certificate management capabilities allow you to securely store, manage, and use X.509 certificates within your applications and services.

---

### References

[About Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview)  
[The Relationship Between Keys, Secrets and Certificates in Azure Key Vault](https://michaelhowardsecure.blog/2021/04/29/the-relationship-between-keys-secrets-and-certificates-in-azure-key-vault/)  
[About Azure Key Vault certificates](https://learn.microsoft.com/en-us/azure/key-vault/certificates/about-certificates)  
[About Azure Key Vault secrets](https://learn.microsoft.com/en-us/azure/key-vault/secrets/about-secrets)  
[About keys](https://learn.microsoft.com/en-us/azure/key-vault/keys/about-keys)  
[Best practices for secrets management in Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/secrets/secrets-best-practices)  

---

## Q051:

You are tasked with storing sensitive information in Azure Key Vault.
You need to identify which types of sensitive information can be stored in your Azure key vault.
Which three of the following can you store in your Azure key vault? Each correct answer provides a complete solution.

Choose the correct answers

- Blueprints
- Secrets
- Certificates
- Tags
- Keys

---

### Answer:
- Secrets
- Certificates
- Keys

Azure Key Vault is a secure and centralized cloud service provided by Microsoft for managing cryptographic keys, secrets, and certificates used by cloud applications and services. It helps to safeguard sensitive information and provides secure access control.
Secrets are small-sized data elements, such as passwords, connection strings, and API keys. Storing secrets in Azure Key Vault ensures that sensitive data remains secure and accessible only to authorized users and applications.
Certificates are digital files that provide authentication and secure communication. Azure Key Vault allows you to store X.509 certificates and their private keys, enabling applications to securely access these certificates for authentication and encryption purposes.
Keys refer to cryptographic keys used for encryption, decryption, signing, and verification. Azure Key Vault supports the storage of keys, including symmetric keys and asymmetric key pairs. By storing keys in Azure Key Vault, you enhance the security of cryptographic operations.
Tags are not stored in Azure Key Vault. Instead, they are key-value pairs that you can assign to resources like Azure resources, resource groups, or other objects to help organize and categorize them.
Blueprints are not stored in Azure Key Vault. They are used to define a repeatable set of Azure resources that adhere to an organization's standards, patterns, and requirements.

---

### References

[About Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  
[What is Azure Blueprints (Preview)?](https://learn.microsoft.com/en-us/azure/governance/blueprints/overview)  

---

## Q50:

You are responsible for administering a Microsoft Entra environment. To alleviate the workload on your IT department in managing external users, you have been asked to enable self-service account management for your customers.
You need to recommend the most suitable solution.
What solution should you recommend to fulfill this requirement?

Choose the correct answer

- Active Directory Federation Service (AD FS)
- Azure AD B2C
- Microsoft Entra B2B direct connect
- Microsoft Entra B2B

---

### Answer:
- Azure AD B2C

You should recommend Azure Active Directory (Azure AD) B2C. Azure AD business-to-consumer (B2C) is designed for scenarios involving external consumers, such as customers. Azure AD B2C provides robust self-service capabilities, allowing customers to register, manage their profiles, reset passwords, and engage in other account-related activities on their own. It is a suitable choice if you want to provide a self-service functionality for your external customers to manage their accounts.
You should not recommend Microsoft Entra B2B. Microsoft Entra B2B enables external users to utilize their favored credentials for logging into your Microsoft applications and various enterprise software, including software as a service (SaaS) and custom-developed applications. B2B collaboration participants are reflected within your directory, usually designated as guest users. While it facilitates access to resources for external users, it does not provide the same level of self-service account management as Azure AD B2C. Microsoft Entra B2B is more about allowing external entities to access your organization's resources rather than enabling self-service for your customers.
You should not recommend Microsoft Entra B2B direct connect. With this solution you can create a bidirectional and mutually trusted connection with another Microsoft Entra organization to facilitate effortless collaboration. B2B direct connect presently offers support for Teams shared channels, allowing external users to conveniently access your resources using their native Teams instances. While using this solution you can collaborate with external users and grant them access to certain resources, the self-service aspect is more limited compared to Azure AD B2C.
You should not recommend Active Directory Federation Service (AD FS). On-premises Active Directory (AD) with Federation involves using your organization's on-premises AD in conjunction with federation services. It is not directly related to enabling self-service account management for external users, and it might not provide the desired self-service capabilities for customers.

---

### References

[Design for Azure Active Directory B2C (business-to-customer)](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/5-design-business-customer)  

[Design for Microsoft Entra business-to-business (B2B)](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/4-design-business-business)  

[Introduction to Microsoft Entra External ID](https://learn.microsoft.com/en-us/entra/external-id/external-identities-overview)  

[AD FS Overview](https://learn.microsoft.com/en-us/windows-server/identity/ad-fs/ad-fs-overview)   

---

## Q049:

You are managing the Microsoft Entra environment for your organization.
You need to enable secure access for applications and services to Azure resources or applications.
What account or identity should you create for this purpose?

Choose the correct answer

- group Managed Service account (gMSA)
- Service principal identity
- Device identity
- User account

---

### Answer:
- Service principal identity

You should create a service principal identity to enable secure access for applications and services to Azure resources or applications. A service principal is an application identity that you create in Microsoft Entra to provide secure authentication and authorization for applications and services. Service principals allow applications and services to interact with Azure resources using OAuth2 tokens while adhering to controlled access through roles and permissions. They are designed to represent applications, services, or automation tools and play a critical role in enabling secure and controlled access to Azure environment. Service principals are commonly used for scenarios like deploying resources through Azure Resource Manager (ARM) templates, granting permissions to applications, and enabling secure access to APIs and other Azure services.
You should not create a user account, a service account, or a device identity.
User accounts are intended for individual human users, group Managed Service accounts (gMSAs) refer to a traditional concept in on-premises Active Directory, and device identities primarily concern device management in Microsoft Entra ID, rather than authenticating applications or services.

---

### References

[Securing cloud-based service accounts](https://learn.microsoft.com/en-us/entra/architecture/secure-service-accounts)  
[Governing Microsoft Entra service accounts](https://learn.microsoft.com/en-us/entra/architecture/govern-service-accounts)  
[What are user accounts in Microsoft Entra ID?](https://learn.microsoft.com/en-us/training/modules/create-users-and-groups-in-azure-active-directory/2-user-accounts-azure-ad)  
[Secure group managed service accounts](https://learn.microsoft.com/en-us/entra/architecture/service-accounts-group-managed)  
[What is a device identity?](https://learn.microsoft.com/en-us/entra/identity/devices/overview)  

---

## Q048:

A development team is developing a web application App1, which is running on a Azure virtual machine (VM) VM1. App1 stores the data in Azure SQL database DB1. The source code of App1 is stored in a GitHub code repository. A recently performed Static Application Security Testing (SAST) has identified clear text SQL database connection string stored within the code.
You need to recommend a solution to remediate this vulnerability. The solution must minimize costs as well as administrative and development effort.

What should you recommend?

Choose the correct answer

- Azure Storage account
- Microsoft Entra ID
- Azure Key Vault
- BitLocker

---

### Answer:
- Azure Key Vault

You should recommend Azure Key Vault. Azure Key Vault is used to securely store secrets, SQL database connection strings, and certificates. Usually, multi-tier applications need to be authenticated to back-end services. Storing authentication secrets in clear text directly in the code is risky and vulnerable to be revealed to, and misused by, an ineligible user. It is good security practice to separate valuable secrets from application code. Azure Key Vault provides a solution to securely store and manage all the secrets your applications need. By using Azure Key Vault in this scenario you will follow the best security practice.
You should not recommend BitLocker. BitLocker is a solution to encrypt computer hard drives. It provides data protection in case a computer is lost, stolen, or inappropriately decommissioned. In this scenario, the source code is stored in source control solution GitHub. As the SQL database secure string is stored within the code, which is stored in the source code control solution GitHub, encrypting computer hard drives does not meet the requirement
You should not recommend Microsoft Entra ID. Microsoft Entra is Microsoft's Cloud Identity and Access Management Solution. Although you could use Microsoft Entra to allow web applications to authenticate to SQL database, in this scenario, it would require additional administrative and development effort.
You should not recommend Azure Storage account. Azure Storage account provides a container to store data, such as files, queues, tables, and disks. The data can be accessed securely from anywhere around the world. However, Azure Storage account is not designed to hold an application's secrets. You should use Azure Key Vault in this scenario.

---

### References

[Design service principals for applications](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/9-two-design-service-principals)  
[Design for Azure Key Vault](https://learn.microsoft.com/en-us/training/modules/design-authentication-authorization-solutions/10-design-for-azure-key-vault)  
[BitLocker overview](https://learn.microsoft.com/en-us/windows/security/operating-system-security/data-protection/bitlocker/)  
[Use Microsoft Entra authentication](https://learn.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview?view=azuresql)  

---

## Q047:

Your company has a Microsoft Entra subscription. Recent security breaches have resulted from inappropriate or outdated access privilege assignments. You are designing access policies for different departments throughout your company to ease the problem. You want to implement a solution that:

- Provides permissions only when needed
- Lets you set start and end dates for permission assignments
- Sends notifications when privileged roles are activated

You need to identify the requirements for implementing this security design.
What should you recommend? To answer, select the correct recommendations from the drop-down menus.

Choose the correct options

Microsoft Entra ID minimum license:
P2

Microsoft Entra feature:
Privileged Identity Management

---

### Answer:

You should recommend P2 as the Microsoft Entra ID license implementation of the Privileged Identity Management (PIM) feature. PIM lets you manage, control, and monitor access to important resources through features that include:

- Just-in-time (JIT) privileged access control
- Time-bound access to resources with start and end dates
- Required approval to activate privileged roles
- Notification when privileged roles are activated

This gives you the just-in-time and time-sensitive controls needed with the desired notifications. Full implementation of PIM requires Microsoft Entra P2.
You should not recommend Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to help protect against identity-based threats, including monitoring for risks and implementing remediation steps.
You should not recommend Microsoft Entra Connect. Microsoft Entra Connect is used to provide synchronization between on-premises AD and Microsoft Entra ID.
You should not recommend Conditional Access. Conditional Access is used to block or allow access to domain resources based on qualifying criteria such as:

- User account or group membership
- Physical location
- Accessing device

Conditional access does not provide the types of protection needed. Conditional access can identify some risky behaviors and force actions such as password changes or multi-factor authentication.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  

[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  

[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  

[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  

---

## Q046:

Your company has a Microsoft Entra ID P2 environment that hosts over 10,000 licensed users. The domain supports over 100 business-critical applications. Your company is concerned about its ability to recognize and respond to external threats for the applications.
You need to recommend a solution that supports identifying sign-in risks and provides remediation strategies.

What should you recommend?

Choose the correct answer

- Microsoft Entra Connect Health
- Azure Policies
- Microsoft Entra ID Protection
- Microsoft Entra Privileged Identity Management (PIM)

---

### Answer:
- Microsoft Entra ID Protection

You should recommend Microsoft Entra ID Protection. Identity Protection enables an organization to:
Automate detection and remediation of identity-based risks
Investigate risks based on portal data
Export risk data for additional analysis
You can use Identity Protection to detect risks like anonymous IP address use for a user or leaked credentials, and implement remediation steps. Full implementation of Identity Protection requires a Microsoft Entra ID P2 license.
You should not recommend Microsoft Entra Privileged Identity Management (PIM). PIM lets you manage, control, and monitor access to important resources through features that include:
Just-in-time (JIT) privileged access control
Time-bound access to resources with start and end dates
Required approval to activate privileged roles
Notification when privileged roles are activated
Access audit history
PIM does not provide for the protection and remediation needed in this scenario.
You should not recommend Microsoft Entra Connect Health. Connect Health is not a security monitoring and remediation solution, though it provides monitoring of your on-premises identity infrastructure when supporting both an on-premises AD and Microsoft Entra ID. It provides information such as performance monitoring and usage analytics.
You should not recommend Azure Policies. Azure Policy is used to ensure and assess compliance to defined standards. This can include regulatory compliance, resource consistency, security, costs, and so forth. Azure Policy works by comparing resource properties with policy definitions that act as business rules.


---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  
[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)  

---

## Q045:

Company1 has a Microsoft Entra tenant named company1.com.
Company1 uses groups to provide access to resources. Each employee has a user in company1.com. Employees frequently change company positions and often still have access to resources they no longer need.
You need to recommend a solution to automatically remove users from groups that they no longer need to be in.

What should you recommend?

Choose the correct answer

- Microsoft Entra ID Protection
- Conditional access
- Group expiration
- Access review


---

### Answer:
- Access review

You should recommend access review. Access review can be performed once or periodically and you can configure who should perform the review, either the group owner or the group members themselves. At the end of the evaluation period, users who no longer need membership can be automatically removed from the group. Based on the configuration, users for whom the review is not performed can also be removed from the group automatically.
You should not recommend conditional access. Conditional access enables you to control access to cloud apps. You can specify who can access the app, from which devices, which threat level is allowed, and any other conditions that must be met before the user can access the app.
You should not recommend group expiration. Group expiration can be configured only for Office 365 groups, not security groups. Only security groups can be granted access to resources, not Office 365 groups.
You should not recommend Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to detect and prevent risky sign-ins. It cannot be used to remove users from groups automatically.

---

### References

[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)   
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  
[Configure the expiration policy for Microsoft 365 groups](https://learn.microsoft.com/en-us/entra/identity/users/groups-lifecycle)  

---

## Q044:

You create an Azure subscription for your company. You plan to create a resource group for each department in your company. You want to allow only members of a particular department to create resources in the resource group assigned to their department.
You need to identify an Azure feature to support this design.
Which Azure feature should you use?

Choose the correct answer

- Policies
- Locks
- Role-based access control (RBAC)
- Initiatives

---

### Answer:
- Role-based access control (RBAC)

You should use Role-based access control (RBAC). RBAC helps you control which users or groups have access to resources, and which permissions they have on those resources. You can apply RBAC access to a management group, a subscription, a resource group, or a resource. Permissions are inherited from higher scopes to lower scopes.

You should not use locks. Locks allow you to prevent resources from being modified or deleted. This helps you prevent unexpected or accidental changes. The two types of lock levels are CanNotDelete (Delete in the Portal) and ReadOnly (Read-only in the Portal).

You should not use policies. Policies allow you to configure rules that control the types of Azure resources that are allowed in a subscription or resource group. For example, you can prevent VMs of a certain size from being deployed.
You should not use initiatives. Initiatives are groups of policies. Initiatives allow you to manage a set of policies together.

---

### References

[What is Azure Policy?](https://learn.microsoft.com/en-us/azure/governance/policy/overview)  
[Lock your resources to protect your infrastructure](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json)  
[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  

---

## Q043:

A medium-sized company is using Microsoft Entra ID to control access to their applications and services that are deployed in Azure. A recent security audit shows that the Global Administrator group is populated with people who do not need such broad access to the resources.
You need to restrict the resources appropriately. In addition, you want to be able to grant elevated access only for specific periods of time, such as for only an hour or a day or for situations when a person needs temporary access for a specific task.
What two actions should you perform? Each correct answer presents part of the solution.

Choose the correct answers

- Assign more granular roles to the administrators according to their functions.
- Use managed identities to further restrict access to the resources.
- Use Privileged Identity Management (PIM) to create additional rules for access.
- Add conditional access policies to your current access restrictions.

---

### Answer:
- Assign more granular roles to the administrators according to their functions.
- Use Privileged Identity Management (PIM) to create additional rules for access.

You should assign more granular roles to the administrators according to their functions. Unless your organization is very small, the administrators are responsible for specific aspects of a system or only for certain resources. The tasks that are performed by particular administrators should map directly to their roles. You should always assign the least possible privilege that allows users to perform their assigned tasks.
You should also use Privileged Identity Management (PIM) to create additional rules for access, including just-in-time (JIT) privileged access to Azure resources. This includes activating access for only a specific length of time, such as one hour or one day.
You should not use managed identities to further restrict access to the resources. Managed identities provide access for resources, not for users. For example, if an application needs to retrieve secrets from a key vault, it can use its managed identity to retrieve a token.
You should not add conditional access policies to your current access restrictions. Conditional access policies are intended to address issues that are related to remote access and bring your own device (BYOD) workplaces. You can, for example, allow or block access for certain locations or device types.

---

### References

[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[Securing privileged access for hybrid and cloud deployments in Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/identity/role-based-access-control/security-planning)  
[What are managed identities for Azure resources?](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview)  
[What is Conditional Access?](https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview)  

---

## Q042:

Your organization has 10 Azure subscriptions, one for each branch office. Five offices are located in the eastern region, and the other five are located in the western region. Each region is grouped into a management group. Each subscription has multiple resource groups, and each resource group has multiple resources. You want to ensure that only the IT personnel in each region have permission to create or modify the Azure resources for that region.
You need to assign role-based access control (RBAC) permissions to the IT personnel. Your solution must require minimal maintenance.
Which scope should you apply the permissions to?

Choose the correct answer

- Management group
- Resource group
- Subscription
- Resource

---

### Answer:
- Management group

You should apply role-based access control (RBAC) permissions to the management group scope. A management group allows you to group multiple subscriptions for RBAC assignment. In this scenario, each region has multiple subscriptions, and only the IT personnel in a region should be able to create and modify the Azure resources for that region. By creating two management groups, one for each region, you can apply RBAC permissions to a management group and have them affect all the resources in all subscriptions for that management group.
You should not apply RBAC permissions to the subscription scope. This would require you to manage RBAC permissions for 10 subscriptions instead of two management groups.
You should not apply RBAC permissions to the resource group scope. This would require you to manage multiple RBAC permission assignments for multiple resource groups across 10 subscriptions.
You should not apply RBAC permissions to the resource scope. This would require you to manage multiple RBAC permission assignments for every resource group across 10 subscriptions.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q041:

Your organization has four Azure subscriptions, one for each branch office. Each subscription has multiple resource groups, and each resource group has multiple resources. You want to ensure that only the IT personnel in each branch office have permission to create or modify the Azure resources for that office.
You need to assign role-based access control (RBAC) permissions to the IT personnel. Your solution must require minimal maintenance.
Which scope should you apply the permissions to?

Choose the correct answer

- Management group
- Resource group
- Resource
- Subscription

---

### Answer:
- Subscription

You should apply RBAC permissions to the subscription scope. Each office has its own subscription, and only the IT personnel in a branch office should be able to create and modify the Azure resources for that office. By applying the permissions to a subscription, you allow those permissions to affect only that subscription and the resources in that subscription.
You should not apply RBAC permissions to the management group scope. A management group allows you
to group multiple subscriptions for RBAC assignment. In this scenario, RBAC permissions should be assigned to each subscription, not across multiple subscriptions.
You should not apply RBAC permissions to the resource group scope. This would require you to manage multiple RBAC permission assignments for multiple resource groups in a single subscription.
You should not apply RBAC permissions to the resource scope. This would require you to manage multiple RBAC permission assignments for every resource group in a subscription.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q040:

Your organization has one Azure subscription. You plan to deploy four SQL Server virtual machines (VMs) to that subscription. Developers and IT personnel must be able to connect to these VMs. However, only the IT personnel should have permission to modify or delete the SQL Server VMs.
You need to design an authorization strategy. Your solution should follow the principle of least privilege.

Which strategy should you use?

Choose the correct answer

- Deploy the SQL Server VMs in a separate resource group. Add the developers to a Developers group and the IT personnel to an IT group. Assign the Reader role to the Developers group and the Owner role to the IT group at the resource group level.

- Deploy the SQL Server VMs in a separate resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the resource group level.

- Deploy the SQL Server VMs in an existing resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the subscription level.

- Deploy the SQL Server VMs in an existing resource group. Add the developers to a Developers group and the IT personnel to an IT group. Assign the Reader role to the Developers group and the Owner role to the IT group at the resource group level.

---

### Answer:
- Deploy the SQL Server VMs in a separate resource group. Add the IT personnel to an IT group. Assign the Virtual Machine Contributor role to the IT group at the resource group level.

You should deploy the SQL Server VMs in a separate resource group and delegate permissions at the resource group level. Permissions will be inherited by virtual machines in the resource group. The Virtual Machine Contributor role enables you to manage virtual machines. If you delegate this role at the resource group level, you will be able to manage virtual machines in the resource group. Users do not need any additional permissions to be able to connect to existing virtual machines, so you should not assign Developers to any role.
You should not assign the Developers group to the Reader role. This action would assign permissions to the developers that are not required to perform their task of connecting to the virtual machine, and would not conform to the principle of least privilege.
You should not deploy SQL Server VMs to the existing resource group and delegate permissions at the
resource group level. If you take this action, permissions would be inherited by all objects in the resource group, not only by SQL Server VMs. This action would not conform to the principle of least privilege.
You should not assign the Virtual Machine Contributor role to the IT group at the subscription level. If you take this action, permissions would be inherited by all resource groups in the subscription and by all VMs. This action would assign the IT group permissions that are not required and would not conform to the principle of least privilege.

---

### References

[What is Azure role-based access control (Azure RBAC)?](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)  
[Azure management groups documentation](https://learn.microsoft.com/en-us/azure/governance/management-groups/)  

---

## Q039:

Your company has a Microsoft Entra infrastructure. Your company wants to securely share applications and services with guest users from another organization. The guest organization has an on-premises Active Directory (AD) domain but it does not support Microsoft Entra ID.
You need to implement a solution that gives guest users access to select resources while minimizing any additional user account management overhead. You want to minimize introducing any additional security risks.
What should you do?

Choose the correct answer

- Require the guest organization to implement Microsoft Entra ID.
- Create a user account in your Microsoft Entra for each guest user.
- Implement business-to-business (B2B) collaboration.
- Configure synchronization between the guest organization's on-premises AD and your Microsoft Entra ID.

---

### Answer:
- Implement business-to-business (B2B) collaboration.

You should implement business-to-business (B2B) collaboration. With B2B collaboration the guest users must have valid email addresses but an IT infrastructure for user management, such as Microsoft Entra, is not required. The guest organization has full responsibility for managing the external guest users. Your company will invite guest users through a simple invitation and redemption process. You configure conditional access policies to control access to your company content..
You should not create a user account in your Microsoft Entra for each guest user. This is not necessary and would require additional management overhead for your organization.
You should not configure synchronization between the guest organization's on-premises AD and your Microsoft Entra ID. There is no need for any type of synchronization between the organizations.
You should not require the guest organization to implement Microsoft Entra ID. This is not a requirement for the guest organization.

---

### References

[B2B collaboration overview](https://learn.microsoft.com/en-us/entra/external-id/what-is-b2b)  
[Add Microsoft Entra B2B collaboration users in the Microsoft Entra admin center](https://learn.microsoft.com/en-us/entra/external-id/add-users-administrator)  
[Microsoft Entra B2B collaboration FAQS](https://learn.microsoft.com/en-us/entra/external-id/faq)  

---

## Q038:

Your company has an on-premises Active Directory (AD) forest and a Microsoft Entra ID P1 tenant. All Microsoft Entra users are assigned a P1 license. You plan to have users use the same usernames and passwords for on-premises and Microsoft Entra authentication. Password changes are currently managed through the helpdesk to ensure that passwords remain synchronized.
Your company is considering deploying Microsoft Entra Connect on the on-premises network. You need to identify features from this action that will help to reduce the management overhead for your network infrastructure and helpdesk personnel.
Which two features could you use? Each correct answer presents a complete solution.

Choose the correct answers

- Privileged Identity Management (PIM)
- Identity Protection
- Password writeback
- Periodic access review
- Self-service password reset

---

### Answer:
- Password writeback
- Self-service password reset

You should not use periodic access review. This does not require Microsoft Entra Connect and would not reduce the helpdesk workload. Access review is an aid to manage group memberships, access to enterprise applications, and role assignments.
You should not use Privileged Identity Management (PIM). PIM is not a feature specific to a hybrid infrastructure using Microsoft Entra Connect. PIM lets you manage, control, and monitor access to important resources through features that include:
Just-in-time (JIT) privileged access control
Time-bound access to resources with start and end dates
Required approval to activate privileged roles
Notification when privileged roles are activated
Access audit history
You should not use Identity Protection. This is not a feature specific to a hybrid infrastructure using Microsoft Entra Connect. Identity Protection provides a way to detect and remediate identity-based risks. It can generate reports to identify risky users and risky sign-ins. It also supports a risk detection report. Identity Protection requires a Microsoft Entra P2 license.


---

### References

[What is Microsoft Entra Connect?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-azure-ad-connect)  
[How does self-service password reset writeback work in Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/authentication/concept-sspr-writeback)  
[Self-service password reset frequently asked questions](https://learn.microsoft.com/en-us/entra/identity/authentication/passwords-faq) 
[What is Microsoft Entra Privileged Identity Management?](https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection)  
[What are access reviews?](https://learn.microsoft.com/en-us/entra/id-governance/access-reviews-overview)  

---

## Q037:

You have a hybrid identity environment that includes an on-premises Active Directory environment and a Microsoft Entra ID.
You need to create an authentication solution that verifies the following:

- That on-premises AD security policies are applied.
- That passwords are validated against your on-premises AD.
- That passwords are not stored in the cloud.

What solution should you use?

Choose the correct answer

- Password hash synchronization
- Pass-through authentication
- Federated authentication
- Single sign-on (SSO)

---

### Answer:
- Pass-through authentication

You should use pass-through authentication. This authentication method does not store passwords in the cloud (Microsoft Entra ID). Instead, passwords are stored only in the on-premises AD. Consequently, when an authentication request is made, only the on-premises AD is used to validate the request. A lightweight agent is used on-premises to communicate with Microsoft Entra ID.
You should not use federated authentication. Federation allows an external, third-party system to authenticate users. This can include biometrics, smart-cards, and so on. Federation does not authenticate against on-premises AD.
You should not use password hash synchronization. This is the most basic and least-effort solution for a hybrid authentication system. Passwords are stored in the cloud (Microsoft Entra ID). This method is best for simple scenarios where users need access only to Microsoft Entra resources, such as Office 365 and SaaS apps.
You should not use single sign-on (SSO). SSO is an umbrella term that means users only have to sign in once, after that they are automatically authenticated to additional resources. You should not use SSO when you have to sync or store passwords.

---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[User sign-in with Microsoft Entra pass-through authentication](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/how-to-connect-pta)  

---

## Q036:

Your company has an on-premises Active Directory (AD) domain that uses AD Connect to access Microsoft Entra ID. Employees use single sign-on (SSO) from corporate devices or their own devices to gain access to resources to do their jobs. IT administrators reported that the Remote Desktop Protocol (RDP) port of the on-premises AD server was open to perform a one-time administrative task.
After reports of identity theft at a partner company, management is particularly concerned about identity security.
You need to strengthen your security policy to mitigate risks associated with identity compromise.
Which two actions should you perform? Each correct answer presents part of the solution.

Choose the correct answers

- Enable Microsoft Entra conditional access.
- Disable password hash synchronization.
- Close Remote Desktop Protocol (RDP) ports on the on-premises AD server.
- Require strong passwords that expire each month.

---

### Answer:
- Enable Microsoft Entra conditional access.
- Close Remote Desktop Protocol (RDP) ports on the on-premises AD server.

You should enable Microsoft Entra conditional access. This ensures that users gain access to corporate resources only from devices that meet security standards.
You should also close Close Remote Desktop Protocol (RDP) ports on the on-premises AD server. This prevents unauthorized remote access to the server. The only ports that should be open are those that are required for the connection between the AD server and AD Connect.
You should not disable password hash synchronization. You should enable this to provide an extra measure of security. AD Connect compares password hashes to known compromised passwords to make sure that a user's password has not been compromised.
You should not require strong passwords with very frequent expiration. Microsoft research has shown that this restrictive policy causes users to choose passwords that are easy to guess.

---

### References

[Five steps to securing your identity infrastructure](https://learn.microsoft.com/en-us/azure/security/fundamentals/steps-secure-identity)  
[What is Identity Protection?](https://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection) 
[What is password hash synchronization with Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-phs)  
[Hybrid Identity Required Ports and Protocols](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/reference-connect-ports)  

---

## Q035:

Your company has a Microsoft Entra tenant named Company.com. The company has a Marketing, Finance and Research department.
You are designing multi-factor authentication (MFA) for Company.com.
You need to ensure that MFA is only implemented for users in the Research department.
Which requirement should you include in the design?

Choose the correct answer

- Create a conditional access policy.
- Configure authentication methods.
- Implement Microsoft Entra ID Protection.
- Implement Microsoft Entra Privileged Identity Management (PIM).

---

### Answer:
- Create a conditional access policy.

You should include a requirement to create a conditional access policy. A conditional access policy is used to grant access to cloud apps. A conditional access policy can be targeted to users and groups. One of the access controls that you can require in a conditional access policy is the use of multi-factor authentication (MFA).
You should not include a requirement to configure authentication methods. In authentication methods, you can configure custom smart lockout, custom banned passwords and password protection for Active Directory Domain Services. You cannot enable or configure MFA in authentication methods.
You should not include a requirement to implement Microsoft Entra ID Protection. Microsoft Entra ID Protection is used to detect and prevent risky sign-ins. One of the Microsoft Entra ID Protection features is the ability to require MFA for sign-in, but only if sign-in is detected as risky.
You should not implement Microsoft Entra Privileged Identity Management (PIM). PIM is used for just-in- time activation of privileged roles. You can use PIM to require MFA if a user wants to activate privileged role membership. PIM cannot be used to require MFA at user sign in.

---

### References

[What authentication and verification methods are available in Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/authentication/concept-authentication-methods)  
[What is Conditional Access?](https://learn.microsoft.com/en-us/entra/identity/conditional-access/overview)  


---

## Q034:

You are designing a hybrid identity solution for your organization. It consists of an on-premises Active Directory (AD) domain and a Microsoft Entra tenant. Your solution must meet the following requirements:
Allow a user who logs in to their on-premises account to automatically authenticate in Microsoft Entra to access Azure services.
Minimize administrative effort to deploy and maintain.
You need to set up authentication.

Which mechanism should you use?

Choose the correct answer

- Single sign-on (SSO) with password hash sync
- Federation without password hash sync
- Federation with password hash sync
- Single sign-on (SSO) with pass-through authentication

---

### Answer:
- Single sign-on (SSO) with password hash sync

You should use single sign-on (SSO) with password hash sync. This requires the least amount of administrative effort for deployment and maintenance. The on-premises AD domain services stores the user password as a hash. AD Connect then retrieves the user password hash and hashes that hash. It then sends the second hash to Microsoft Entra ID. When the user logs in to Microsoft Entra ID, Microsoft Entra ID compares the password hashes.
You should not use SSO with pass-through authentication. This requires you to install a pass-through authentication agent on an on-premises server.
You should not use federation with or without password hash sync. With this authentication method,
Microsoft Entra delegates authentication to a separate trusted authentication system, such as AD Federation
Services (FS). This requires you to deploy and maintain additional software.

---

### References

[Choose the right authentication method for your Microsoft Entra hybrid identity solution](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/choose-ad-authn)  
[What is password hash synchronization with Microsoft Entra ID?](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/whatis-phs)  
[Implement password hash synchronization with Microsoft Entra Connect Sync](https://learn.microsoft.com/en-us/entra/identity/hybrid/connect/how-to-connect-password-hash-synchronization)  

---

## Q033:

A recent audit of your Microsoft Entra environment has resulted in a requirement to retain your activity logs for 15 years.
You need to meet the requirement, while minimizing complexity and costs.

What should you recommend?

Choose the correct answer

- Azure Service Bus
- Azure Log Analytics workspace
- Azure Event Hubs
- Azure storage account

---

### Answer:
- Azure storage account

You should recommend an Azure storage account. Azure storage accounts provide a cost-effective and relatively simple way to retain logs for a long period while minimizing complexity. You can configure your logs to be stored in an Azure storage account, which offers durable and scalable storage capabilities. Azure Storage provides a straightforward solution for long-term retention without the need for complex configurations or additional services. For the specific scenario of retaining logs for a long duration with minimal complexity and costs, an Azure storage account is the recommended choice.
You should not recommend an Azure Log Analytics workspace. An Azure Log Analytics workspace is a service designed for collecting, analyzing, and visualizing data from your resources. While it provides powerful capabilities for log analysis and monitoring, it is not suited for long-term storage.
You should not recommend Azure Event Hubs. Azure Event Hubs is a scalable event streaming platform that can ingest and process large volumes of data in real-time. While it provides flexibility, it is more geared towards real-time data streaming rather than long-term storage.
You should not recommend Azure Service Bus. Azure Service Bus is a messaging service designed for reliable communication between different applications and services. It provides features like queues and topics for asynchronous messaging patterns. However, for the specific requirement of retaining activity logs for 15 years while minimizing complexity and costs, Azure Service Bus is not the most suitable choice.

---

### References

[How to archive Microsoft Entra activity logs to an Azure storage account](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/howto-archive-logs-to-storage-account)  
[Microsoft Entra data retention](https://learn.microsoft.com/en-us/entra/identity/monitoring-health/reference-reports-data-retention)  
[Azure Event Hubs - A real-time data streaming platform with native Apache Kafka support](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)  
[Log Analytics workspace overview](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview)  
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  


---

## Q032:

You are managing an Azure hybrid environment with resources on premises and in the cloud. You are requested to collect all diagnostic and audit logs centrally in an external third-party security information and event management (SIEM) system.
You need to recommend the most suitable solution to route resource logs to an external third-party SIEM. The solution must provide maximum flexibility and customization in terms of real-time processing and routing logs to different destinations.

What should you recommend?

Choose the correct answer

- Azure Event Hubs
- Azure storage account
- Azure Log Analytics workspace
- Azure Service Bus

---

### Answer:
- Azure Event Hubs

You should recommend Azure Event Hubs. Azure Event Hubs is designed for high-throughput, real-time data streaming and provides the flexibility to process and route logs to various destinations, including an external third-party SIEM system. You can configure your resources to send their diagnostic and audit logs to an Azure Event Hubs and then use custom applications or integrations to transform and forward the logs to your external SIEM. This approach allows you to implement custom processing logic, filtering, and formatting before sending the logs to the SIEM, ensuring that they meet the requirements of the third-party system. Azure Event Hubs' scalability, data distribution capabilities, and real-time processing make it a strong candidate for managing logs in a hybrid environment and integrating them with external SIEM solutions.
While Azure Log Analytics and an Azure storage account have their use cases, neither provides the same level of flexibility and real-time processing capabilities as Azure Event Hubs for sending logs to an external third-party SIEM system, especially when the goal is to process and route logs to different destinations. A partner solution could also be a strong choice if it aligns well with your requirements and provides the necessary customization and integration options.
Azure Log Analytics workspace is a service that helps you to collect, analyze, and visualize data from your resources. It is commonly used for monitoring and managing Azure resources, as well as on-premises resources through agents. You can configure diagnostic and audit logs from your resources to be sent to a Log Analytics workspace. Once collected, you can perform queries, create alerts, and build custom dashboards to monitor and troubleshoot your environment. While Log Analytics is powerful for Azure monitoring, it might not be the best option for sending logs to an external third-party SIEM system.
Azure storage accounts allow you to store various types of data, including logs, in different storage services like Blobs, Tables, Queues, and Files. While you could store your resource logs in an Azure storage account, it is primarily a storage solution. You would need to implement a mechanism or application to regularly fetch logs from the storage and send them to your third-party SIEM system. This approach would require more custom development and might not provide the same level of log processing and real-time integration as Azure Event Hubs.
Azure Service Bus is primarily designed for reliable messaging and communication between applications. While Azure Service Bus is a good choice for facilitating information exchange among various applications and services through messages, it may not be the optimal solution for the task of centralizing logs. In this context, a more fitting solution is Azure Event Hubs. In the broader sense, a message takes on the form of a container adorned with metadata, serving as a vessel for encapsulating data. This data has the capacity to encompass a variety of information types, including structured data encoded in prevalent formats such as JSON, XML, Apache Avro, and Plain Text.

---

### References

[Azure Event Hubs: A real-time data streaming platform with native Apache Kafka support](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about)   
[Log Analytics workspace overview](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview)    
[Storage account overview](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview)  
[What is Azure Service Bus?](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview)  

---

## Q031:

You are planning the migration of a large on-premises IT system to Azure. You want to ensure that proper alerting and metric-collection systems are configured for all resources once they are deployed and in production.
To ensure that the correct Azure functionality is used within the overall monitoring infrastructure, you need to identify the most appropriate feature or tool for each requirement.
What solution should you use for each scenario? To answer, drag the appropriate tool or feature to each requirement. A solution may be used once, more than once, or not at all.
Drag and drop the answers

Tool or feature:

Activity Logs
Azure Monitor
Log Analytics Workspace
Resource Manager Templates
Azure Dashboards
Smart Groups

Requirement:

You need to create standardized deployments of Azure resources that can ensure that standard alerting rules are put into place:
Resource Manager Templates

You need a service to centralize all metrics from Azure resources and create alerts based on those metrics:
Azure Monitor

You need subscription-level events that occur within Azure and which can be used to identify changes that are made to resources:
Activity Logs

You need to do manual analysis of collected Azure log data using the Kusto query language:
Log Analytics Workspace

---

### Answer:

You should use Azure Resource Manager (ARM) templates to support standardized deployments. ARM templates are scripts that can be authored to deploy nearly any resource type within Azure. Deploying a standardized alert configuration as part of the resource at the time of original deployment is a powerful automation feature of Azure.
You should use Azure Monitor to centralize all metrics from Azure resources and create alerts based on those metrics. Azure Monitor collects all log and metric data that is collected within Azure. This provides you with a centralized location for collecting the data.
You should use Activity logs to obtain subscription-level events that occur within Azure and that can be
used to identify changes that are made to resources. Core Azure alerts are surfaced through Activity Logs.
These include service-level alerts, in addition to logging all activities made on resources themselves, and
support alerts at the subscription scope. You should use Log Analytics workspaces to do manual analysis of collected Azure log data using the Kusto
query language. This is a powerful tool that is useful for performing a detailed analysis of historical metrics
or activities for systems within Azure.

---

### References

[Understand the structure and syntax of ARM templates](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/syntax)  
[Azure Monitor Documentation](https://learn.microsoft.com/en-us/azure/azure-monitor/)  
[Azure Monitor data platform](https://learn.microsoft.com/en-us/azure/azure-monitor//data-platform)  
[Overview of Azure platform logs](https://learn.microsoft.com/en-us/azure/azure-monitor/data-sources)  
[Log queries in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-query-overview)  


---

## Q030:

You are designing a monitoring solution for a microservice solution hosted in an Azure Kubernetes Service (AKS) cluster.
You need to recommend a monitoring solution that meets the following requirements:
Measure the memory consumption of cluster nodes.
Monitor the health of pods and deployments.
Create alerts when persistent volumes are more than 80% full.
Visualize the metrics and dashboards within the Azure Portal.

What should you recommend?

Choose the correct answer

- Grafana
- Container insights
- Application Insights
- VM insights

---

### Answer:
- Container insights

You should recommend Container insights. You can use Container insights to monitor workloads running Kubernetes-based solutions in Azure. You can monitor and create alerts for all components in the cluster, including node memory and processor usage, pod and deployment health, and persistent volume usage. You can access the Container insights dashboards and metrics within the Azure Portal or integrate them with external monitoring tools, such as Prometheus or Grafana.
You should not recommend VM insights. You can use VM insights to monitor the performance and health of virtual machines (VMs). It should be possible to use VM insights to monitor some cluster nodes; however, it is not possible to monitor the internal resources in the cluster, like pods and persistent volumes.
You should not recommend Application Insights. You can use Application Insights to monitor application
performance, error rate caused by exceptions, and how users interact with the application, including most- visited pages and other metrics.
You should not recommend Grafana. You can use Grafana to visualize and create dashboards based on multiple sources of metrics. You can use the Azure Monitor data source plugin to query data from Container insights to create custom dashboards. However, you cannot monitor an AKS cluster only with Grafana.

---

### References

[Azure Monitor overview](https://learn.microsoft.com/en-us/azure/azure-monitor/overview)  
[Container insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-overview)  
[Overview of VM insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-overview)  
[Application Insights overview](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)  
[Monitor your Azure services in Grafana](https://learn.microsoft.com/en-us/azure/azure-monitor/visualize/grafana-plugin)  

---

## Q029:

You are migrating your on-premises workloads running on Windows and Linux virtual machines to Azure. During the migration, the applications will run on a hybrid model, with some components running in both Azure and on-premises servers to ensure no downtime for the application during the migration process.
You need to design a monitoring strategy solution so that the logs and metrics are available in a centralized location in Azure. The logs should be stored for at least 18 months and must be easy to view and query.
Which solution should you configure?

Choose the correct answer

- A Storage Account
- Log Analytics
- Event Hubs

---

### Answer:
- Log Analytics

You should configure Log Analytics for this scenario. Log Analytics provides you with capabilities to use the Kusto query language to read the logs and metrics. The logs and metrics can be collected using Log Analytics agents, which can be deployed on all the virtual machines running on-premises or in Azure. The data collected can be retained for a maximum of two years in the workspace. You can create a single workspace for all the logs to centralize the management.
You should not recommend a storage account. This is a cheap solution to store the logs of Azure resources through diagnostic settings. It cannot ship logs from virtual machines directly to the storage account. Even if you can get the logs to the storage account, it is not easy to query them.
You should not configure Event Hubs. Event Hubs is a useful solution when you need to send the logs and metrics to other third-party solutions. Azure diagnostic logs and metrics can be sent to Event Hubs. You can directly send logs to the event hub from the virtual machines using the Linux diagnostic extension 4.0 for Linux virtual machines and Windows Azure Diagnostic extension (WAD) for Windows virtual machines. Despite this, the logs cannot be viewed from the event hub directly and it would need to be integrated with another solution that can ingest the logs and provide querying capabilities. Also, event hubs do not have permanent storage; the retention is short, with seven days maximum for the standard tier.

---

### References

[Azure Monitor Logs cost calculations and options](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/cost-logs)  
[Log Analytics tutorial](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-tutorial)  
[Log Analytics agent overview](https://learn.microsoft.com/en-us/azure/azure-monitor/agents/log-analytics-agent)  
[Use the Linux diagnostic extension 4.0 to monitor metrics and logs](https://learn.microsoft.com/en-us/azure/virtual-machines/extensions/diagnostics-linux?tabs=azcli)  
[Send data from Windows Azure diagnostics extension to Azure Event Hubs](https://learn.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-stream-event-hubs)  
[Diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?tabs=CMD)  

---

## Q026-028:

Your company has a Line of Business (LOB) application running in the Azure cloud. The application should be integrated with an enterprise resource planning (ERP) system running from your company's on-premises main office. Due to compliance reasons, these applications should not receive network traffic from the public internet. Your company's main office has 500 workstations and 100 employees working from home with company-managed laptops that use the LOB application.

You need to configure a connectivity solution that meets the following requirements:

- Provide all employees working from home with secure connectivity to cloud-based and on-premises services.
- Connect the on-premises main office to the Azure network.

---

### References

[VPN Gateway design](https://learn.microsoft.com/en-us/azure/vpn-gateway/design)  
[What is Azure VPN Gateway?](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways)  
[What is Azure Express Route?](https://learn.microsoft.com/en-us/azure/expressroute/expressroute-introduction)  
[About VPN devices and IPsec/IKE parameters for Site-to-Site VPN Gateway connections](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpn-devices)  

---

## Q028:

Solution: You configure a Point-to-Site (P2S) VPN connection for the remote employees and an Express Route between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should provide a connection to give remote employees access to the LOB application through a Point-to-Site (P2S) VPN tunnel. Additionally, you should use an Express Route connection to connect the on-premises head office to the Azure cloud network.

---

## Q027:

Solution: You configure an Express Route connection for the remote employees and between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
No


---

## Q026:

Solution: You configure a Point-to-Site (P2S) VPN connection for remote employees and a Site-to-Site VPN connection between Azure and the main office network.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should use a P2S VPN tunnel to provide a connection for remote employees to access the LOB application and a Site-to-Site VPN connection between the on-premises main office and the Azure network. Access to the applications would be only available inside the on-premises network.

---

## Q022-025:

You are assisting a startup that is planning to launch an online retail platform that sells a wide range of products. The platform's success depends on providing a seamless shopping experience to customers while efficiently managing unpredictable traffic spikes during promotions and sales events. The platform requires additional operating system configuration and control over the server resources.
You need to recommend a solution to meet requirements for significantly increased traffic during Black Friday promotion sales, so that the system can automatically handle the increased load in the most efficient way.

---

### References

---

## Q025:

Solution: You recommend implementing a VM autoscaling mechanism.

Does this solution meet the goal?


---

### Answer:
Yes

This solution meets the goal. Implementing an autoscaling mechanism enables the gradual reduction of the number of virtual machines (VMs) and containers as traffic increases and then returns to normal levels after the promotion. Autoscaling mechanisms automatically adjust resources based on demand, helping to conserve resources and optimize costs. Azure Virtual Machines autoscaling is a feature that allows you to dynamically adjust the number of VM instances based on demand, ensuring optimal performance and cost efficiency. It automates the process of scaling up or down based on predefined rules and conditions, so you can accommodate varying workloads without manual intervention. This feature is particularly useful when your application experiences fluctuations in traffic or resource utilization.

---

###  References

[Virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/overview)
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)  

---

## Q024:

Solution: You recommend microservice architecture to run each service in its own isolated containers.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The microservice architecture recommended allows different services, such as inventory updates, cart calculations, and payment processing, to run independently in isolated containers. This promotes modularity, scalability, and efficient resource utilization. Microservice architecture is an architectural style used in software development to design and build applications as a collection of small, independent, and loosely coupled services. In contrast to monolithic architectures, where the entire application is developed as a single unit, microservices break down the application into smaller, independently deployable services that communicate with each other through well-defined APIs. Each service in a microservices architecture focuses on a specific business capability and can be developed, deployed, and scaled independently. Microservices, each running in its own isolated container, offering control over server resources. Containers allow for customization of the operating system configuration and resource allocation. You can scale individual microservices independently.

---

###  References

[Microservices architecture design](https://learn.microsoft.com/en-us/azure/architecture/microservices/)  
[Microservices assessment and readiness](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/microservices-assessment)  

---

## Q023:

Solution: You recommend to request your IT department to provision additional virtual machines as needed.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. Although requesting additional virtual machines could help to scale resources, it might not be as automatic and dynamic as needed to handle sudden traffic spikes efficiently. Manually provisioning VMs can be time consuming and may not respond quickly to sudden traffic spikes. 

---

###  References

[Virtual machines in Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/overview)
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)  

---

## Q022:

Solution: You recommend serverless computing
Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. Serverless computing, often referred to as "serverless", is a cloud computing model that allows developers to build and run applications without the need to manage underlying infrastructure or servers. In a serverless architecture, developers focus on writing code to implement specific functions or tasks, and the cloud provider takes care of provisioning, scaling, and managing the necessary compute resources. This abstraction of infrastructure management frees developers from the operational complexities of traditional server management. While serverless platforms can automatically scale resources to handle traffic spikes, they do not provide server control, which might not align with the requirement for additional operating system configuration and control over server resources.

---

###  References

[Serverless Computing](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-serverless-computing)  
[Create serverless applications](https://learn.microsoft.com/en-us/training/paths/create-serverless-applications/)  

---

## Q017-021:

> Overview

You work for CompanyA, a large-scale online retail store company which sells electronic products. 
The company has operations in the North America and Europe regions.

> on-premise Environment

The company hosts the application from two data centers located in New York and London. The data centers are connected using high-speed VPN connections. The application is a monolith application deployed on multiple virtual machines hosted on Hyper-V. SQL Database is used to store the application's data. The product asset files, like images and videos, are stored in Network Attached Storage (NAS) file shares.

> Planned Changes

CompanyA wants to modernize applications and data to deliver an improved experience to users. It has decided to use Azure as a cloud service provider and build cloud-native applications. The core platform features of the online store will be built as microservices that will be published as APIs for consumption. A fully-managed relational database will be used by the APIs to store the state. Azure storage accounts will be used for the product assets, along with other documents that will be generated by invoice and billing APIs for purchases.

> Resiliency Requirements

- The APIs should run in an active-active environment from multiple Azure regions.
- The compute infrastructure hosting the APIs must be spread across the availability zone.
- User requests to the APIs should automatically failover in cases of outages.
- The database must have geo-replication enabled.

> Business Requirements

- Administrative effort must be minimized to maintain the solution.
- Cost should be minimized.
- Developers should be able to easily add or remove new microservices to the solution with least effort. 

> Network Requirements

- The API endpoints should be available from a single domain using path-based routing.
- Requests to the APIs should be sent to the region with the lowest latency.
- The database must be available privately for the APIs.

> Security Requirements

- The APIs must be available only on the HTTPS protocol.
- Access to Azure services from APIs must be provided securely.

---

## Q021:

The invoice and billing API is required to send information to the document generator API whenever a transaction is made.
You need to identify which communication type and communications service are required to establish a connection between these APIs to send the transaction details.

Which communication type and communications service should you identify? To answer, select the appropriate options in the answer area.

Communication type: Asynchronous
Communications service: Service Bus queues

---

### Answer:

You should identify the asynchronous communication type. Generally, microservices APIs operate independently and should not be coupled with any other service. To establish communication between the services, asynchronous messaging is the best solution. There is a publisher who sends the details to a messaging solution and a subscriber can see those messages and act on them. The billing and invoice API is a publisher that sends a message, while the document generator API is the subscriber that reads the messages and creates the document. This allows decoupling of the services and results in a pure microservice architecture.
You should not identify the synchronous communication type. This is the direct approach where the API communicates by calling the other API endpoint. This solution looks the easiest but can cause too many problems. For example, if the endpoint is unreachable, the documents will not be generated. The number of requests sent directly to the endpoint can affect the performance of the application. The API cannot be scaled horizontally based on metrics. You would need to implement a retry feature in other APIs in the event of failures. This is also an example of tight coupling between services. Ideally, you should decouple the communication aspect of microservices so that they can work individually.
You should identify Service Bus queues to send the transaction details. Azure Service Bus queues allow you to decouple your microservices API for asynchronous messaging solutions. The invoice and billing API should send a message with all the transaction details to generate the documents for the purchase. Service Bus queues provide functionalities like duplicate message-detection, they support At-Most-Once and At- Least-Once delivery guarantees, they provide atomic operation support, (meaning that all the messages in a transaction are considered as a single unit), as well as dead-letter queues for failed message-delivery. With messaging architecture, you can also scale your APIs properly. Depending on the number of messages in queues, you can scale your APIs horizontally in a predictable manner.
You should not identify Azure queue storage to send the transaction details. This is a very simple messaging service, but it does not allow atomicity, cannot detect duplicate messages, and does not include the concept of dead-letter queues. Service Bus queues are a better solution when working with microservices.
You should not identify Service Bus topics to send the transaction details. Service Bus topics are used for a pub/sub messaging model where you need to publish a message to the topic which is read by multiple subscribers. Since you only have a single microservice that will be receiving the messages, you should use Service Bus queues.
You should not identify REST API calls to send the transaction details. APIs can communicate directly by calling each other by sending a request to the endpoint of the API. However, this will have bottlenecks when the number of requests increases, and scaling the application would not be optimal. This is a type of synchronous request and brings a lot more challenges. The APIs are tightly coupled, so if you make any changes to the API, the other API will need to be modified as well. It is best to use a messaging solution to decouple the communication. You can then modify the API and send messages to the queue, without worrying about any implementation changes.

---

### References

[Storage queues and Service Bus queues - compared and contrasted](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted)  

[Service Bus queues, topics, and subscriptions](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions)  

[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  

[Communication in a microservice architecture](https://learn.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/communication-in-microservice-architecture)  

---

## Q020:

The total size of the product assets files (containing images and videos) in the Network Attached Storage (NAS) file share is 35G. You need to migrate the assets over a content delivery network (CDN) to improve the performance of the retail store.
Which solution should you recommend to migrate the files to a storage account?

- Azure File Sync
- Azure Storage Explorer
- Azure Data Box
- AzCopy

---

### Answer:
- AzCopy

You should recommend AzCopy to transfer the files from the Network Attached Storage (NAS) file share to blob storage. Since the files will be served using Azure Content Delivery Network (CDN), they need to be stored in blob storage. CDN only supports serving static files from blob storage. Azcopy is an open-source CLI tool, which can be used to transfer files over the internet. Since the total size of the files is relatively small, transferring them over the internet should be fairly easy. When migrating data with AzCopy, a job is created that keeps track of the files transferred. If, for any reason, the job fails, you can resume the job to reinitiate the transfer.
You should not recommend Azure File Sync. File Sync is a good option to migrate data from NAS file shares to Azure file shares. You can enable sync and all the files will be stored in the Azure file share. However, since the files need to be served from a CDN, which supports blob storage, this solution cannot be used.
You should not recommend Azure Data Box. Data Box is an offline service that can be used to migrate large amounts of data, up to 100TB. It uses standard NAS protocols to transfer the data from on-premises file shares and the physical box can then be shipped to an Azure region where your infrastructure is deployed. Since the asset data is only in GB, using Azure Data Box is not required.
You should not recommend Azure Storage Explorer. This is a graphical tool to work with an Azure storage account. You can upload your files using the options available in the tool. However, you cannot automate the data transfer. Depending on how the file share is structured and how the data will be stored in the blob storage, you would need to do a lot of manual work to upload all the files.

---

### References

[Upload files to Azure Blob storage by using AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-upload)  
[Planning for an Azure File Sync deployment](https://learn.microsoft.com/en-us/azure/storage/file-sync/file-sync-planning)  
[What is Azure Data Box?](https://learn.microsoft.com/en-us/azure/databox/data-box-overview)  
[Choose an Azure solution for data transfer](https://learn.microsoft.com/en-us/azure/storage/common/storage-choose-data-transfer-solution)  

---

## Q019:

Which two networking solutions should you recommend for traffic routing? Each correct answer presents part of the solution.

- Azure Load Balancer
- Azure Traffic Manager
- Azure Front Door
- Azure Application Gateway

---

### Answer:
- Azure Front Door
- Azure Application Gateway

You should recommend Azure Application Gateway. To use Application Gateway to expose the APIs, you need to use Application Gateway Ingress Controller (AGIC), which is a Kubernetes application. AGIC monitors the cluster to update Application Gateway whenever a new service is selected to be exposed to the outside world. Application Gateway allows you to perform path-based routing and SSL offloading, and you can also enable web application firewall (WAF) features to protect against malicious attacks.
You should also recommend Azure Front Door. Since the clusters are deployed in two regions, you need to use another global load-balancing solution. For this, you can use Azure Front Door to route traffic based on the latency routing policy. This will send the traffic requests to the nearest region. The backend pool for Azure Front Door will be the two application gateways deployed in each region. You can perform SSL offloading and path-based routing in Front Door, as well. Another benefit of Azure Front Door that is useful in this scenario is traffic acceleration, which allows the user to establish a connection to an edge location. From the edge location, the connection to the backend pools is established.
You should not recommend Azure Traffic Manager. Even though Azure Front Door internally uses Traffic Manager, it provides additional features like SSL offloading, which makes the connection route through the HTTPS protocol. This also satisfies the security requirements, where the APIs are only made available over the HTTPS protocol. Traffic Manager is useful for non-HTTPS traffic. Traffic Manager is a DNS-based traffic load balancer, which also makes it less efficient than Azure Front Door whenever failover needs to happen due to DNS caching and DNS TTLS.
You should not recommend Azure Load Balancer. Load Balancer is useful when creating a Load Balancer- type service in Kubernetes. When you create a Load Balancer service in Kubernetes, the cloud controller creates a new Azure Load balancer to provide access to the service outside the cluster. Creating a load balancer for the entire microservice will be a costly solution. Instead, an ingress controller like AGIC should be used to reduce the cost as well as configuration.

--- 

### References

[Use Application Gateway Ingress Controller (AGIC) with a multi-tenant Azure Kubernetes Service](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/aks-agic/aks-agic)  
[Traffic acceleration](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-traffic-acceleration?pivots=front-door-classic)  
[Load-balancing options](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/load-balancing-overview)  
[Traffic routing methods to origin](https://learn.microsoft.com/en-us/azure/frontdoor/routing-methods)  
[What is Application Gateway Ingress Controller?](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview)   
[Use a public standard load balancer in Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/load-balancer-standard)  

---

## Q018:

You need to recommend a solution for the microservices to connect to the Azure SQL database. The traffic should always remain private and must be secure.

What should you do?

- Use service endpoints.
- Use Azure Private Link.
- Define outbound firewall rules.
- Define network access controls.

---

### Answer:
- Use Azure Private Link.

You should use Azure Private Link to connect to the Azure SQL database. With Azure Private Link, you can create a private endpoint for the Azure SQL database. This endpoint is private to the virtual network (VNet). This means that traffic never leaves the VNet boundary. When you create a private link, the resources get a private endpoint in the VNet, which is accessible like any other resource deployed inside a VNet and so you can reach the database using the private IP address. Azure Private Link is designed to provide access to Azure Platform-as-a-Service (PaaS) services inside a VNet.
You should not use service endpoints. With service endpoints, the traffic is routed through the Microsoft backbone network. This allows you to access the resources inside your VNet. However, Azure Private Link is a more secure solution compared to service endpoints, since service endpoints are publicly routable addresses.
You should not define network access controls. This is used when you want to allow access from other Azure resources or allow connections from a specific IP address. The connection is not private, and so the traffic traverses the internet to establish the connection.
You should not define outbound firewall rules. This is required for the outbound connection to storage accounts and other Azure SQL logical servers. This is useful when enabling features like auditing, vulnerability scanning, etc.

---

### References

[Azure Private Link for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/private-endpoint-overview?view=azuresql) 
[Use virtual network service endpoints and rules for servers in Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/vnet-service-endpoint-rule-overview?view=azuresql)  
[What is Azure Private Link?](https://learn.microsoft.com/en-us/azure/private-link/private-link-overview)  
[Outbound firewall rules for Azure SQL Database and Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/azure-sql/database/outbound-firewall-rule-overview?view=azuresql)  
[Azure SQL Database and Azure Synapse Analytics network access controls](https://learn.microsoft.com/en-us/azure/azure-sql/database/network-access-controls-overview?view=azuresql)  


---

## Q017:

Which compute solution should you recommend for the APIs? The solution must meet the resiliency requirements.

- Azure Kubernetes Service (AKS)
- Azure Container Instance (ACI)
- Azure Functions
- Azure App Service

---

### Answer:
- Azure Kubernetes Service (AKS)

You should recommend Azure Kubernetes Service (AKS) for the APIs. The scenario suggests that the core platform features of the online store will be built as microservices. Generally, microservices communicate with each other and need some form of orchestration. Kubernetes can handle all the complexity of orchestrating the microservices and therefore reduce the operational overhead. With AKS, the developers do not have to worry about networking and service discovery when adding or removing microservices. Also, the requirements suggest that the compute infrastructure should be spread across availability zones for high availability and fault tolerance. Also, AKS is a managed service that is free; customers only pay for the node pools attached to the cluster. Using AKS, you can also perform active-active deployment in multiple regions.
You should not recommend Azure App Service. Azure App Service is a good solution for REST APIs and web apps. However, as the number of APIs increases, the effort to maintain them starts to increase as well. Each App Service would have its own ARM templates for deployment and application settings. Also, it does not include a feature for service discovery of new APIs.
You should not recommend Azure Functions. Azure Functions is a good solution for event-driven serverless APIs. However, it is not an efficient solution for the requirements given in the scenario. It cannot auto- discover other APIs and it is not easy to orchestrate when the number of services increases. You would need other solutions for the inter-communication between these APIs, like a publish-subscribe model using Azure Service Bus or Azure Event Grid.
You should not recommend Azure Container Instance (ACI). ACI is used for small-scale applications and task automation. Hosting microservices is not feasible using ACI.

---

### References
[Azure Kubernetes Service (AKS) architecture design](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-start-here)  
[Advanced Azure Kubernetes Service (AKS) microservices architecture](https://learn.microsoft.com/en-in/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices-advanced)  
[AKS baseline for multiregion clusters](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-multi-region/aks-multi-cluster)  
[Tutorial: Host a RESTful API with CORS in Azure App Service](https://learn.microsoft.com/en-in/azure/app-service/app-service-web-tutorial-rest-api)  
[Building serverless microservices in Azure sample architecture](https://azure.microsoft.com/en-us/blog/building-serverless-microservices-in-azure-sample-architecture/)  
[What is Azure Container Instances?](https://learn.microsoft.com/en-in/azure/container-instances/container-instances-overview)  

---

## Q014-016:

CompanyA is a large online clothing retail store that operates from various datacenters in multiple cities in the US. The IT operations team faces many challenges to meet the high demand whenever a sale is announced for the products. You need to recommend a highly available and fault-tolerant application design that meets the following requirements:

- The application must scale based on traffic and demand.
- The application must be able to self-diagnose and self-heal in the case of a failure.
- The database must be available in case of Azure outages.
- The application must be optimized for static content like photos and videos.
- SSL offloading and certificate management must minimize complexity.

---

### References

[Traffic Manager routing methods](https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-routing-methods)  
[Overview of TLS termination and end to end TLS with Application Gateway](https://learn.microsoft.com/en-us/azure/application-gateway/ssl-overview)  
[Active geo-replication](https://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql)  
[Availability sets overview](https://learn.microsoft.com/en-us/azure/virtual-machines/availability-set-overview)  

[What is a content delivery network on Azure?](https://learn.microsoft.com/en-in/azure/cdn/cdn-overview)    
[Automatic instance repairs for Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-automatic-instance-repairs?tabs=portal-1%2Cportal-2%2Ccli-3%2Crest-api-4%2Crest-api-5)   
[Overview of autoscale with Azure Virtual Machine Scale Sets](https://learn.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview)   

[Caching with Azure Front Door](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-caching?pivots=front-door-standard-premium)   
[What is Azure Front Door?](https://learn.microsoft.com/en-us/azure/frontdoor/front-door-overview)    

---

## Q016:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. The VM scale sets have an autoscaling rule based on CPU utilization. Use Azure SQL Database with Geo-replication enabled. Use Azure Front Door to perform the SSL offloading and route traffic using the priority routing method. Use the caching feature of Azure Front Door to optimize the application's performance for static content.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale sets deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in case of failures. The autoscaling rule to increase and decrease the number of VMs can manage the changes in demand and traffic. In this way, the IT operations team does not have any overhead to perform manual intervention whenever the traffic requirements change. The autoscaling policy can take care of this requirement very efficiently. The instance repair feature of VM scale sets can monitor the health of the instances and replace any instance that fails the health checks. Thus, this ensures that the system self-heals and recovers automatically from a failure.
The Azure SQL Database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Azure Front Door has many features which can be used to fulfill a number of requirements. Its SSL- offloading capabilities also reduce the overhead from the backend pool VMs. Azure Front Door also has caching capabilities that can serve content from point of presence (PoP) locations to improve the application's performance when using static content.

---

## Q015:

Solution: Deploy the application on Azure VM scale sets in multiple availability zones in two regions in active/passive mode. Enable scheduled VMSS autoscaling for the holiday sale dates throughout the year. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager using priority routing between the two regions. Use Azure Content Delivery Network (CDN) to cache the static content and optimize the application's performance.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. The VM scale set deployed in multiple availability zones are fault-tolerant and can scale based on demand using the autoscaling policies. A fault-tolerant system continues to operate continuously even when there are failures. VM scale sets can take care of both high availability and self- healing in the event of failures. High availability is achieved using the scheduled autoscaling. For the holiday sale, the autoscaling policy can increase the number of VMs based on the predicted demand. With multi- availability zone deployment, the application will still be available when any availability zone goes down and the VM scale set maintains the number of desired VMs across the zones. Also, with the instance repair feature of VM scale sets, you can monitor the status of the instances and, if any instance fails, the VM scale set replaces that instance with a new one.
Additionally, the Azure SQL database with Geo-replication enabled would solve the requirement related to ensuring the availability of the data in the case of Azure outages. With geo-replication, you can replicate the data to the passive site and perform a failover whenever the active site is down. Furthermore, Azure Application Gateway can minimize complexity and the overhead from the VMs via SSL offloading and distributing the traffic to the backend pool of the VMs where the application is deployed.
Azure Traffic Manager with priority routing always sends the request to the active site. The request will automatically failover if the active site becomes unavailable. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes. This fulfills the requirement of self-healing infrastructure, as well. The operation team does not have to take any manual action if the active site goes down. The traffic gets routed to the passive site automatically.
CDN can optimize the application's performance by a lot. The photos and videos can get served from the point of presence (PoP) location nearest to the user. The user will experience low latency when the content is served from the PoP locations.

---

## Q014:

Solution: Deploy the application on Azure VMs with an availability set within two regions in active/passive mode. Use Azure SQL Database with Geo-replication enabled. Use Azure Application Gateway for SSL offloading and route traffic using Azure Traffic Manager with performance routing between the two regions.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. The VMs would not scale based on demand, and you would need manual intervention to scale the number of VMs based on the traffic. Furthermore, this solution also does not address the requirement of optimizing performance for static content and cannot self-heal in case of failures. The performance aspect can be improved by using some caching mechanism, like an Azure Content Delivery Network (CDN), which can cache the photos and videos to the points of presence (POP) locations around the world for faster access. For self-healing, you would need a mechanism that can check the status of the VM and replace it if the health check fails. This can be done using a VM scale set instance repair feature, but it is not available for VMs.
The VMs deployed in the availability set would ensure high availability in the event of Azure outages or during maintenance, as they would be spread across multiple upgrade domains and fault domains, which maintain availability during a maintenance activity and failures respectively. Azure SQL database with geo- replication enabled would also ensure high availability in the event of an Azure outage. With geo- replication, you can replicate the data to the passive site and perform a failover whenever the active site is down.
Also, Azure AppGateway could reduce the overhead from the VMs to perform the SSL offloading and distribute the traffic to the backend pool of the VMs where the application is deployed. As well as this, Azure Traffic Manager with performance routing would route traffic to the location's closest region where the network latency is lowest. Since the application is deployed in active/passive mode, the traffic should always route to the active site. It should only send traffic to the passive site when the active one is no longer available. Traffic Manager determines the health of the active/passive endpoints by running periodic health probes.

---

## Q011-013:

Your company has an on-premises SQL Server 2019 instance hosted on a dedicated server running Windows Server 2019 Datacenter edition. The database supports a high-volume transaction processing application with hundreds of users connecting to the database at any time. User connections and transaction volume are expected to grow rapidly as your company expands. Rather than investing in new on-premises resources, the company decides to move the database to the cloud.
The database currently contains 3 TB of data and is expected to grow to no more than 4 TB. The solution should support at least one read-only replica in addition to the primary read-write database.

You need to deploy the database to Azure and meet the requirements.

---

### References

[DTU-based purchasing model overview](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql)  
[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql&tabs=azure-portal)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  
[Resource limits for single databases using the DTU purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-single-databases?view=azuresql)  

---

## Q013:


Solution: Deploy an Azure SQL Managed instance under the vCore-based purchasing model and choose the Business Critical service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Managed Instance under the vCore-based purchasing model and choose the Business Critical service tier. The Business Critical service tier supports up to 4 TB storage and multiple read-only replicas. In case of failure of the primary database, one of the read-only replicas is promoted to be the new primary database with read-write support.

---

## Q012:


Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Hyperscale service tier.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You should deploy an Azure SQL Database under the vCore-based purchasing model and choose the Hyperscale service tier. The Hyperscale service tier supports up to 100 TB of storage. It supports up to four read-only replicas in addition to the primary read-write database. This is the most expensive option, but it meets all of the requirements.

---

## Q011:

Solution: Deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not deploy an Azure SQL Database under the DTU-based purchasing model and choose the Premium service P6 tier. This deployment option does not meet the storage requirements for the scenario. Premium service P6 is limited to no more than 500 GB. You would need to select a Premium P11 or Premium P15 deployment to meet the storage requirements. This solution also does not meet the read replica requirements.

---

## Q008-010:

A manufacturing company is looking to improve its manufacturing processes. lot sensors throughout the production line collect real-time data. The company wants to perform real-time data collection and analysis from these devices. The solution must support bi-directional communication to send commands back to the lot sensors.

You need to develop a data flow process to meet this requirement.

---

### References

[IoT Concepts and Azure IoT Hub](https://learn.microsoft.com/en-us/azure/iot-hub/iot-concepts-and-iot-hub)  
[Welcome to Azure Stream Analytics](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction)  
[Choose between Azure messaging services - Event Grid, Event Hubs, and Service Bus](https://learn.microsoft.com/en-us/azure/service-bus-messaging/compare-messaging-services)  
[Connecting IoT Devices to Azure: IoT Hub and Event Hubs](https://learn.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs)  
[Stream Azure monitoring data to an event hub or external partner](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/stream-monitoring-data-event-hubs)
[What is dedicated SQL pool (formerly SQL DW) in Azure Synapse Analytics?](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is)  

---

## Q010:

Solution: You configure an lot hub to receive the data and send the data to Azure Stream Analytics.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal and the scenario requirements. The lot hub provides communication between the lot devices and Azure Stream Analytics. Azure Stream Analytics is a real-time event processing engine that can process a high volume of fast streaming data from a variety of sources. This allows for real-time analysis of telemetry streams for lot devices.

---

## Q009:

Solution: You configure an lot hub to receive the data and send the data to Azure Data Factory.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. An lot hub can be used to receive the data and stream it to a destination, but Azure Data Factory is not used for real-time data analysis. Azure Data Factory is a cloud- based extract-transform-load (ETL) and data integration service designed for data movement and large-scale data transformations.

---

## Q008:

Solution: You configure the lot sensors with an event hub and send the data to Azure Stream Analytics for analysis.

Does this solution meet the goal?

---

### Answer: 
No

This solution does not meet the goal. You can use an event hub for streaming data as an input to Azure Stream Analytics. However, an event hub does not provide bi-directional communication with lot devices.

---

## Q004-Q007:

> Overview

CompanyA sells building materials online and in retail outlets around the world. CompanyA's IT infrastructure is hosted in on-premises data centers in multiple locations. The CEO and CTO realize the potential of Microsoft Azure Cloud to increase the company's competitive advantage and eliminate security concerns.

> Existing Environment

- CompanyA operates public websites serving its online shop.
- An online shop web application is connected to an SQL database server in the backend.

- The company's Microsoft SQL database server holds a 100GB product catalog and online order data. Rapid growth of the database size is not planned.

- The Active Directory (AD) authentication provider is in place. The AD domain name is companya.com.
- Marketing documents, media files, and product manuals are stored on file share servers on-premises at the head office's location.

- Marketing documents are accessed via mapped drives on Windows 10 clients.

> Problem Statement

- The SQL database server is not highly available. Every outage means a drop in sales and loss in customer confidence.

- The online shop website's performance on Black Friday is slow. The Microsoft SQL database has been identified as the bottleneck.

- Searching the site when viewing many items on the page is slow.

- There are security concerns regarding the possible loss of financial and personal data due to the potential unauthorized access by support personnel or administrators of the SQL database content.

- Content on file share servers includes not only current files, but also historical data. This historical data is rarely accessed and is needed only in the case of requests relating to legal and compliance matters. Due to legal requirements, the historical data will have to be kept for seven years. This data occupies additional storage and is costly to manage.

- Some remote locations have a slow internet connection and, therefore, access to marketing documents from remote locations is very slow.
- Privileges and rights granted to users and identities are not supervised.

> Planned Changes

- The online shop web servers will be migrated into Azure Cloud.

- The SQL database servers will be migrated into Azure Cloud with the least amount of effort.

- Data security practices will undergo modernization according to industry standard best practices.

- Marketing documents will be migrated from file share servers onto the cloud.

- A solution to reduce costs for historical data on file shares is to be implemented.

> Technical & Business Requirements

- The SQL database must be highly available. Latency must be reduced. 

- CompanyA requires a solution to obtain real-time business insights about customers' purchasing behavior by analyzing data collected from its own online shop's website, social media channels, and partner websites.

> Security & Policy Requirements

- Legal regulations require customer data stored in Microsoft SQL Database to reside in the corresponding region of customer residence.
- All the data has to be secured at rest, in transit and in use.
- Compliance policy requires the retention of online order data for a minimum of seven years.
- Privileges and rights be granted to users and identities must be supervised.

---

## Q007:

In your solution design you need to recommend a solution to meet the technical requirements for Azure SQL Database.
Which solution for Azure database service tier should you recommend?

- Azure SQL Hyperscale
- Azure SQL Business Critical
- An SQL Server on an Azure Virtual Machine
- Azure SQL General Purpose
- Security and policy requirements

---

### Answer:
- Azure SQL Business Critical

You should recommend Azure SQL Business Critical tier. This Azure database service tier is designed to serve for mission-critical applications that require low latency and minimal downtime. This Azure database service tier utilizes Always On availability groups and high performance direct attached SSD storage. This is the best solution to meet the requirement in this scenario.
You should not recommend the Azure SQL General Purpose. This Azure database service tier provides budget-oriented balance between compute and storage. It utilizes multiple failover nodes with spare capacity. In the case of an outage, spare nodes can create a new SQL Server instance and the workload can be failed over. In addition to a higher latency compared to business critical architecture, this service tier introduces additional downtime to create a new SQL Server instance and failover workloads. In this scenario, especially for Black Friday, it is not acceptable.
You should not recommend the Azure SQL Database Hyperscale solution. This Azure database service tier is most suitable for rapid-changing storage needs, which can be rapidly scaled out up to 100TB. As the SQL database in this scenario is only 100GB and rapid growth is not expected, this service tier is not the solution you should recommend.
You should not recommend SQL Server on Azure Virtual Machine. This type of deployment is similar to what you have on premises, except that SQL server is running on an Azure virtual machine. This deployment is a one-server deployment with no guarantee of the required availability.

---

### References

[vCore purchasing model Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql)  
[Design for Azure SQL Database](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/2-design-for-azure-sql-database)  
[What is an Always On availability group?](https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15)  
[Hyperscale service tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql)  

---

## Q006:

You need to recommend a solution to include in your design to meet the security requirement for Azure SQL Database.

What should you recommend?

- SQL Elastic pools
- Azure Cosmos DB with multi-region writes
- Horizontal scaling by Sharding
- Elastic database tools
- Security and policy requirements

---

### Answer:
- Horizontal scaling by Sharding

You should recommend to include Horizontal scaling by Sharding in your SQL database design. Horizontal scaling by Sharding provides a solution to partition (sharding) a database into multiple databases, which can be scaled independently. The Sharding solution utilizes a special database named shard map manager, which maintains global mapping information about all shards (databases) in a shard set. The application uses this shard map to save data into the proper shard. In this scenario, with Sharding, you meet the legal regulations to maintain customer data in the region of respective customer residence by placing each shard with data in the region of customer residence.
You should not recommend SQL Elastic pools. SQL Elastic pools provide a mechanism to scale computer power up or down for SQL Database as needed. In this scenario, SQL Elastic pools cannot meet the requirement for customer data residence as providing more or fewer compute resources does not change the residence of the data. For this specific requirement, you have to implement Horizontal scaling by Sharding in your SQL database design.
You should not recommend Elastic database tools. Elastic database tools provide the solution to query data across multiple databases and provide output to Microsoft Excel or third-party tools for visualization. Elastic database tools have no influence on the data residency, and as such it cannot meet this requirement.
You should not recommend Azure Cosmos DB with multi-region writes. Azure Cosmos DB is a NoSQL database, which provides real-time access, multi-region writes and data distribution to any Azure region, and it can scale storage and throughput across any Azure region elastically and independently. Azure Cosmos DB can provide the best solution for mobile, gaming and lot applications. Although there are possibilities to migrate data from SQL Server to Azure Cosmos DB and implement data residency restrictions, it would be very expensive in terms of efforts and costs. In the given scenario, it is required for the data to reside in the region of corresponding customer residence and the SQL database migration needs to be executed with the least amount of effort.

---

### References

[Recommend a solution for database scalability](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/5-recommend-database-scalability)  
[Welcome to Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)  
[Understanding distributed NoSQL databases](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql)  

---

## Q005:

You are tasked with remediating the security concerns and implement data security requirements.

Which three encryption solutions should you include in your design? Each correct answer presents part of the solution.

- Always Encrypted
- Server-level firewall
- Network security groups (NSGs)
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security


---

### Answer:

- Always Encrypted
- Transparent Data Encryption (TDE)
- SSL-Secure Sockets Layer/TLS - Transport Layer Security

To remediate the security concerns for data-at-rest you should recommend Transparent Data Encryption (TDE). To ensure data privacy, data sovereignty, and data compliance it is indispensable and mandatory to encrypt data at rest. TDE provides such a data encryption capability for data at rest and, as such, the data is protected from the unauthorized access of raw offline database files on a hard disk or a backup copy. The encryption is performed by TDE on a page level (logical partitioning of the data within a data file (.mdf or .ndf) in a database, numbered contiguously from 0 to n), so that data is encrypted as it is written from memory to the disk and decrypted as it is read from the disk to memory.
To remediate the security concerns for data-in-use you should also recommend Always Encrypted. Always Encrypted is a security technique to hide sensitive data stored in specific database columns. The data can be only decrypted while it is loaded into memory of a client computer and processed by a client application. This technique protects data also from administrators or other support personnel that are authorized to access databases for maintenance.
To remediate the security concerns for data-in-transit you should, additionally, recommend SSL/TLS. This encryption provides protection of data from man-in-the-middle (MITM) or side channel attacks during data transportation from the backend SQL database server to the front end application server.
You should not recommend a server-level firewall. Although server-level firewalls protect from unauthorized access by end-users, it does not protect from unauthorized access by support personnel or administrators. Server-level firewalls work on the network layer, which is layer 3 of the International Standardization Organization (OSI) reference model. As an administrator, you must have access to the server in order to be able to pass by firewall for SQL server maintenance.
You should not recommend Network security groups (NSGs). Using an NSG in Azure you can filter network traffic between Azure resources, such as virtual machines within a virtual network (VLAN). The functionality of an NSG is similar to the functionality of a firewall. Similar to firewalls, NSGs can prohibit access by any unauthorized user or resource, but it cannot prohibit access by support personnel or administrators.
References
Design security for data at rest, data in motion, and data in use
Protect data in-transit and at rest
Windows Network Architecture and the OSI Model

---

### References

[Network security groups](https://learn.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview)   

---

## Q004:

You need to recommend a storage design solution on the Azure Cloud platform for the marketing documents. Administrative effort and cost must be minimized.

What should you recommend?

- Azure Files
- Azure Queue Storage
- Azure managed disks
- Azure Blob Storage

---

### Answer:
- Azure Files

You should recommend Azure Files as a storage design solution for the marketing documents. Azure Files is a file share hosted on the Azure Cloud platform. Files hosted in Azure Files can be accessed via an industry- standard Server Message Block (SMB) protocol for file sharing. SMB allows applications to read and write to files in a computer network. Azure Files can be mapped as a shared drive on all main operating systems and thus can replace the company's on-premises file shares. In this scenario, marketing documents are accessed via mapped drives on Windows 10 clients, as such Azure Files is the best replacement solution.
You should not recommend Azure Blob Storage. Azure Blob Storage is an object-oriented storage solution.
It is optimized to store unstructured data, such as large binary files, media files, log files, and backup files. Unlike Azure Files, Azure Blob Storage cannot be mapped as a shared drive, which is the main reason why, in this scenario, it is not the ideal storage solution for the marketing documents.
You should not recommend Azure managed disks. Azure managed disks are block-level storage volumes used by virtual machines to store data. Usually, you create Azure managed disks at the same time as a virtual machine.
You should not recommend Azure Queue Storage. Azure Queue Storage is a service to provide an interface for service-to-service communication. Azure Queue Storage can store a large amount of messages and can be accessed from anywhere using the HTTP or HTTPS protocols.

---

### References

[Design for data storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/2-design-for-data-storage)  
[Design for Azure Files](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files)  
[Design for Azure Blob Storage](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage)  
[Design for Azure managed disks](https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/7-design-for-azure-disk-solutions)  
[What is Azure Queue Storage?](https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction)  
[Overview of file sharing using the SMB 3 protocol in Windows Server](https://learn.microsoft.com/en-us/windows-server/storage/file-server/file-server-smb-overview)  

---

## Q001-Q003:

Your organization is moving its business operations to Azure. Your company is organized into three departments that will deploy Azure app services and databases. A fourth department is responsible for internal operations and will deploy resources as necessary to support those activities.
Your company wants to be cost-conscious in its move to the cloud, and exercise cost and budget controls at the department level. The company plans to use the Azure Resource Usage and Rate Card APIs to colle billing data for analysis.

Initially, you set up your company's Azure with one subscription.

You need to design a solution for cost reporting by department. 
The solution should follow best practices for organizing resources and should minimize the effort required to maintain the solution.

---

### References

[Organize your Azure resources effectively](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-setup-guide/organize-resources?tabs=AzureManagementGroupsAndHierarchy)  
[Resource naming and tagging decision guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming-and-tagging-decision-guide)  
[Track costs across business units, environments, or projects](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/track-costs)  
[Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources)  

---

## Q003:

Solution: You create a separate resource group for each type of resource, and tag the resources with department and billing code.

Does this solution meet the goal?

---

### Answer:
Yes

This solution meets the goal. You can create a separate resource group for each type of resource, and tag the resources with department and billing code. Microsoft's best practices recommend various options for organizing resources into resource groups, such as by resource type, by project, by lifecycle, and so forth. The use of tags lets you organize the data, and retrieve data by department for cost accounting and budgeting purposes.

---

## Q002:

Solution: You create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure role-based access control (Azure RBAC).

Does this solution meet the goal?

---

### Answer:
No

The solution does not meet the goal. You should not create a separate resource group for each department to host that department's resources, and limit access to the resource group through Azure RBAC. Organizing by department is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information. Azure RBAC is used to control and manage access to resources, but it does not retrieve the information you need.

---

## Q001:

Solution: 
You create a separate subscription for each department, and use resource groups to organize resources by project.

Does this solution meet the goal?

---

### Answer:
No

This solution does not meet the goal. You should not create a separate subscription for each department, and use resource groups to organize resources by project. Creating separate subscriptions for each department adds administrative overhead. Organizing by project is an acceptable way of organizing resources, but it does nothing toward retrieving and reporting cost accounting and budget information.

---
